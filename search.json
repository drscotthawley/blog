[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I dabble in a variety of technologies, usually with the goal of improving the physics education of audio engineering students. But I’m pretty hard-core into Deep Learning applications for musical audio production: I got “addicted” to machine learning and neural networks about 10 years ago and it’s changed my life.\n\nAffiliations\nProfessor of Physics, Belmont University. Professorly Web Page\nTechnical Fellow, Harmonai + Stability AI\nSenior Data Fellow, Belmont Data Collaborative"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html",
    "href": "posts/2019-12-21-pca-from-scratch.html",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "",
    "text": "splash pic i made"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#basic-idea",
    "href": "posts/2019-12-21-pca-from-scratch.html#basic-idea",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Basic Idea",
    "text": "Basic Idea\nPut simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or “features”) you started with to a set of axes ‘aligned with the data itself,’ and doing this almost always means that you can get rid of a few of these ‘components’ of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation.\nOnce you “get it,” you’ll find PCA to be almost no big deal, if it weren’t for the fact that it’s so darn useful!\nWe’ll define the following terms as we go, but here’s the process in a nutshell: 1. Covariance: Find the covariance matrix for your dataset 2. Eigenvectors: Find the eigenvectors of that matrix (these are the “components” btw) 3. Ordering: Sort the eigenvectors/‘dimensions’ from biggest to smallest variance 4. Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space 6. (Check: How much did we lose by that truncation?)"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#covariance-matrix",
    "href": "posts/2019-12-21-pca-from-scratch.html#covariance-matrix",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Covariance matrix",
    "text": "Covariance matrix\nSo what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that \\(N-1\\) in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn’t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix.\nNumpy gives us a handy thing to call:\n\ndata = np.stack((x,y),axis=1)   # pack the x & y data together in one 2D array\nprint(\"data.shape =\",data.shape)\ncov = np.cov(data.T)   # .T b/c numpy wants varibles along rows rather than down columns?\nprint(\"covariance matrix =\\n\",cov)\n\ndata.shape = (100, 2)\ncovariance matrix =\n [[1.39271497 0.68832432]\n [0.68832432 0.39477528]]"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#find-the-eigenvalues",
    "href": "posts/2019-12-21-pca-from-scratch.html#find-the-eigenvalues",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "1. Find the eigenvalues",
    "text": "1. Find the eigenvalues\nOk I’m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don’t know what a determinant is good for (e.g. tons of proofs & properties all rest on the determinant), they still at least remember how to calculate one.\nThe way to get the eigenvalues is to take the determinant of the difference between a \\(\\bf{A}\\) and \\(\\lambda\\) times the identity matrix \\(\\bf{I}\\) (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero…\n\\[det( \\bf{A} - \\lambda I) = 0 \\]\n\nJust another observation: Since \\({\\bf I}\\) is a square matrix, that means \\(\\bf{A}\\) has to be a square matrix too.\n\nThen solving for \\(\\lambda\\) will give you a polynomial equation in \\(\\lambda\\), the solutions to (or roots of) which are the eigenvectors \\(\\lambda_i\\).\nLet’s do an example:\n\\[\n{\\bf A} =\n\\begin{bmatrix}\n-2 & 2 & 1\\\\\n-5 & 5 & 1\\\\\n-4 & 2 & 3\n\\end{bmatrix}\n\\] To find the eigenvalues we set \\[\ndet( \\bf{A} - \\lambda I) =\n\\begin{vmatrix}\n-2-\\lambda & 2 & 1\\\\\n-5 & 5-\\lambda & 1\\\\\n-4 & 2 & 3-\\lambda\n\\end{vmatrix} = 0.\\]\nThis gives us the equation… \\[0 = \\lambda^3 - 6\\lambda^2 + \\lambda - 6\\]\nwhich has the 3 solutions (in descending order) \\[ \\lambda = 3, 2, 1.\\]\n(Aside: to create an integer matrix with integer eigenvalues, I used this handy web tool).\nJust to check that against the numpy library:\n\nA = np.array([[-2,2,1],[-5,5,1],[-4,2,3]])\n\ndef sorted_eig(A):  # For now we sort 'by convention'. For PCA the sorting is key. \n    lambdas, vs = LA.eig(A)\n    # Next line just sorts values & vectors together in order of decreasing eigenvalues\n    lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True))\n    return lambdas, np.array(vs).T  # un-doing the list-casting from the previous line\n\nlambdas, vs = sorted_eig(A)\nlambdas # hold off on printing out the eigenvectors until we do the next part!\n\n(3.0000000000000013, 1.999999999999999, 1.0000000000000004)\n\n\nClose enough!"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#principal-component-analysis",
    "href": "posts/2019-12-21-pca-from-scratch.html#principal-component-analysis",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "“Principal Component” Analysis",
    "text": "“Principal Component” Analysis\nNow we have our components (=eigenvectors), and we have them “ranked” by their “significance.” Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will “squish” the data along the smallest-variance directions.\nFor the above 3D dataset, we’re going to squish it into a 2D pancake – by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors.\nYea, but how to do this projection?"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#projecting-the-data",
    "href": "posts/2019-12-21-pca-from-scratch.html#projecting-the-data",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Projecting the data",
    "text": "Projecting the data\nIt’s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)!\nOH WAIT! Hey, you want to see a cool trick!? Check this out:\n\nlambdas, vs = sorted_eig(cov)  \n\nproj_cov = vs.T @ cov @ vs   # project the covariance matrix, using eigenvectors\nproj_cov\n\narray([[2.23494234e+00, 1.70482998e-16, 2.02405702e-16],\n       [5.45916585e-17, 2.57703730e-01, 1.33554659e-16],\n       [2.12481941e-16, 1.73636845e-16, 4.35661297e-02]])\n\n\nWhat was THAT? Let me clean that up a bit for you…\n\nproj_cov[np.abs(proj_cov) &lt; 1e-15] = 0\nproj_cov\n\narray([[2.23494234, 0.        , 0.        ],\n       [0.        , 0.25770373, 0.        ],\n       [0.        , 0.        , 0.04356613]])\n\n\nImportant point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a ‘coordinate system’ where different ‘directions’ decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the ‘arbitrary’ coordinates that we ‘humans’ may have assigned initially.\nThe numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z.\nSo really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we’ll literally just drop a whole index’s-worth of data-dimension in this new coordinate system. :-)\nSo, instead of \\(x\\), \\(y\\) and \\(z\\), let’s have three coordinates which (following physicist-notation) we’ll call \\(q_1\\), \\(q_2\\) and \\(q_3\\), and these will run along the directions given by the three eigenvectors.\nLet’s write our data as a N-by-3 matrix, where N is the number of data points we have.\n\ndata = np.stack((x,y,z),axis=1)\ndata.shape   # we had a 100 data points, so expecting 100x3 matrix \n\n(100, 3)\n\n\nThere are two ways of doing this, and I’ll show you that they’re numerically equivalent:\n\nUse all the eigenvectors to “rotate” the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data.\nTruncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors.\n\nLet’s show them both:\n\nprint(\"\\n 1. All data, rotated into new coordinate system\")\nW = vs[:,0:3]  # keep the all the eigenvectors\nnew_data_all = data @ W  # project all the data \nprint(\"Checking: new_data_all.shape =\",new_data_all.shape)\nprint(\"New covariance matrix = \\n\",np.cov(new_data_all.T) )\n\n\nprint(\"\\n 2. Truncated data projected onto principal axes of coordinate system\")\n\nW = vs[:,0:2]  # keep only the first and 2nd eigenvectors \nprint(\"W.shape = \",W.shape) \nnew_data_proj = data @ W   # project \nprint(\"Checking: new_data_proj.shape =\",new_data_proj.shape)\nprint(\"New covariance matrix in projected space = \\n\",np.cov(new_data_proj.T) )\n\n# Difference between them\ndiff = new_data_all[:,0:2] - new_data_proj\nprint(\"\\n Absolute maximum difference between the two methods = \",np.max(np.abs(diff)))\n\n\n 1. All data, rotated into new coordinate system\nChecking: new_data_all.shape = (100, 3)\nNew covariance matrix = \n [[2.23494234e+00 4.35665465e-17 3.01388743e-16]\n [4.35665465e-17 2.57703730e-01 9.62562340e-17]\n [3.01388743e-16 9.62562340e-17 4.35661297e-02]]\n\n 2. Truncated data projected onto principal axes of coordinate system\nW.shape =  (3, 2)\nChecking: new_data_proj.shape = (100, 2)\nNew covariance matrix in projected space = \n [[2.23494234e+00 4.35665465e-17]\n [4.35665465e-17 2.57703730e-01]]\n\n Absolute maximum difference between the two methods =  0.0\n\n\n…Nada. The 2nd method will be faster computationally though, because it doesn’t calculate stuff you’re going to throw away.\nOne more comparison between the two methods. Let’s take a look at the “full” dataset (in blue) vs. the projected dataset (in red):\n\nfig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2],\n        mode='markers', marker=dict(size=4,opacity=0.5), name=\"full data\" ))])\nfig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0,\n        mode='markers', marker=dict(size=4,opacity=0.5), name=\"projected\" ) )\nfig.update_layout(scene_aspectmode='data')\nfig.show()\n\n\n                                                \n\n\n(Darn it, if only Plot.ly would support orthographic projections [2] it’d be a lot easier to visually compare the two datasets!)"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#how-do-you-know-how-many-dimensions-to-throw-out",
    "href": "posts/2019-12-21-pca-from-scratch.html#how-do-you-know-how-many-dimensions-to-throw-out",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "How do you know how many dimensions to throw out?",
    "text": "How do you know how many dimensions to throw out?\nIn other words, how many ‘components’ should you choose to keep when doing PCA? There are a few ways to make this judgement call – it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an ‘elbow’ at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we’ll make such a graph. For more on this topic, see this post by Arthur Gonsales."
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#example-handwritten-digits",
    "href": "posts/2019-12-21-pca-from-scratch.html#example-handwritten-digits",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Example: Handwritten Digits",
    "text": "Example: Handwritten Digits\nThe scikit-learn library uses this as an example and I like it. It goes as follows: 1. Take a dataset of tiny 8x8 pixel images of handwritten digits. 2. Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. 3. Show on a plot how the different digits cluster in different regions of the space. 4. (This part we’ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.)\nTo be clear: In what follows, each pixel of the image counts as a “feature,” i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions.\n\nfrom sklearn.datasets import load_digits \nfrom sklearn.decomposition import PCA\ndigits  = load_digits()\nX = digits.data / 255.0 \nY = digits.target\nprint(X.shape, Y.shape,'\\n')\n\n# Let's look a a few examples\nfor i in range(8):  # show 8 examples \n    print(\"This is supposed to be a '\",Y[i],\"':\",sep=\"\")\n    plt.imshow(X[i].reshape([8,8]))\n    plt.show()\n\n(1797, 64) (1797,) \n\nThis is supposed to be a '0':\nThis is supposed to be a '1':\nThis is supposed to be a '2':\nThis is supposed to be a '3':\nThis is supposed to be a '4':\nThis is supposed to be a '5':\nThis is supposed to be a '6':\nThis is supposed to be a '7':\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s do the PCA thang… First we’ll try going down to 2 dimensions. This isn’t going to work super great but we’ll try:\n\ndigits_cov = np.cov(X.T)\nprint(\"digits_cov.shape = \",digits_cov.shape)\nlambdas, vs = sorted_eig(np.array(digits_cov)) \n\nW = vs[:,0:2]  # just keep two dimensions\nproj_digits = X @ W\nprint(\"proj_digits.shape = \", proj_digits.shape)\n\n# Make the plot \nfig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2],\n                mode='markers', marker=dict(size=6, opacity=0.7, color=Y), text=['digit='+str(j) for j in Y] )])\nfig.update_layout( xaxis_title=\"q_1\", yaxis_title=\"q_2\", yaxis = dict(scaleanchor = \"x\",scaleratio = 1) )\nfig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5)))\nfig.show()\n\ndigits_cov.shape =  (64, 64)\nproj_digits.shape =  (1797, 2)\n\n\n\n                                                \n\n\nThis is ‘sort of ok’: There are some regions that are mostly one kind of digit. But you may say there’s there’s too much intermingling between classes for a lot of this plot. So let’s try it again with 3 dimensions for PCA:\n\nW = vs[:,0:3]  # just three dimensions\nproj_digits = X @ W\nprint(\"proj_digits.shape = \", proj_digits.shape)\n\n# Make the plot, separate them by \"z\" which is the digit of interest.  \nfig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2],\n                mode='markers', marker=dict(size=4, opacity=0.8, color=Y, showscale=True), \n                text=['digit='+str(j) for j in Y] )])\nfig.update_layout(title=\"8x8 Handwritten Digits\", xaxis_title=\"q_1\", yaxis_title=\"q_2\", yaxis = dict(scaleanchor = \"x\",scaleratio = 1) )\nfig.show()\n\nproj_digits.shape =  (1797, 3)\n\n\n\n                                                \n\n\nNow we can start to see some definition! The 6’s are pretty much in one area, the 2’s are in another area, and the 0’s are in still another, and so on. There is some intermingling to be sure (particularly between the 5’s and 8’s), but you can see that this ‘kind of’ gets the job done, and instead of dealing with 64 dimensions, we’re down to 3!"
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#graphing-variance-vs.-components",
    "href": "posts/2019-12-21-pca-from-scratch.html#graphing-variance-vs.-components",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Graphing Variance vs. Components",
    "text": "Graphing Variance vs. Components\nEarlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the ‘significance’ that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a ‘kink’ or ‘elbow’ in the graph as a place to truncate our representation…\n\nplt.plot( np.abs(lambdas)/np.sum(lambdas) )\nplt.xlabel('Number of components')\nplt.ylabel('Significance') \nplt.show()\n\n\n\n\n…So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the ‘elbow’ in the graph lies."
  },
  {
    "objectID": "posts/2019-12-21-pca-from-scratch.html#further-reading",
    "href": "posts/2019-12-21-pca-from-scratch.html#further-reading",
    "title": "Principal Component Analysis (PCA) from Scratch",
    "section": "Further reading",
    "text": "Further reading\nThere are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following…\n\nExamples & links\n\n“Eigenstyle: Principal Component Analysis and Fashion” by Grace Avery. Uses PCA on Fashion-MNIST. It’s good!\nNeat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone.\nAndrew Ng’s Machine Learning Course, Lecture on PCA. How I first learned about this stuff.\nPCA using Python by Michael Galarnyk. Does similar things to what I’ve done here, although maybe better!\nPlot.ly PCA notebook examples"
  },
  {
    "objectID": "posts/2017-02-23-Following-Gravity-Colab.html",
    "href": "posts/2017-02-23-Following-Gravity-Colab.html",
    "title": "Following Gravity - ML Foundations Part I",
    "section": "",
    "text": "header_img\nImage credit: NASA"
  },
  {
    "objectID": "posts/2017-02-23-Following-Gravity-Colab.html#optimization-basics-gradient-descent",
    "href": "posts/2017-02-23-Following-Gravity-Colab.html#optimization-basics-gradient-descent",
    "title": "Following Gravity - ML Foundations Part I",
    "section": "Optimization Basics: Gradient Descent",
    "text": "Optimization Basics: Gradient Descent\nLet’s put the “sample problem” aside for now, and talk about the general problem of optimization. Often we may wish to minimize some function \\(f(x)\\). In science, doing so may enable us to fit a curve to our data, as we’ll do below. Similarly,‘machine learning’ systems often operate on the basis of minimizing a ‘cost’ function to discern patterns in complex datasets.\nThus we want to find the value of \\(x\\) for which \\(f(x)\\) is the smallest. A graph of such a function might look like this…\n(Python code follows, to make the graph)\n\nimport numpy as np, matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'f'})\nx = np.arange(-5,7,0.1)\nax.plot(x,(x-1)**2+1)\nplt.show()\n\n\n\n\nIf \\(f(x)\\) is differentiable and the derivative (i.e., slope) \\(df/dx\\) can be evaluated easily, then we can perform a so-called “gradient descent”.\nWe do so as follows:\n\nStart with some initial guess for \\(x\\)\n“Go in the direction of \\(-df/dx\\)”: \\[x_{new} = x_{old} - \\alpha {df\\over dx},\\] where \\(\\alpha\\) is some parameter often called the “learning rate”. All this equation is saying is, “If the function is increasing, then move to the left; and if the function is decreasing then move to the right.” The actual change to \\(x\\) is given by \\(\\Delta x \\equiv - \\alpha (df/dx)\\).\n\nRepeat step 2 until some approximation criterion is met.\n\nA nice feature of this method is that as \\(df/dx \\rightarrow 0\\), so too \\(\\Delta x\\rightarrow0\\). So an “adaptive stepsize” is built-in.\nNow let’s try this out with some Python code…\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\n\ndef f(x):\n    return (x-1)**2+1\n\ndef dfdx(x):\n    return 2*(x-1)\n\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'f'})\nx = np.arange(-5,7,0.1)\nax.plot(x,f(x),ls='dashed')\n\nfor alpha in ([0.002,0.1,0.25,0.8]):\n    print(\"alpha = \",alpha)\n    x = -5                           # starting point\n    x_arr = [x]\n    y_arr = [f(x)]\n    maxiter = 50\n    for iter in range(maxiter):      # do the descent\n        # these two lines are just for plotting later\n        x_arr.append(x)\n        y_arr.append( f(x) )\n\n        # Here's the important part: update via gradient descent\n        x = x - alpha * dfdx(x)\n\n    # report and make the plot\n    print(\"              final x = \",x)\n    ax.plot(x_arr,y_arr,'o-',label=\"alpha = \"+str(alpha))\n\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.show()\n\nalpha =  0.002\n              final x =  -3.910414704056598\nalpha =  0.1\n              final x =  0.9999143651384377\nalpha =  0.25\n              final x =  0.9999999999999947\nalpha =  0.8\n              final x =  0.999999999951503\n\n\n\n\n\nNotice how the larger learning rate (\\(\\alpha\\)=0.8) meant that the steps taken were so large that they “overshot” the minimum, whereas the too-small learning rate (\\(\\alpha=0.002\\)) still hadn’t come anywhere close to the minimum before the maximum iteration was reached.\nExercise: Experiment by editing the above code: Try different learning rates and observe the behavior.\n\nChallenge: Instability\nYou may have noticed, if you made the learning rate too large, that the algorithm does not converge to the solution but instead ‘blows up’. This is the ‘flip side’ of the ‘adaptive step size’ feature of this algorithm: If you jump “across” the minimum to the other side and end up a greater distance from the minimum that where you started, you will encounter an even larger gradient, which will lead to an even larger \\(\\Delta x\\), and so on.\nWe can see this with the same code from before, let’s just use a different starting point and a step size that’s clearly too large…\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\n\ndef f(x):\n    return (x-1)**2+1\n\ndef dfdx(x):\n    return 2*(x-1)\n\nalpha = 1.1                     # \"too big\" learning rate\n\nprint(\"alpha = \",alpha)\nx = -1                           # starting point\nx_arr = []\ny_arr = []\nmaxiter = 12\nfor iter in range(maxiter):      # do the descent\n    x_arr.append(x)\n    y_arr.append( f(x) )\n    x = x - alpha * dfdx(x)\n\n# report and make the plot\nprint(\"              final x = \",x)\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'f'})\nplt.plot(x_arr,y_arr,'r',zorder=2,)\nplt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis)\nxlim = ax.get_xlim()                   # find out axis limits\nx = np.arange(xlim[0],xlim[1],1)       # dashed line\nplt.plot(x,f(x),zorder=1,ls='dashed')\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.show()\n\nalpha =  1.1\n              final x =  -16.83220089651204\n\n\n\n\n\nIn the above plot, we colored the points by iteration number, starting with the dark purple at the initial location of x=-1, and bouncing around ever-farther from the solution as the color changes to yellow. As this happens, the error is growing exponentially; this is one example of a numerical instability. Thus, this algorithm is not entirely stable.\n\n\n\nnot_entirely_stable\n\n\nOne way to guard against this to check: is our value of \\(f(x)\\) at the current iteration larger than the value it was at the previous iteration? If so, that’s a sign that our learning rate is too large, and we can use this criterion to dynamically adjust the learning rate.\nLet’s add some ‘control’ code to that effect, to the previous script, and also print out the values of the relevant variables so we can track the progress:\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\n\ndef f(x):\n    return (x-1)**2+1\n\ndef dfdx(x):\n    return 2*(x-1)\n\n\nalpha = 13.0                     # \"too big\" learning rate\n\nprint(\"alpha = \",alpha)\nx = -1                           # starting point\nx_arr = []\ny_arr = []\nmaxiter = 20\nf_old = 1e99   # some big number\nfor iter in range(maxiter):      # do the descent\n    # these two lines are just for plotting later\n    x_arr.append(x)\n\n    f_cur = f(x)\n    y_arr.append( f_cur )\n    \n    print(\"iter = \",iter,\"x = \",x,\"f(x) =\",f(x),\"alpha = \",alpha)\n    \n    if (f_cur &gt; f_old):         # check for runaway behavior\n        alpha = alpha * 0.5\n        print(\" decreasing alpha. new alpha = \",alpha)\n    f_old = f_cur\n    \n    # update via gradient descent\n    x = x - alpha * dfdx(x)\n\n# report and make the plot\nprint(\"              final x = \",x)\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'f'})\nplt.plot(x_arr,y_arr,'r',zorder=2,)\nplt.scatter(x_arr,y_arr,zorder=3,c=range(len(x_arr)),cmap=plt.cm.viridis)\nxlim = ax.get_xlim()\nx = np.arange(xlim[0],xlim[1],1)          # x for dashed line\nplt.plot(x,f(x),zorder=1,ls='dashed')\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.show()\n\nalpha =  13.0\niter =  0 x =  -1 f(x) = 5 alpha =  13.0\niter =  1 x =  51.0 f(x) = 2501.0 alpha =  13.0\n decreasing alpha. new alpha =  6.5\niter =  2 x =  -599.0 f(x) = 360001.0 alpha =  6.5\n decreasing alpha. new alpha =  3.25\niter =  3 x =  3301.0 f(x) = 10890001.0 alpha =  3.25\n decreasing alpha. new alpha =  1.625\niter =  4 x =  -7424.0 f(x) = 55130626.0 alpha =  1.625\n decreasing alpha. new alpha =  0.8125\niter =  5 x =  4641.625 f(x) = 21535401.390625 alpha =  0.8125\niter =  6 x =  -2899.390625 f(x) = 8412266.77758789 alpha =  0.8125\niter =  7 x =  1813.744140625 f(x) = 3286042.31937027 alpha =  0.8125\niter =  8 x =  -1131.965087890625 f(x) = 1283610.8903790116 alpha =  0.8125\niter =  9 x =  709.1031799316406 f(x) = 501411.1134293014 alpha =  0.8125\niter =  10 x =  -441.5644874572754 f(x) = 195864.32555832085 alpha =  0.8125\niter =  11 x =  277.6028046607971 f(x) = 76510.1115462191 alpha =  0.8125\niter =  12 x =  -171.8767529129982 f(x) = 29887.37169774183 alpha =  0.8125\niter =  13 x =  109.04797057062387 f(x) = 11675.363944430403 alpha =  0.8125\niter =  14 x =  -66.52998160663992 f(x) = 4561.298415793126 alpha =  0.8125\niter =  15 x =  43.20623850414995 f(x) = 1782.36656866919 alpha =  0.8125\niter =  16 x =  -25.37889906509372 f(x) = 696.8463158864023 alpha =  0.8125\niter =  17 x =  17.486811915683575 f(x) = 272.8149671431259 alpha =  0.8125\niter =  18 x =  -9.304257447302234 f(x) = 107.17772154028356 alpha =  0.8125\niter =  19 x =  7.440160904563896 f(x) = 42.47567247667326 alpha =  0.8125\n              final x =  -3.025100565352435\n\n\n\n\n\nSo in the preceding example, we start at \\(x=-1\\), then the unstable behavior starts and we begin diverging from the minimum, so we decrease \\(\\alpha\\) as often as our criterion tells us to. Finally \\(\\alpha\\) becomes low enought to get the system ‘under control’ and the algorithm enters the convergent regime.\nExercise: In the example above, we only decrease \\(\\alpha\\) by a factor of 2 each time, but it would be more efficient to decrease by a factor of 10. Try that and observe the behavior of the system.\nYou may say, “Why do I need to worry about this instability stuff? As long as \\(\\alpha&lt;1\\) the system will converge, right?” Well, for this simple system it seems obvious what needs to happen, but with multidimensional optimization problems (see below), it’s not always obvious what to do. (Sometimes different ‘dimensions’ need different learning rates.) This simple example serves as an introduction to phenomena which arise in more complex situations.\n\n\nChallenge: Non-global minima\nTo explore more complicated functions, we’re going to take advantage of the SymPy package, to let it take derivatives for us. Try executing the import in the next cell, and if nothing happens it means you have SymPy installed. If you get an error, you may need to go into a Terminal and run “pip install sympy”.\n\nimport sympy\n\nYou’re good? No errors? Ok, moving on…\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\nfrom sympy import Symbol, diff\n\nx = Symbol('x')\n\n# our function, more complicated (SymPy handles it!)\nf = (x-1)**4 - 20*(x-1)**2 + 10*x + 1\ndfdx = diff(f,x)\n\n# setup\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'f'})\nx_arr = np.arange(-5,7,0.1)\ny_arr = np.copy(x_arr)\nfor i, val in enumerate(x_arr):\n    y_arr[i] = f.evalf(subs={x:val})\nax.plot(x_arr,y_arr,ls='dashed')   # space of 'error function'\n\n# for a variety of learning rates...\nfor alpha in ([0.002,0.01,0.03]): \n    print(\"alpha = \",alpha)\n    xval = 6                     # starting point\n    x_arr = [xval]\n    y_arr = [f.evalf(subs={x:xval})]\n\n    maxiter = 50\n    # do the descent\n    for iter in range(maxiter):\n\n        # these two lines are just for plotting later\n        x_arr.append(xval)\n        y_arr.append( f.evalf(subs={x:xval}) )\n\n        # update via gradient descent\n        xval = xval - alpha * dfdx.evalf(subs={x:xval})\n        \n    print(\"              final xval = \",xval)\n    ax.plot(x_arr,y_arr,'o-',label=\"alpha = \"+str(alpha))\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.show()\n\nalpha =  0.002\n              final xval =  4.02939564594151\nalpha =  0.01\n              final xval =  4.02896613891181\nalpha =  0.03\n              final xval =  -2.00328879556504\n\n\n\n\n\nAll the runs start at \\(x=6\\). Notice how the runs marked in organge and green go on to find a “local” minimum, but they don’t find the “global” minimum (the overall lowest point) like the run marked in red does. The problem of ending up at non-global local minima is a generic problem for all kinds of optimization tasks. It tends to get even worse when you add more parameters…\n\n\nMultidimensional Gradient Descent\n(A descent into darkness…)\nLet’s define a function of two variables, that’s got at least one minimum in it. We’ll choose \\[f(x,y) = -\\left( \\cos x + 3\\cos y \\right) /2,\\] which actually has infinitely many minima, but we’ll try to ‘zoom in’ on just one.\nWe can vizualize this function via the graph produced by the code below; in the graph, darker areas show lower values than ligher areas, and there is a minimum at the point \\(x=0,y=0\\) where \\(f(0,0)=-2\\).\n\nimport numpy as np, matplotlib.pyplot as plt\ndef f(x,y):\n    return -( np.cos(x) + 3*np.cos(y) )/2\nx = y = np.linspace(-4, 4, 100)\nz = np.zeros([len(x), len(y)])\nfor i in range(len(x)):\n    for j in range(len(y)):\n        z[j, i] = f(x[i], y[j])\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'y'})\ncs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot)\nplt.gca().set_aspect('equal', adjustable='box')\ncbar = fig.colorbar(cs, orientation='vertical')\nplt.show()\n\n\n\n\nThe way we find a minimum is similar to what we did before, except we use partial derivatives in the x- and y-directions:\n\\[x_{new} = x_{old} + \\Delta x,\\ \\ \\ \\ \\ \\ \\Delta x = - \\alpha {\\partial f\\over \\partial x}  \\] \\[y_{new} = y_{old} + \\Delta y,\\ \\ \\ \\ \\ \\ \\Delta y = - \\alpha {\\partial f\\over \\partial y},\\]\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\n\n# our function\ndef f(x,y):\n    return -( np.cos(x) + 3*np.cos(y) )/2\ndef dfdx(x,y):\n    return np.sin(x)/2\ndef dfdy(x,y):\n    return 3*np.sin(y)/2\n\n# variables for this run\nalpha = 0.5 \nxval, yval = 2.5, 1.5       # starting guess(es)\nx_arr = []\ny_arr = []\n\nmaxiter = 20\nfor iter in range(maxiter):  # gradient descent loop\n    x_arr.append(xval)\n    y_arr.append(yval)\n\n    xval = xval - alpha * dfdx(xval,yval)\n    yval = yval - alpha * dfdy(xval,yval)\n\nprint(\"Final xval, yval = \",xval,yval,\".  Target is (0,0)\")\n\n\n# background image: plot the color background\nx = y = np.linspace(-4, 4, 100)\nz = np.zeros([len(x), len(y)])\nfor i in range(len(x)):\n    for j in range(len(y)):\n        z[j, i] = f(x[i], y[j])\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'y'})\ncs = ax.pcolor(x, y, z, cmap=plt.cm.afmhot)\nplt.gca().set_aspect('equal', adjustable='box')\ncbar = fig.colorbar(cs, orientation='vertical')\n\n# plot the progress of our optimization\nplt.plot(x_arr,y_arr,zorder=1)\nplt.scatter(x_arr,y_arr,zorder=2,c=range(len(x_arr)),cmap=plt.cm.viridis)\nhandles, labels = ax.get_legend_handles_labels()\nplt.show()\n\nFinal xval, yval =  0.0272555602238 3.59400699273e-12 .  Target is (0,0)\n\n\n\n\n\nIn the above figure, we’ve shown the ‘path’ the algorithm takes in \\(x\\)-\\(y\\) space, coloring the dots according to iteration number, so that the first points are dark purple, and later points tend to yellow.\nNote that due to the asymmetry in the function (between \\(x\\) and \\(y\\)), the path descends rapidly in \\(y\\), and then travels along the “valley” in \\(x\\) to reach the minimum. This “long narrow valley” behavior is common in multidimensional optimization problems: the system may ‘solve’ one parameter quickly, but require thousands of operations to find the other one.\nMany sophisticated schemes have arisen to handle this challenge, and we won’t cover them here. For now, suffice it to say that, yes, this sort of thing happens. You may have ‘found’ highly accurate values for certain parameters, but others are bogging down the process of convergence.\nNext time, we’ll cover a common application of optimization: Least Squares Regression…"
  },
  {
    "objectID": "posts/2017-02-23-Following-Gravity-Colab.html#least-squares-regression",
    "href": "posts/2017-02-23-Following-Gravity-Colab.html#least-squares-regression",
    "title": "Following Gravity - ML Foundations Part I",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\nThis is such a common thing to do in science and statistics, that everyone should learn how it works. We’ll do it for linear relationships, but it generalizes to nonlinear situations as well.\n\nHow to Fit a Line\nLet’s say we’re trying to fit a line to a bunch of data. We’ve been given \\(n\\) data points with coordinates \\((x_i,y_i)\\) where \\(i=1..n\\). The problem becomes, given a line \\(f(x) = mx+b\\), find the values of the parameters \\(m\\) and \\(b\\) which minimize the overall “error”.\n\nadd some kinda picture here?\nThe error can take many forms; one is the squared error \\(SE\\), which is just the sum of the squares of the “distances” between each data point’s \\(y\\)-value and the “guess” from the line fit \\(f\\) at each value of \\(x\\):\n\\[ SE = (f(x_1) - y_1)^2 + (f(x_2) - y_2)^2 + ... (f(x_n)-y_n)^2,\\]\nWe can write this concisely as \\[ SE = \\sum_{i=1}^n (f(x_i)-y_i)^2.\\]\nAnother popular form is the “mean squared error” \\(MSE\\), which is just \\(SE/n\\):\n\\[ MSE = {1\\over n}\\sum_{i=1}^n (f(x_i)-y_i)^2.\\]\nThe MSE has the nice feature that as you add more data points, it tends to hold a more-or-less consistent value (as opposed to the SE which gets bigger as you add more points). We’ll use the MSE in the work that follows.\nSo expanding out \\(f(x)\\), we see that the MSE is a function of \\(m\\) and \\(b\\), and these are the parameters we’ll vary to minimize the MSE: \\[ MSE(m,b) = {1\\over n}\\sum_{i=1}^n (mx_i+b-y_i)^2.\\]\nSo, following our earlier work on multidimensional optimization, we start with guesses for \\(m\\) and \\(b\\) and then update according to gradient descent:\n\\[m_{new} = m_{old} + \\Delta m,\\ \\ \\ \\ \\ \\ \\Delta m = -\\alpha{\\partial (MSE)\\over\\partial m} = -\\alpha{2\\over n}\\sum_{i=1}^n (mx_i+b-y_i)(x_i) \\] \\[b_{new} = b_{old} + \\Delta b,\\ \\ \\ \\ \\ \\ \\Delta b =  -\\alpha{\\partial (MSE)\\over\\partial b} = -\\alpha{2\\over n}\\sum_{i=1}^n (mx_i+b-y_i)(1).\\]\nSo, to start off, let’s get some data…\n\n# Set up the input data\nn = 20\nnp.random.seed(1)               # for reproducability \nx_data = np.random.uniform(size=n)   # random points for x\nm_exact = 2.0\nb_exact = 1.5\ny_data = m_exact * x_data + b_exact\ny_data += 0.3*np.random.normal(size=n)   # add noise\n\n# Plot the data\ndef plot_data(x_data, y_data, axis_labels=('x','y'), zero_y=False):\n    fig, ax = plt.subplots()\n    ax.update({'xlabel':axis_labels[0], 'ylabel':axis_labels[1]})\n    ax.plot(x_data, y_data,'o')\n    if (zero_y):\n        ax.set_ylim([0,ax.get_ylim()[1]*1.1])\n    plt.show()\n    \nplot_data(x_data,y_data, zero_y=True)\n\n\n\n\nNote: in contrast to earlier parts of this document which include complete python programs in every code post, for brevity’s sake we will start using the notebook “as intended”, relying on the internal state and adding successive bits of code which make use of the “memory” of previously-defined variables.\nLet’s map out the MSE for this group of points, as a function of possible \\(m\\) and \\(b\\) values…\n\n# map out the MSE for various values of m and b\ndef MSE(x,y,m,b):\n    # Use Python array operations to compute sums\n    return ((m*x + b - y)**2).mean()  \n\nmm = bb = np.linspace(0, 4, 50)\nz = np.zeros([len(mm), len(bb)])\nfor i in range(len(mm)):\n    for j in range(len(bb)):\n        z[j, i] = MSE(x_data,y_data, mm[i],bb[j])\n        \nfig, ax = plt.subplots()\nax.update({'xlabel':'m', 'ylabel':'b'})\ncs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\nWe see the minimum near the “exact” values chosen in the begininng. (Note that we’ve plotted the logarithm of the MSE just to make the colors stand out better.)\nNext we will choose starting guesses for \\(m\\) and \\(b\\), and use gradient descent to fit the line…\n\nm = 3.5         # initial guess\nb = 3.5 \nm_arr = []\nb_arr = []\n\ndef dMSEdm(x,y,m,b):\n    return (2*(m*x + b - y) *x).mean()\n\ndef dMSEdb(x,y,m,b):\n    return (2*(m*x + b - y)).mean()\n\nalpha = 0.1\nmaxiter, printevery = 500, 4\nfor iter in range(maxiter):\n    m_arr.append(m)\n    b_arr.append(b)\n    if (0 == iter % printevery):\n        print(iter,\": b, m = \",b,m,\", MSE = \",MSE(x_data,y_data,m,b))\n    \n    m = m - alpha * dMSEdm(x_data,y_data,m,b)\n    b = b - alpha * dMSEdb(x_data,y_data,m,b)\n    \nprint(\"Final result: m = \",m,\", b = \",b)\n\n# background image: plot the color background (remembered from before)\nfig, ax = plt.subplots()\nax.update({'xlabel':'m', 'ylabel':'b'})\ncs = ax.pcolor(mm, bb, np.log(z), cmap=plt.cm.afmhot)\nplt.gca().set_aspect('equal', adjustable='box')\n\n# plot the progress of our descent\nplt.plot(m_arr,b_arr,zorder=1)\nplt.scatter(m_arr,b_arr,zorder=2,c=range(len(m_arr)),cmap=plt.cm.viridis)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.show()\n\n0 : b, m =  3.5 3.5 , MSE =  6.86780331186\n4 : b, m =  2.07377614457 2.89890882764 , MSE =  0.98222306593\n8 : b, m =  1.55966423863 2.66310750082 , MSE =  0.194874325956\n12 : b, m =  1.37811928553 2.56128194633 , MSE =  0.0877947061277\n16 : b, m =  1.31767685375 2.50899769214 , MSE =  0.0718728682069\n20 : b, m =  1.30118421505 2.47541762467 , MSE =  0.0683627241086\n24 : b, m =  1.30049838878 2.44926336309 , MSE =  0.0666665839706\n28 : b, m =  1.30535957938 2.4263945595 , MSE =  0.0653286635918\n32 : b, m =  1.3120331485 2.40527655618 , MSE =  0.0641346050201\n36 : b, m =  1.31916510087 2.38532649645 , MSE =  0.0630441987699\n40 : b, m =  1.32626977167 2.36630978168 , MSE =  0.0620438837066\n44 : b, m =  1.33317800067 2.34811979986 , MSE =  0.0611251920416\n48 : b, m =  1.33983578596 2.33069751134 , MSE =  0.0602811821983\n52 : b, m =  1.34623082683 2.31400206963 , MSE =  0.059505695069\n56 : b, m =  1.35236573256 2.29800006645 , MSE =  0.0587931380437\n60 : b, m =  1.35824825911 2.28266157418 , MSE =  0.0581383942813\n64 : b, m =  1.36388775858 2.26795866993 , MSE =  0.0575367695918\n68 : b, m =  1.3692938954 2.2538648666 , MSE =  0.0569839532221\n72 : b, m =  1.374476189 2.2403548761 , MSE =  0.0564759850457\n76 : b, m =  1.37944385765 2.22740449497 , MSE =  0.0560092265247\n80 : b, m =  1.3842057718 2.21499053585 , MSE =  0.0555803344137\n84 : b, m =  1.38877044688 2.20309077677 , MSE =  0.0551862367311\n88 : b, m =  1.39314605011 2.19168391801 , MSE =  0.0548241107275\n92 : b, m =  1.39734041211 2.18074954278 , MSE =  0.0544913626571\n96 : b, m =  1.4013610397 2.17026808021 , MSE =  0.054185609197\n100 : b, m =  1.40521512901 2.16022077017 , MSE =  0.0539046603748\n104 : b, m =  1.40890957817 2.1505896296 , MSE =  0.0536465038826\n108 : b, m =  1.41245099963 2.14135742035 , MSE =  0.0534092906637\n112 : b, m =  1.41584573193 2.1325076183 , MSE =  0.0531913216685\n116 : b, m =  1.41909985108 2.12402438375 , MSE =  0.0529910356848\n120 : b, m =  1.42221918141 2.11589253313 , MSE =  0.0528069981559\n124 : b, m =  1.42520930601 2.10809751176 , MSE =  0.0526378909052\n128 : b, m =  1.42807557671 2.10062536785 , MSE =  0.052482502695\n132 : b, m =  1.43082312365 2.09346272749 , MSE =  0.0523397205507\n136 : b, m =  1.4334568645 2.08659677074 , MSE =  0.0522085217891\n140 : b, m =  1.43598151321 2.08001520867 , MSE =  0.0520879666937\n144 : b, m =  1.43840158849 2.07370626138 , MSE =  0.0519771917836\n148 : b, m =  1.44072142187 2.0676586369 , MSE =  0.0518754036286\n152 : b, m =  1.44294516546 2.06186151097 , MSE =  0.051781873167\n156 : b, m =  1.44507679941 2.0563045077 , MSE =  0.0516959304829\n160 : b, m =  1.44712013898 2.05097768097 , MSE =  0.0516169600082\n164 : b, m =  1.44907884142 2.04587149665 , MSE =  0.0515443961135\n168 : b, m =  1.45095641248 2.04097681551 , MSE =  0.0514777190569\n172 : b, m =  1.45275621269 2.03628487687 , MSE =  0.0514164512613\n176 : b, m =  1.45448146341 2.03178728296 , MSE =  0.0513601538933\n180 : b, m =  1.45613525255 2.0274759838 , MSE =  0.0513084237208\n184 : b, m =  1.45772054011 2.0233432629 , MSE =  0.0512608902243\n188 : b, m =  1.4592401635 2.01938172337 , MSE =  0.051217212943\n192 : b, m =  1.4606968426 2.0155842747 , MSE =  0.0511770790366\n196 : b, m =  1.46209318462 2.01194412009 , MSE =  0.0511402010444\n200 : b, m =  1.46343168877 2.00845474426 , MSE =  0.0511063148261\n204 : b, m =  1.46471475077 2.00510990182 , MSE =  0.0510751776703\n208 : b, m =  1.46594466707 2.00190360604 , MSE =  0.0510465665558\n212 : b, m =  1.46712363904 1.99883011819 , MSE =  0.0510202765543\n216 : b, m =  1.46825377682 1.99588393722 , MSE =  0.0509961193626\n220 : b, m =  1.46933710318 1.99305978998 , MSE =  0.0509739219537\n224 : b, m =  1.4703755571 1.99035262169 , MSE =  0.0509535253378\n228 : b, m =  1.47137099724 1.98775758697 , MSE =  0.0509347834233\n232 : b, m =  1.47232520527 1.98527004115 , MSE =  0.0509175619703\n236 : b, m =  1.47323988906 1.98288553192 , MSE =  0.0509017376297\n240 : b, m =  1.47411668575 1.98059979141 , MSE =  0.0508871970587\n244 : b, m =  1.47495716466 1.97840872852 , MSE =  0.0508738361101\n248 : b, m =  1.4757628301 1.97630842161 , MSE =  0.0508615590853\n252 : b, m =  1.47653512409 1.97429511148 , MSE =  0.0508502780498\n256 : b, m =  1.47727542891 1.97236519463 , MSE =  0.0508399122026\n260 : b, m =  1.47798506956 1.97051521684 , MSE =  0.050830387298\n264 : b, m =  1.47866531621 1.96874186695 , MSE =  0.0508216351133\n268 : b, m =  1.47931738637 1.96704197096 , MSE =  0.0508135929607\n272 : b, m =  1.47994244714 1.96541248634 , MSE =  0.050806203238\n276 : b, m =  1.48054161728 1.96385049656 , MSE =  0.0507994130159\n280 : b, m =  1.4811159692 1.96235320595 , MSE =  0.0507931736592\n284 : b, m =  1.4816665309 1.96091793458 , MSE =  0.0507874404782\n288 : b, m =  1.4821942878 1.95954211357 , MSE =  0.0507821724088\n292 : b, m =  1.48270018448 1.95822328041 , MSE =  0.0507773317183\n296 : b, m =  1.48318512642 1.95695907462 , MSE =  0.0507728837349\n300 : b, m =  1.4836499816 1.95574723348 , MSE =  0.0507687965998\n304 : b, m =  1.48409558201 1.954585588 , MSE =  0.0507650410387\n308 : b, m =  1.48452272522 1.95347205901 , MSE =  0.0507615901523\n312 : b, m =  1.48493217573 1.95240465348 , MSE =  0.0507584192235\n316 : b, m =  1.4853246664 1.95138146095 , MSE =  0.0507555055402\n320 : b, m =  1.48570089973 1.95040065006 , MSE =  0.0507528282332\n324 : b, m =  1.48606154909 1.94946046531 , MSE =  0.0507503681262\n328 : b, m =  1.48640726001 1.94855922395 , MSE =  0.0507481075984\n332 : b, m =  1.48673865124 1.94769531289 , MSE =  0.0507460304589\n336 : b, m =  1.48705631592 1.94686718587 , MSE =  0.0507441218299\n340 : b, m =  1.48736082261 1.9460733607 , MSE =  0.0507423680409\n344 : b, m =  1.48765271634 1.94531241654 , MSE =  0.0507407565302\n348 : b, m =  1.48793251954 1.94458299143 , MSE =  0.0507392757554\n352 : b, m =  1.48820073302 1.94388377984 , MSE =  0.0507379151103\n356 : b, m =  1.48845783683 1.94321353027 , MSE =  0.0507366648493\n360 : b, m =  1.48870429115 1.9425710431 , MSE =  0.0507355160173\n364 : b, m =  1.48894053709 1.94195516839 , MSE =  0.0507344603858\n368 : b, m =  1.48916699749 1.9413648038 , MSE =  0.0507334903937\n372 : b, m =  1.48938407768 1.94079889271 , MSE =  0.0507325990934\n376 : b, m =  1.48959216619 1.9402564222 , MSE =  0.050731780101\n380 : b, m =  1.48979163547 1.93973642136 , MSE =  0.0507310275504\n384 : b, m =  1.48998284254 1.93923795946 , MSE =  0.0507303360514\n388 : b, m =  1.49016612963 1.93876014435 , MSE =  0.0507297006512\n392 : b, m =  1.49034182478 1.93830212081 , MSE =  0.0507291167986\n396 : b, m =  1.49051024246 1.93786306905 , MSE =  0.0507285803118\n400 : b, m =  1.49067168412 1.93744220325 , MSE =  0.0507280873482\n404 : b, m =  1.49082643871 1.93703877012 , MSE =  0.0507276343768\n408 : b, m =  1.49097478321 1.9366520476 , MSE =  0.0507272181534\n412 : b, m =  1.49111698313 1.9362813435 , MSE =  0.0507268356966\n416 : b, m =  1.491253293 1.93592599432 , MSE =  0.0507264842671\n420 : b, m =  1.49138395677 1.93558536407 , MSE =  0.0507261613477\n424 : b, m =  1.49150920832 1.93525884305 , MSE =  0.0507258646256\n428 : b, m =  1.49162927183 1.93494584685 , MSE =  0.0507255919755\n432 : b, m =  1.4917443622 1.93464581526 , MSE =  0.0507253414444\n436 : b, m =  1.4918546854 1.93435821128 , MSE =  0.050725111238\n440 : b, m =  1.49196043891 1.93408252014 , MSE =  0.0507248997074\n444 : b, m =  1.49206181201 1.93381824839 , MSE =  0.0507247053375\n448 : b, m =  1.49215898613 1.93356492304 , MSE =  0.0507245267361\n452 : b, m =  1.4922521352 1.93332209068 , MSE =  0.0507243626239\n456 : b, m =  1.49234142595 1.93308931667 , MSE =  0.0507242118256\n460 : b, m =  1.49242701819 1.93286618439 , MSE =  0.0507240732609\n464 : b, m =  1.49250906511 1.93265229447 , MSE =  0.0507239459375\n468 : b, m =  1.49258771357 1.93244726408 , MSE =  0.0507238289434\n472 : b, m =  1.49266310433 1.93225072625 , MSE =  0.0507237214405\n476 : b, m =  1.49273537233 1.93206232921 , MSE =  0.050723622659\n480 : b, m =  1.49280464692 1.93188173576 , MSE =  0.0507235318913\n484 : b, m =  1.49287105209 1.93170862267 , MSE =  0.0507234484872\n488 : b, m =  1.49293470669 1.9315426801 , MSE =  0.0507233718493\n492 : b, m =  1.49299572466 1.93138361102 , MSE =  0.0507233014288\n496 : b, m =  1.4930542152 1.93123113075 , MSE =  0.0507232367213\nFinal result: m =  1.93108496636 , b =  1.49311028301\n\n\n\n\n\nNote that the optimized values \\((m,b)\\) that we find may not exactly match the “exact” values we used to make the data, because the noise we added to the data can throw this off. In the limit where the noise amplitude goes to zero, our optimized values will exactly match the “exact” values used to generated the data.\nLet’s see the results of our line fit…\n\n\n# plot the points\nfig, ax = plt.subplots()\nax.update({'xlabel':'x', 'ylabel':'y'})\nax.plot(x_data,y_data,'o')\nax.set_ylim([0,ax.get_ylim()[1]*1.1])\n\n# and plot the line we fit\nxlim = ax.get_xlim()\nx_line = np.linspace(xlim[0],xlim[1],2)\ny_line = m*x_line + b\nax.plot(x_line,y_line)\nplt.show()\n\n\n\n\nGreat!\n\n\n\nLeast Squares Fitting with Nonlinear Functions\nWe can generalize the technique describe above to fit polynomials \\[ f(x) = c_0 + c_1 x + c_2 x^2 + ...c_k x^k,\\] where \\(c_0...c_k\\) are the parameters we will tune, and \\(k\\) is the order of the polynomial. (Typically people use the letter \\(a\\) for polynomial coefficients, but in the math rendering of Jupter, \\(\\alpha\\) and \\(a\\) look too much alike, so we’ll use \\(c\\).) Written more succinctly, \\[ f(x) = \\sum_{j=0}^k c_j x^j.\\]\n(Indeed, we could even try non-polynomial basis functions, e.g., $ f(x) = c_0 + c_1 g(x) + c_2 h(x) + …,$ but let’s stick to polynomials for now.)\nThe key thing to note is that for each parameter \\(c_j\\), the update \\(\\Delta c_j\\) will be\n\\[\\Delta c_j = -\\alpha {\\partial (MSE)\\over \\partial c_j}\n= -\\alpha {\\partial (MSE)\\over \\partial f}{\\partial f\\over \\partial c_j}\\] \\[= -\\alpha {2\\over n}\\sum_{i=1}^n [f(x_i)-y_i](x_i)^{j} \\]\n(Note that we are not taking the derivative with respect to \\(x_i\\), but rather with respect to \\(c_j\\). Thus there is no “power rule” that needs be applied to this derivative. Also there is no sum over j.)\nThe following is a complete code for doing this, along with some added refinements:\n\n\\(\\alpha\\) is now \\(\\alpha_j\\), i.e. different learning rates for different directions\nwe initialise \\(\\alpha_j\\) such that larger powers of \\(x\\) start with smaller coefficients\nwe put the fitting code inside a method (with a bunch of parameters) so we can call it later\n\n\nfrom __future__ import print_function    # for backwards-compatibility w/ Python2\nimport numpy as np, matplotlib.pyplot as plt\n\ndef f(x,c):\n    y = 0*x                    # f will work on single floats or arrays\n    for j in range(c.size):\n        y += c[j]*(x**j)\n    return y\n\ndef polyfit(x_data,y_data, c_start=None, order=None, maxiter=500, printevery = 25,\n            alpha_start=0.9, alpha_start_power=0.3):\n    \n    # function definitions\n\n    def MSE(x_arr,y_arr,c):\n        f_arr = f(x_arr,c)\n        return ((f_arr - y_arr)**2).mean()\n\n    def dMSEdcj(x_arr,y_arr,c,j):  # deriviative of MSE wrt cj (*not* wrt x!)\n        f_arr = f(x_arr,c)\n        return ( 2* ( f_arr - y_arr) * x_arr**j ).mean()  \n\n    if ((c_start is None) and (order is None)):\n        print(\"Error: Either specify initial guesses for coefficients,\",\n              \"or specify the order of the polynomial\")\n        raise  # halt\n        \n    if c_start is not None:\n        order = c_start.size-1\n        c = np.copy(c_start)\n    elif order is not None:\n        c = np.random.uniform(size=order+1)     # random guess for starting point\n\n    assert(c.size == order+1)             #  check against conflicting info\n    k = order\n        \n    print(\"               Initial guess:    c = \" ,np.array_str(c, precision=2))\n    \n    alpha = np.ones(c.size)\n    for j in range(c.size):             # start with smaller alphas for higher powers of x\n        alpha[j] = alpha_start*(alpha_start_power)**(j)   \n\n    MSE_old = 1e99\n    for iter in range(maxiter+1):           # do the descent\n        for j in range(c.size): \n            c[j] = c[j] - alpha[j] * dMSEdcj(x_data,y_data,c,j)\n            \n            MSE_cur = MSE(x_data,y_data,c)\n            if (MSE_cur &gt; MSE_old):         # adjust if runaway behavior starts\n                alpha[j] *= 0.3\n                print(\"     Notice: decreasing alpha[\",j,\"] to \",alpha[j])\n            MSE_old = MSE_cur\n\n        if (0 == iter % printevery):        # progress log\n            print('{:4d}'.format(iter),\"/\",maxiter,\": MSE =\",'{:9.6g}'.format(MSE_cur),\n                 \", c = \",np.array_str(c, precision=3),sep='')\n    print(\"\")\n    return c\n\n# Set up input data \nn = 100 \nnp.random.seed(2)                                 # for reproducability \nx_data = np.random.uniform(-2.5,3,size=n)       # some random points for x\nc_data = np.array([-4,-3,5,.5,-2,.5])           # params to generate data (5th-degree polynomial)\ny_data = f(x_data, c_data)\ny_data += 0.02*np.random.normal(size=n)*y_data    # add a (tiny) bit of noise\n\n#---- Perform Least Squares Fit \nc = polyfit(x_data, y_data, c_start=c_data*np.random.random(), maxiter=500)\n\n#----- Plot the results\ndef plot_data_and_curve(x_data,y_data,axis_labels=('x','y'), ):\n    # plot the points\n    fig, ax = plt.subplots()\n    ax.update({'xlabel':axis_labels[0], 'ylabel':axis_labels[1]})\n    ax.plot(x_data,y_data,'o')\n\n    # and plot the curve we fit\n    xlim = ax.get_xlim()\n    x_line = np.linspace(xlim[0],xlim[1],100)\n    y_line = f(x_line, c)\n    ax.plot(x_line,y_line)\n    plt.show()\n\nplot_data_and_curve(x_data,y_data)\n\n               Initial guess:    c =  [-3.52 -2.64  4.4   0.44 -1.76  0.44]\n     Notice: decreasing alpha[ 3 ] to  0.00729\n     Notice: decreasing alpha[ 4 ] to  0.002187\n     Notice: decreasing alpha[ 5 ] to  0.0006561\n   0/500: MSE =  258.233, c = [-5.438 -1.633  4.24   0.555 -1.904  0.765]\n     Notice: decreasing alpha[ 5 ] to  0.00019683\n  25/500: MSE = 0.529541, c = [-4.265 -1.545  5.668 -0.392 -2.146  0.612]\n  50/500: MSE = 0.424417, c = [-4.304 -1.808  5.659 -0.241 -2.137  0.595]\n  75/500: MSE = 0.335586, c = [-4.256 -2.034  5.552 -0.105 -2.115  0.578]\n 100/500: MSE = 0.275848, c = [-4.212 -2.218  5.457  0.006 -2.096  0.564]\n 125/500: MSE = 0.236521, c = [-4.175 -2.367  5.38   0.096 -2.08   0.553]\n 150/500: MSE =  0.21068, c = [-4.146 -2.488  5.317  0.17  -2.068  0.544]\n 175/500: MSE = 0.193702, c = [-4.122 -2.586  5.267  0.229 -2.058  0.537]\n 200/500: MSE = 0.182549, c = [-4.103 -2.665  5.226  0.277 -2.049  0.531]\n 225/500: MSE = 0.175222, c = [-4.087 -2.73   5.192  0.316 -2.042  0.526]\n 250/500: MSE = 0.170408, c = [-4.075 -2.782  5.165  0.347 -2.037  0.522]\n 275/500: MSE = 0.167245, c = [-4.064 -2.824  5.143  0.373 -2.033  0.519]\n 300/500: MSE = 0.165167, c = [-4.056 -2.859  5.126  0.393 -2.029  0.516]\n 325/500: MSE = 0.163802, c = [-4.049 -2.886  5.111  0.41  -2.026  0.514]\n 350/500: MSE = 0.162905, c = [-4.044 -2.909  5.1    0.424 -2.024  0.513]\n 375/500: MSE = 0.162316, c = [-4.039 -2.927  5.09   0.435 -2.022  0.511]\n 400/500: MSE = 0.161929, c = [-4.036 -2.942  5.083  0.444 -2.02   0.51 ]\n 425/500: MSE = 0.161675, c = [-4.033 -2.954  5.076  0.451 -2.019  0.509]\n 450/500: MSE = 0.161508, c = [-4.031 -2.964  5.071  0.457 -2.018  0.508]\n 475/500: MSE = 0.161398, c = [-4.029 -2.972  5.067  0.462 -2.017  0.508]\n 500/500: MSE = 0.161326, c = [-4.027 -2.978  5.064  0.465 -2.017  0.507]\n\n\n\n\n\n\nNow, it turns out that polynomials are often terrible things to try to fit arbitrary data with, because they can ‘blow up’ as \\(|x|\\) increases, and this causes instability. But for a variety of physics problems (see below), polynomials can be just what we’re after. Plus, that made a nice demonstration, for now.\n(For more general functions, I actually wrote a multi-parameter SymPy gradient-descient that is completely general, but it’s terrifically slow so I won’t be posting it here. If you really want it, contact me.)"
  },
  {
    "objectID": "posts/2017-02-23-Following-Gravity-Colab.html#learning-gravity",
    "href": "posts/2017-02-23-Following-Gravity-Colab.html#learning-gravity",
    "title": "Following Gravity - ML Foundations Part I",
    "section": "Learning Gravity",
    "text": "Learning Gravity\nOk. Now we’re all we’re going to do next is fit a parabola to the motion of a falling ball – and that’s supposed to tell us something deep about physics. Sounds silly, right? ‘Everybody’ knows objects moving in a gravitational field follow parabolas (both in space & time); the more math-savvy may complain that we’re simply going to ‘get out of this’ what we ‘put into it.’\nWell, from a philosophical standpoint and from the way that these methods will generalize to other situations, there are significant implications from the methodology we’re about to follow.\nThe Challenge: Given a set of one-dimensional data of position vs. time \\(y(t)\\), can we find the underlying equation that gives rise to it? Better put, can we fit a model to it, and how well can we fit it, and what kind of model will it be anyway?\n(This is the sort of thing that statisticians do, but it’s also something physicists do, and one could argue, this is what everybody does all the time. )\nLet’s get started. I’m just going to specify y(t) at a series of \\(n+1\\) time steps \\(t_i\\) (\\(t_0\\)…\\(t_n\\)) and we’ll make them evenly spaced, and we’ll leave out any noise at all – perfect data. :-)\n\ng_exact = 9.8         # a physical parater we'll find a fit for\ndt      = 0.01\ntmax    = 1         # number of time steps\nt_data  = np.arange(0,tmax,step=dt)     # time values\nnt = t_data.size\nprint(\"dt = \",dt,\", nt = \",nt)\n\ny0      = 1.234         # initial position, choose anything\nv0      = 3.1415        # initial velocity\n\n#assign the data\ny_data  = y0 + v0*t_data - 0.5 * g_exact * t_data**2  \n# y_data *= np.random.uniform(low=.9, high=1.1, size=(y_data.size)) # for later; add noise in\n\nplot_data(t_data,y_data, axis_labels=('t','y'))\n\ndt =  0.01 , nt =  100\n\n\n\n\n\nCan we fit this with a polynomial? Sure, let’s do that, using the code from before…\n\nc = polyfit(t_data, y_data, order=2, alpha_start = 10.0, maxiter=1000, printevery=100)\nprint(\"Our fit:          y(t) = \",c[0],\" + \",c[1],\"*t + \",c[2],\"*t**2\",sep='')\nprint(\"Compare to exact: y(t) = \",y0,  \" + \",v0,  \"*t - \",0.5*g_exact,\"*t**2\",sep='')\nprint(\"Estimate for g = \",-2*c[2])\n\nplot_data_and_curve(t_data,y_data, axis_labels=('t','y'))\n\n               Initial guess:    c =  [ 0.72  0.71  0.77]\n   0/1000: MSE =  5.41899, c = [-2.186  7.319 -1.042]\n     Notice: decreasing alpha[ 0 ] to  3.0\n     Notice: decreasing alpha[ 0 ] to  0.9\n 100/1000: MSE =0.0314071, c = [ 1.642  0.749 -2.528]\n 200/1000: MSE =0.00280409, c = [ 1.356  2.427 -4.191]\n 300/1000: MSE =0.000250355, c = [ 1.27   2.928 -4.688]\n 400/1000: MSE =2.23522e-05, c = [ 1.245  3.078 -4.837]\n 500/1000: MSE =1.99565e-06, c = [ 1.237  3.122 -4.881]\n 600/1000: MSE =1.78176e-07, c = [ 1.235  3.136 -4.894]\n 700/1000: MSE =1.59079e-08, c = [ 1.234  3.14  -4.898]\n 800/1000: MSE =1.42029e-09, c = [ 1.234  3.141 -4.899]\n 900/1000: MSE =1.26806e-10, c = [ 1.234  3.141 -4.9  ]\n1000/1000: MSE =1.13215e-11, c = [ 1.234  3.141 -4.9  ]\n\nOur fit:          y(t) = 1.23400775143 + 3.14145457517*t + -4.89995497009*t**2\nCompare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2\nEstimate for g =  9.79990994018\n\n\n\n\n\nWhat if we try fitting higher-order terms? Are their coefficients negligible? The system may converge, but it will take a lot more iterations… (be prepared to wait!)\n\nc = polyfit(t_data, y_data, order=3, alpha_start = 1.0, maxiter=700000, printevery=10000)\nprint(\"Our fit:          y(t) = \",c[0],\" + \",c[1],\"*t + \",c[2],\"*t**2 + \",c[3],\"*t**3\",sep='')\nprint(\"Compare to exact: y(t) = \",y0,  \" + \",v0,  \"*t - \",0.5*g_exact,\"*t**2\",sep='')\nprint(\"Estimate for g = \",-2*c[2])\n\n               Initial guess:    c =  [ 0.33  0.23  0.63  0.41]\n   0/700000: MSE = 0.828106, c = [ 1.189 -0.045  0.563  0.398]\n     Notice: decreasing alpha[ 0 ] to  0.3\n10000/700000: MSE =0.000464818, c = [ 1.291  2.454 -3.188 -1.138]\n20000/700000: MSE =0.000369748, c = [ 1.285  2.528 -3.373 -1.015]\n30000/700000: MSE =0.000294122, c = [ 1.279  2.594 -3.538 -0.906]\n40000/700000: MSE =0.000233965, c = [ 1.275  2.654 -3.685 -0.808]\n50000/700000: MSE =0.000186111, c = [ 1.27   2.706 -3.817 -0.72 ]\n60000/700000: MSE =0.000148045, c = [ 1.266  2.753 -3.934 -0.642]\n70000/700000: MSE =0.000117765, c = [ 1.263  2.795 -4.038 -0.573]\n80000/700000: MSE =9.36783e-05, c = [ 1.26   2.833 -4.131 -0.511]\n90000/700000: MSE =7.4518e-05, c = [ 1.257  2.866 -4.214 -0.456]\n100000/700000: MSE =5.92766e-05, c = [ 1.254  2.896 -4.289 -0.407]\n110000/700000: MSE =4.71526e-05, c = [ 1.252  2.922 -4.355 -0.363]\n120000/700000: MSE =3.75083e-05, c = [ 1.25   2.946 -4.414 -0.323]\n130000/700000: MSE =2.98366e-05, c = [ 1.248  2.967 -4.466 -0.288]\n140000/700000: MSE =2.37341e-05, c = [ 1.247  2.986 -4.513 -0.257]\n150000/700000: MSE =1.88797e-05, c = [ 1.246  3.003 -4.555 -0.229]\n160000/700000: MSE =1.50182e-05, c = [ 1.244  3.018 -4.592 -0.205]\n170000/700000: MSE =1.19465e-05, c = [ 1.243  3.031 -4.626 -0.183]\n180000/700000: MSE =9.50301e-06, c = [ 1.242  3.043 -4.655 -0.163]\n190000/700000: MSE =7.55933e-06, c = [ 1.241  3.054 -4.682 -0.145]\n200000/700000: MSE =6.0132e-06, c = [ 1.24   3.063 -4.705 -0.129]\n210000/700000: MSE =4.7833e-06, c = [ 1.24   3.072 -4.726 -0.115]\n220000/700000: MSE =3.80496e-06, c = [ 1.239  3.079 -4.745 -0.103]\n230000/700000: MSE =3.02672e-06, c = [ 1.239  3.086 -4.762 -0.092]\n240000/700000: MSE =2.40766e-06, c = [ 1.238  3.092 -4.777 -0.082]\n250000/700000: MSE =1.91521e-06, c = [ 1.238  3.097 -4.79  -0.073]\n260000/700000: MSE =1.52349e-06, c = [ 1.237  3.102 -4.802 -0.065]\n270000/700000: MSE =1.21188e-06, c = [ 1.237  3.106 -4.813 -0.058]\n280000/700000: MSE =9.64014e-07, c = [ 1.237  3.11  -4.822 -0.052]\n290000/700000: MSE =7.66841e-07, c = [ 1.236  3.114 -4.83  -0.046]\n300000/700000: MSE =6.09997e-07, c = [ 1.236  3.117 -4.838 -0.041]\n310000/700000: MSE =4.85233e-07, c = [ 1.236  3.119 -4.845 -0.037]\n320000/700000: MSE =3.85987e-07, c = [ 1.236  3.122 -4.851 -0.033]\n330000/700000: MSE =3.0704e-07, c = [ 1.235  3.124 -4.856 -0.029]\n340000/700000: MSE =2.4424e-07, c = [ 1.235  3.126 -4.861 -0.026]\n350000/700000: MSE =1.94285e-07, c = [ 1.235  3.127 -4.865 -0.023]\n360000/700000: MSE =1.54547e-07, c = [ 1.235  3.129 -4.869 -0.021]\n370000/700000: MSE =1.22937e-07, c = [ 1.235  3.13  -4.872 -0.019]\n380000/700000: MSE =9.77925e-08, c = [ 1.235  3.132 -4.875 -0.017]\n390000/700000: MSE =7.77907e-08, c = [ 1.235  3.133 -4.878 -0.015]\n400000/700000: MSE =6.188e-08, c = [ 1.235  3.134 -4.88  -0.013]\n410000/700000: MSE =4.92235e-08, c = [ 1.235  3.134 -4.882 -0.012]\n420000/700000: MSE =3.91556e-08, c = [ 1.235  3.135 -4.884 -0.01 ]\n430000/700000: MSE =3.1147e-08, c = [ 1.234  3.136 -4.886 -0.009]\n440000/700000: MSE =2.47764e-08, c = [ 1.234  3.136 -4.887 -0.008]\n450000/700000: MSE =1.97088e-08, c = [ 1.234  3.137 -4.889 -0.007]\n460000/700000: MSE =1.56777e-08, c = [ 1.234  3.138 -4.89  -0.007]\n470000/700000: MSE =1.24711e-08, c = [ 1.234  3.138 -4.891 -0.006]\n480000/700000: MSE =9.92037e-09, c = [ 1.234  3.138 -4.892 -0.005]\n490000/700000: MSE =7.89133e-09, c = [  1.234e+00   3.139e+00  -4.893e+00  -4.691e-03]\n500000/700000: MSE =6.27729e-09, c = [  1.234e+00   3.139e+00  -4.894e+00  -4.184e-03]\n510000/700000: MSE =4.99338e-09, c = [  1.234e+00   3.139e+00  -4.894e+00  -3.731e-03]\n520000/700000: MSE =3.97207e-09, c = [  1.234e+00   3.139e+00  -4.895e+00  -3.328e-03]\n530000/700000: MSE =3.15965e-09, c = [  1.234e+00   3.140e+00  -4.896e+00  -2.968e-03]\n540000/700000: MSE =2.5134e-09, c = [  1.234e+00   3.140e+00  -4.896e+00  -2.647e-03]\n550000/700000: MSE =1.99932e-09, c = [  1.234e+00   3.140e+00  -4.896e+00  -2.361e-03]\n560000/700000: MSE =1.5904e-09, c = [  1.234e+00   3.140e+00  -4.897e+00  -2.106e-03]\n570000/700000: MSE =1.26511e-09, c = [  1.234e+00   3.140e+00  -4.897e+00  -1.878e-03]\n580000/700000: MSE =1.00635e-09, c = [  1.234e+00   3.140e+00  -4.897e+00  -1.675e-03]\n590000/700000: MSE =8.0052e-10, c = [  1.234e+00   3.141e+00  -4.898e+00  -1.494e-03]\n600000/700000: MSE =6.36787e-10, c = [  1.234e+00   3.141e+00  -4.898e+00  -1.332e-03]\n610000/700000: MSE =5.06543e-10, c = [  1.234e+00   3.141e+00  -4.898e+00  -1.188e-03]\n620000/700000: MSE =4.02939e-10, c = [  1.234e+00   3.141e+00  -4.898e+00  -1.060e-03]\n630000/700000: MSE =3.20524e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -9.454e-04]\n640000/700000: MSE =2.54967e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -8.432e-04]\n650000/700000: MSE =2.02818e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -7.520e-04]\n660000/700000: MSE =1.61335e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -6.707e-04]\n670000/700000: MSE =1.28336e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -5.982e-04]\n680000/700000: MSE =1.02087e-10, c = [  1.234e+00   3.141e+00  -4.899e+00  -5.335e-04]\n690000/700000: MSE =8.12072e-11, c = [  1.234e+00   3.141e+00  -4.899e+00  -4.758e-04]\n700000/700000: MSE =6.45976e-11, c = [  1.234e+00   3.141e+00  -4.899e+00  -4.244e-04]\n\nOur fit:          y(t) = 1.23402130221 + 3.1412436463*t + -4.89936171114*t**2 + -0.000424401050714*t**3\nCompare to exact: y(t) = 1.234 + 3.1415*t - 4.9*t**2\nEstimate for g =  9.79872342227\n\n\nSo, in this case, we were able to show not only that the data fits a parabola well, but that the higher order term (for \\(t^3\\)) is negigible!! Great science! In practice, however, for non-perfect data, this does not work out. The higher-order term introduces an extreme sensitivity to the noise, which can render the results inconclusive.\nExercise: Go back to where the data is generated, and uncomment the line that says “# for later; add noise in” and re-run the fitting. You will find that the coefficients for the cubic polynomial do not resemble the original values found at all, whereas the coefficients for a quadratic polynomial, while not being the same as before, will still be “close.”\nThus, by hypothesizing a parabolic dependence, we’re able to correctly deduce the parameters of the motion (initial position & velocity, and acceleration), and we get a very low error in doing so. :-) Trying to show that higher-order terms in a polynomial expansion don’t contribute…that worked for “perfect data” but in a practical case it didn’t work out because polynomials are “ill behaved.” Still, we got some useful physics out of it. And that works for many applications. We could stop here.\n…although…\nWhat if our data wasn’t parabolic? Sure, for motion in a uniform gravitational field this is fine, but what if we want to model the sinusoidal motion of a simple harmonic oscillator? In that case, guessing a parabola would only work for very early times (thanks to Taylor’s theorem). Sure, we could fit a model where we’ve explictly put in a sine function in the code – and I encourage you to write your own code to do this – but perhaps there’s a way to deduce the motion, by looking at the local behavior and thereby ‘learning’ the differential equation underlying the motion.\nExercise: Copy the polyfit() code elsewhere (e.g. to text file or a new cell in this Jupyter notebook or a new notebook) and rename it sinefit(), and modify it to fit a sine function instead of a polynomial:\n\\[y(t) = A\\sin(\\omega t + \\phi),\\]\nwhere the fit parameters will be the amplitude \\(A\\), frequency \\(\\omega\\) and phase constant \\(\\phi\\). Try fitting to data generated for \\(A=3\\), \\(\\omega=2\\), \\(\\phi=1.57\\) on \\(0\\le t \\le 10\\). As an example, you can check your answer against this.  \n\nThe discussion goes on, but I’m breaking it off into a “Part Ib” for a separate post. In that post, we’ll switch from fitting the data “globally” to looking “locally,” in preparation for work in “Time Series Prediction.” \n\n-SH"
  },
  {
    "objectID": "posts/2017-02-23-Following-Gravity-Colab.html#afterward-alternatives-to-simple-gradient-descent",
    "href": "posts/2017-02-23-Following-Gravity-Colab.html#afterward-alternatives-to-simple-gradient-descent",
    "title": "Following Gravity - ML Foundations Part I",
    "section": "Afterward: Alternatives to “Simple” Gradient Descent",
    "text": "Afterward: Alternatives to “Simple” Gradient Descent\nThere are lots of schemes that incorporate more sophisticated approaches in order to achieve convergence more reliabily and more quickly than the “simple” gradient descent we’ve been doing.\nSuch schemes introduce concepts such as “momentum” and go by names such as Adagrad, Adadelta, Adam, RMSProp, etc… For an excellent overview of such methods, I recommend Sebastian Ruder’s blog post which includes some great animations!"
  },
  {
    "objectID": "posts/2019-12-06-Getting-Started-In-ML-Audio.html",
    "href": "posts/2019-12-06-Getting-Started-In-ML-Audio.html",
    "title": "Getting Started in ML+Audio",
    "section": "",
    "text": "Recently, after co-chairing the session on Machine Learning for Musical Acoustics along with the wonderful Bozena Kostek at the Acoustical Society of America Meeting in San Diego, a student came up to me to ask for advice or resources on how to start learning machine learning for audio applications.\nAnd I didn’t have a good answer. Because, while there’s plenty of tips out there for learning image classification or NLP, not a ton of it relates to audio. Students have asked me this before…and so I was frustrated by my own lack of an answer, apart from “read whatever you find, try out demos, take whatever courses you can, etc.”\nSO! Now, with graduate student Ryan Miller (EDIT 6/1/2020: who is now gainfully employed at Creative Labs!), we compiled a list and posted it to GitHub:"
  },
  {
    "objectID": "posts/2019-12-06-Getting-Started-In-ML-Audio.html#ml-audio-start-page",
    "href": "posts/2019-12-06-Getting-Started-In-ML-Audio.html#ml-audio-start-page",
    "title": "Getting Started in ML+Audio",
    "section": "ml-audio-start Page",
    "text": "ml-audio-start Page\n… and soliciting contributions from the community.\nEDIT 02/01/2020: We now have contributions from accomplished researchers at Google, Adobe, and others!"
  },
  {
    "objectID": "posts/2019-02-04-my-first-nn-part-2.html",
    "href": "posts/2019-02-04-my-first-nn-part-2.html",
    "title": "My First Neural Network, Part 2. Bias and CE Loss",
    "section": "",
    "text": "Links to lessons: Part 0, Part 1, Part 2, Part 3\nMoving on from our our previous notebook, we will investigate three things we could do to improve the models developed previously:"
  },
  {
    "objectID": "posts/2019-02-04-my-first-nn-part-2.html#add-a-bias-term",
    "href": "posts/2019-02-04-my-first-nn-part-2.html#add-a-bias-term",
    "title": "My First Neural Network, Part 2. Bias and CE Loss",
    "section": "1. Add a bias term",
    "text": "1. Add a bias term\nOur weighted sums did not include any constant offset or “bias” term. This may be fine for some data, but not for many others. For example, in a simple linear model \\(y = mx+b\\), the choice of \\(b=0\\) limits the model’s ability to accurately fit some data.\n\n\n\nimage of bias vs no biast\n\n\nThat is effectively what we did with our weighted sum \\(\\sum_j X_{ij}w_j\\): there was no constant offset. To correct this lack, we could add a new variable \\(b\\) and make our weighted sum \\(b + \\sum_j X_{ij}w_j\\). Equivalently, and more conveniently for the purposes of coding, we could put an additional column of 1’s in the input \\(X\\), and a new row to our weight matrix \\(w\\). By convention, this is usually done with the zeroth element, so that \\(X_{i0}=1\\) and the columns of \\(X\\) are moved to the right, and \\(w_0 = b\\) will be the new constant offset (because \\(1*w_0 = w_0\\).)\nFor the first problem (Trask’s first problem), this change makes our new matrix equation look like (with new bias terms in red)\n\\[\nf\\left(\n  \\overbrace{\n\\left[ {\\begin{array}{ccc}\n   \\color{red}1 & 0 & 0 & 1 \\\\\n   \\color{red}1 & 0 & 1 & 1\\\\\n   \\color{red}1 & 1 & 0 & 1\\\\\n   \\color{red}1 & 1 & 1 & 1\\\\\n  \\end{array} } \\right]\n}^\\text{X}\n\\overbrace{\n   \\left[ {\\begin{array}{c}\n   \\color{red}{w_0}  \\\\\n    w_1\\\\\n   w_2\\\\\n   w_3\n  \\end{array} } \\right]\n}^{w}\n\\right)\n  =\n  \\overbrace{\n\\left[ {\\begin{array}{c}\n   0   \\\\\n   0  \\\\\n   1  \\\\\n   1 \\\\\n  \\end{array} } \\right]\n}^{\\tilde{Y}}\n\\] Foreshadowing: Note that in this problem, the rightmost column of \\(X\\) already was a column of all 1’s, and so already has something akin to a bias. Thus, adding a new column of all 1’s will not add any information, and so for this problem we expect that adding the bias won’t improve the model performance.)\nWith this change, we can still write our weighted sum as \\(\\sum_j X_{ij}w_j\\), it’s just that \\(j\\) now runs over 0..3 instead of 0..2. To emphasize: We can leave the rest of our code the same as before, provided we change \\(X\\) by adding a column of 1’s.\nIn terms of coding the change to \\(X\\), we can either rewrite it by hand, or pull a numpy trick:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# old data\nX = np.array([  [0,0,1],\n                [0,1,1],\n                [1,0,1],\n                [1,1,1] ])\n\n# add a column of 1's\nnew_col = np.ones((X.shape[0],1)) # array of 1s, w/ same # of rows as X, 1 col wide\nX_bias = np.hstack((new_col,X))   # stack them horizontally\n\nprint(X_bias)\n\n[[1. 0. 0. 1.]\n [1. 0. 1. 1.]\n [1. 1. 0. 1.]\n [1. 1. 1. 1.]]\n\n\nLet’s compare our the use of the bias term without. We’ll define functions for the gradient descent and for the plotting of the loss history, so we can call these again later in this lesson.\n\nY = np.array([[0,0,1,1]]).T  # target output dataset \n\ndef sigmoid(x,deriv=False): \n  if(deriv==True):\n    return x*(1-x)\n  return 1/(1+np.exp(-x))\n\n\ndef calc_loss(Y_pred, Y, X, w, activ, loss_type=\"mse\"):  # MSE loss\n  diff = Y_pred - Y\n  loss = (diff**2).mean()\n  gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True)) # for weight update\n  return loss, gradient\n\n\ndef fit(X, Y, activ, use_bias=True, alpha=3.0, maxiter=10000, loss_type='mse'):\n  \"\"\"\n  fit: Generic routine for doing our gradient decent\n\n  Required arguments:\n      X:     input matrix\n      Y:     target output\n      activ: reference to an activation function\n      \n  Keywork arguments (optional):\n      use_bias: Flag for whether to use bias in the model\n      alpha:    learning rate. Tip: Use the largest alpha 'you can get away with'\n      maxiter:  maximum number of iterations\n      loss_type: Set to MSE for now but we'll extend this later. \n  \"\"\"\n  if use_bias:                       # add a column of 1's to X\n    new_col = np.ones((X.shape[0],1)) \n    X = np.hstack((new_col,X))        \n\n  # Define weights\n  np.random.seed(1)                  # for reproducibility \n  if activ == sigmoid:\n    w = 2*np.random.random((X.shape[1],Y.shape[1])) - 1  # -1..1 \n  else:\n    w = np.random.random((X.shape[1],Y.shape[1]))/10  # only positive weights (for later)\n    \n  loss_hist = []                     # start with an empty list\n  for iter in range(maxiter):\n\n    Y_pred = activ(np.dot(X,w))      # compute prediction, i.e. tilde{Y}\n\n    loss, gradient = calc_loss(Y_pred, Y, X, w, activ, loss_type)\n    loss_hist.append(loss)           # add to the history of the loss\n    \n    w -= alpha * gradient            # weight update\n\n  return w, Y_pred, loss_hist        # send these values back\n\n\n# Now call the fit function twice, to compare with and without bias:\nw_old, Y_pred_old, loss_hist_old = fit(X, Y, sigmoid, use_bias=False)\nw_new, Y_pred_new, loss_hist_new = fit(X, Y, sigmoid)\n\n# Plot the results.  Make a function so we can call this again later\ndef plot_new_old(loss_hist_old, loss_hist_new, labels=[\"no bias\", \"with bias\"]):\n  plt.loglog(loss_hist_old, label=labels[0])\n  plt.loglog(loss_hist_new, label=labels[1])\n  plt.xlabel(\"Iteration\")\n  plt.ylabel(\"MSE Loss (monitoring)\")\n  plt.legend()\n  plt.show()\n\nplot_new_old(loss_hist_old, loss_hist_new)\n# And print the final answers:\nprint(\"No bias: Y_pred =\",Y_pred_old)\nprint(\"With bias: Y_pred =\",Y_pred_new)\n\n\n\n\nNo bias: Y_pred = [[0.00553557]\n [0.00451069]\n [0.99632065]\n [0.9954838 ]]\nWith bias: Y_pred = [[0.00485703]\n [0.00433876]\n [0.99612094]\n [0.99565733]]\n\n\nAs expected, for this problem, the inclusion of bias didn’t make any significant difference. Let’s try the same thing for the 7-segment display problem from Part 1. And let’s try two different runs, one with sigmoid activation, and another with ReLU:\n\nX_7seg = np.array([ [1,1,1,1,1,1,0,1],  # 0\n                    [0,1,1,0,0,0,0,1],  # 1 \n                    [1,1,0,1,1,0,1,1],  # 2\n                    [1,1,1,1,0,0,1,1],  # 3\n                    [0,1,1,0,0,1,1,1],  # 4 \n                    [1,0,1,1,0,1,1,1],  # 5\n                    [1,0,1,1,1,1,1,1],  # 6 \n                    [1,1,1,0,0,0,0,1],  # 7 \n                    [1,1,1,1,1,1,1,1],  # 8 \n                    [1,1,1,1,0,1,1,1]   # 9 \n                 ])\nY_7seg = np.eye(10)\nX, Y = X_7seg, Y_7seg\n\ndef relu(x,deriv=False):   # relu activation\n  if(deriv==True):\n    return 1*(x&gt;0) \n  return x*(x&gt;0)\n\n\n# Call the fit routine twice, once for sigmoid activation, once for relu\nfor activ in [sigmoid, relu]:\n  print(\"\\n\\n--------- activ = \",activ)\n  alpha = 0.5 if activ == sigmoid else 0.005    # assign learning rate\n  w_old, Y_pred_old, loss_hist_old = fit(X, Y, activ, alpha=alpha, use_bias=False)\n  w_new, Y_pred_new, loss_hist_new = fit(X, Y, activ, alpha=alpha)\n\n  # Report results\n  plot_new_old(loss_hist_old, loss_hist_new)\n  np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)}) # 2 sig figs\n  print(\"No bias: Y_pred =\\n\",Y_pred_old)\n  print(\"With bias: Y_pred =\\n\",Y_pred_new)\n\n\n\n--------- activ =  &lt;function sigmoid&gt;\nNo bias: Y_pred =\n [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.02]\n [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00]\n [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.03 0.00]\n [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02]\n [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.03 0.02]\n [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02]\n [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00]\n [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00]\n [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.94 0.00]\n [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.02 0.96]]\nWith bias: Y_pred =\n [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.01]\n [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00]\n [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.02 0.00]\n [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02]\n [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.02 0.02]\n [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02]\n [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00]\n [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00]\n [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.95 0.01]\n [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.01 0.97]]\n\n\n--------- activ =  &lt;function relu&gt;\nNo bias: Y_pred =\n [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.01 0.00]\n [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00]\n [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.01 -0.00]\n [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00]\n [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.01 0.00]\n [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00]\n [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.01 -0.00]\n [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00]\n [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.96 -0.00]\n [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]]\nWith bias: Y_pred =\n [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.00 0.00]\n [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00]\n [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.00 -0.00]\n [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00]\n [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 0.00]\n [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00]\n [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.00 -0.00]\n [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00]\n [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.99 -0.00]\n [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]]\n\n\n\n\n\n\n\n\n…So for this problem, it seems that adding the bias gave us a bit more accuracy, both for the sigmoid and relu activations. Note: in this example, the learning rates were chosen by experiment; you should get in the habit of going back and experimenting with different learning rates."
  },
  {
    "objectID": "posts/2019-02-04-my-first-nn-part-2.html#video-interlude-logistic-regression",
    "href": "posts/2019-02-04-my-first-nn-part-2.html#video-interlude-logistic-regression",
    "title": "My First Neural Network, Part 2. Bias and CE Loss",
    "section": "Video Interlude: Logistic Regression",
    "text": "Video Interlude: Logistic Regression\nWhat we’ve been doing up until now has been a “classification” problem, with “yes”/“no” answers represented by 1’s and 0’s. This sort of operation is closely associated with the statistical method of Logistic Regression. It is akin to linear regression but with a sigmoid activation function. When doing Logistic Regression, one optimizes to fit by finding the maximum “likelihood” of a given model being correct.\nTo gain some insight on Logistic Regression, watch the following StatQuest video. (You can ignore his remarks about his preceding video, “R squared” and “p-value”, etc.)\n\n\n\n        \n        \n\n\nIn what follows, we will be minimizing the negative of the logarithm of the likelihood, a quantity typically known as the Cross-Entropy loss. (This same quantity is also the non-constant part of the “Kullback-Leibler Divergence” or “K-L divergence,” so you may hear it called that sometimes.)"
  },
  {
    "objectID": "posts/2019-02-04-my-first-nn-part-2.html#use-a-different-loss-function-cross-entropy-loss",
    "href": "posts/2019-02-04-my-first-nn-part-2.html#use-a-different-loss-function-cross-entropy-loss",
    "title": "My First Neural Network, Part 2. Bias and CE Loss",
    "section": "2. Use a different loss function: Cross-Entropy loss",
    "text": "2. Use a different loss function: Cross-Entropy loss\nLet’s return to Trask’s first problem for which there is only one target per data point (row) of input, namely a target of 0 or 1. In this case, using the sigmoid function for this classification problem is one of logistic regression, even though we hadn’t it identified it as such.\nWe’ve been using mean squared error (MSE) loss, but other loss functions exist. In particular, for outputs which are either “yes” or “no” such as the classification problem we are solving, a function called “cross entropy” is typically preferred. The cross-entropy loss is written like this:\n\\[L_{CE} = -\\sum_i \\left[ Y_i\\log(\\tilde{Y}_i) + (1-Y_i)\\log(1-\\tilde{Y}_i) \\right]\\]\nNote that since the function \\(\\log(x)\\) is undefined for \\(x\\le0\\), we need to make sure \\(0&lt;\\tilde{Y}_i&lt;1\\) for all \\(i\\). One way to ensure this is to use sigmoid activation! Thus, for classification problems, it is very common to see sigmoid activation (or its multi-class relative “softmax”) immediately before the output, even for many-layer neural networks with all kinds of other activations in other places.\nTo use the CE loss with gradient descent, we need its derivative with respect to the weights. First let’s write the CE loss in terms of the inputs \\(X\\), weights \\(w\\) and activation function \\(f\\):\n…wait, for compactness, let’s write the weighted sum as \\(S_i = \\sum_j X_{ij}w_j\\). Ok, now going forward….\n\\[L_{CE} = -\\sum_i\\left[ Y_i\\log\\left(f\\left(S_i \\right)\\right) + (1-Y_i)\\log\\left(1- f\\left(S_{i}\\right)\\right) \\right]\\]\nFor any function \\(g(x)\\), the derivative of \\(\\log(g(x))\\) with respect to x is just \\(1/g*(du/dx)\\), so our partial derivatives with respect to weights look like\n\\[{\\partial L_{CE}\\over \\partial w_j} = -\\sum_i\\left[ {Y_i\\over\\tilde{Y_i}}{\\partial f(S_i)\\over \\partial w_j}  -\n{1-Y_i\\over 1-\\tilde{Y}_i} {\\partial f(S_i)\\over \\partial w_j} \\right]\\\\\n= -\\sum_i {\\partial f(S_i) \\over \\partial S_i}{\\partial S_i\\over\\partial w_j} \\left[  {Y_i\\over\\tilde{Y_i}}  -\n{1-Y_i\\over 1-\\tilde{Y}_i}  \\right]\n\\] And if we multiply by \\(2/N\\), we can write this as \\[\n{\\partial L_{CE}\\over \\partial w_j}\n= {2\\over N} \\sum_{i=0}^{N-1}  {\\partial f(S_i) \\over \\partial S_i}X_{ij}\n\\left[\n{\\tilde{Y_i}-Y_i\\over \\tilde{Y_i}(1-\\tilde{Y_i}) }\\right]\\] This is similar to the partial derivatives for our MSE loss, except the term in the denominator is new. To see this more clearly, recall that the weight update for MSE (from Part 1) was \\[\nw := w - \\alpha  X^T \\cdot \\left( [\\tilde{Y}-Y]*\\tilde{Y}*(1-\\tilde{Y})\\right)\n\\] whereas for CE we actually get a bit of a simplification because the term in the denominator cancels with a similar term in the numerator: \\[\nw := w - \\alpha  X^T \\cdot \\left( [\\tilde{Y}-Y]*\\tilde{Y}*(1-\\tilde{Y})\\right) / (\\tilde{Y}*(1-\\tilde{Y})) \\\\\nw := w - \\alpha  X^T \\cdot [\\tilde{Y}-Y].\n\\] Thus despite all this seeming complication, our CE weight update is actually simpler than what it was before as MSE!\nLet’s try this out with code now:\n\n# we'll \"overwrite\" the earlier calc_loss function\ndef calc_loss(Y_pred, Y, X, w, activ, loss_type='ce'):\n  diff = Y_pred - Y\n  loss = (diff**2).mean()         # MSE loss\n\n  if 'ce' == loss_type:\n    diff = diff / (Y_pred*(1-Y_pred))     # use this for gradient\n    #loss = -(Y*np.log(Y_tilde) + (1-Y)*np.log1p(-Y_tilde)).mean()  # CE Loss\n    #  Actually we don't care what the loss itself is. \n    #  Let's use MSE loss for 'monitoring' regardless, so we can compare the \n    #  effects of using different gradients-of-loss functions\n\n  gradient = np.dot( X.T, diff*activ(Y_pred, deriv=True))      # same as before\n  return loss, gradient\n\n#---- \nX = X_bias\nY = np.array([[0,0,1,1]]).T  # target output dataset \n\n# Compare old and new\nw_mse, Y_pred_mse, loss_hist_mse = fit(X, Y, sigmoid, alpha=0.5, loss_type='mse')  \nw_ce, Y_pred_ce, loss_hist_ce    = fit(X, Y, sigmoid, alpha=0.5, loss_type='ce')   # fit\n\nplot_new_old(loss_hist_mse, loss_hist_ce, [\"MSE, with bias\", \"CE, with bias\"])\n# And print the final answers:\nprint(\"MSE _loss: Y_pred =\\n\",Y_pred_mse)\nprint(\"CE loss: Y_pred =\\n\",Y_pred_ce)\n\n\n\n\nMSE _loss: Y_pred =\n [[0.01]\n [0.01]\n [0.99]\n [0.99]]\nCE loss: Y_pred =\n [[0.00]\n [0.00]\n [1.00]\n [1.00]]\n\n\nThis works a lot better! To understand why, note that the gradients for MSE loss scale like \\[[\\tilde{Y}-Y]*\\tilde{Y}*(1-\\tilde{Y})\\] and thus these gradients go to zero as \\(\\tilde{Y}\\rightarrow 0\\), and/or \\(\\tilde{Y}\\rightarrow 1\\), which makes training very slow! In contrast, the extra denominator in the CE gradients effectively cancels out this behavior, leaving the remaining term of \\[[\\tilde{Y}-Y]\\] which varies linearly with the difference from the target value. This makes training much more efficient.\n\n# Aside: What happens if we try ReLU activation with CE loss?  Bad things, probably.\n# Recall that ReLU maps negative numbers to 0, and isn't bounded from above.\n#  Thus the \"denominator\" in the 'diff term' in our earlier code will tend to 'explode'.  \n#  Put differently, note that log(x) is undefined for x=0, as is log(1-x) for x=1.\nw_relu_ce, Y_pred_relu_ce, loss_hist_relu_ce = fit(X, Y, relu, alpha=0.001, loss_type='ce') \nplot_new_old(loss_hist_ce, loss_hist_relu_ce, [\"CE, sigmoid\", \"CE, ReLU\"])\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n  \n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater"
  },
  {
    "objectID": "posts/2021-08-06-Live-CL-Demo.html",
    "href": "posts/2021-08-06-Live-CL-Demo.html",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 3 - Live Demo of CL Toy",
    "section": "",
    "text": "Earlier entries: Part 1, Part 2\nI added some more optional controls to the “contrastive loss toy model” from Part 1 that I want to demonstrate interactively, so this Part 3 post is in the form of a video:"
  },
  {
    "objectID": "posts/2021-08-06-Live-CL-Demo.html#addenda",
    "href": "posts/2021-08-06-Live-CL-Demo.html#addenda",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 3 - Live Demo of CL Toy",
    "section": "Addenda:",
    "text": "Addenda:\n(I’m new to this video-making thing and TBH not sure I want to get good at it. I realize I’m over-animated…in a way that doesn’t happen when I’m in front of a class, on stage or on TV!)\n\nWhat I left out:\n\nVarious ways of getting a handle on choosing negative examples and doing so in a computationally efficient way (e.g. via Locality Sensitive Hashing). But the paper by Wu et al has a good survey of these issues – at least ca. 2017. And since then you can find newer treatments of such things as well, and I encourage you to do so – I’ll be joining you. Furthermore, if you follow the “Attract Only” methodology of SimCLR, then you wouldn’t have any negative examples anyway. ;-)\nMessing around with the toy model – giving it “crazy” inputs like huge margins, whatever – is not only fun (to me) but can also be quite informative as a process of “discovery”: I’m sure I learned things in the context of experience that I might otherwise have read (and maybe missed) in a paper.\n\n\n\nWhat said that came off as maybe misleading:\n\nI didn’t mean to imply that CLIP used the “attract-only” scheme I attribute to SimCLR. CLIP has a “contrastive loss” which means negative examples. I was just talking about how the CLIP result on ImageNet demonstrated the utility of metric-based, semi-supervised learning."
  },
  {
    "objectID": "posts/2021-08-06-Live-CL-Demo.html#references",
    "href": "posts/2021-08-06-Live-CL-Demo.html#references",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 3 - Live Demo of CL Toy",
    "section": "References",
    "text": "References\n\nJordan, J. Personal communications. cf. Twitter Page: https://twitter.com/jeremyjordan.\nWu, C.-Y.; Manmatha, R.; Smola, A.J.; Krähenbühl, P. Sampling Matters in Deep Embedding Learning. In Proceedings of the arXiv:1706.07567 [cs]; January 16 2018.\n“CLIP”: Radford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning Transferable Visual Models From Natural Language Supervision. ArXiv210300020 Cs 2021.\n\nFonseca, E.; Ortego, D.; McGuinness, K.; O’Connor, N.E.; Serra, X. Unsupervised Contrastive Learning of Sound Event Representations. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); IEEE, 2021.\nHadsell, R.; Chopra, S.; LeCun, Y. Dimensionality Reduction by Learning an Invariant Mapping. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06); 2006; Vol. 2, pp. 1735–1742.\n“SimCLR”: Chen, T.; Kornblith, S.; Norouzi, M.; Hinton, G. A Simple Framework for Contrastive Learning of Visual Representations. ArXiv200205709 Cs Stat 2020.\nAlamar, J. “Illustrated Word2Vec”, https://jalammar.github.io/illustrated-word2vec/\nAK / akhaliq / ak92501, “Gradio Demo for VQGAN+CLIP”, HuggingFace Spaces, https://huggingface.co/spaces/akhaliq/VQGAN_CLIP"
  },
  {
    "objectID": "posts/2020-12-11-Naughty-by-Numbers.html",
    "href": "posts/2020-12-11-Naughty-by-Numbers.html",
    "title": "Naughty by Numbers: Classifications at Christmas",
    "section": "",
    "text": "(It uses enough JavaScript that this blogging platform will mangle it.)\n\n\n\npreview image"
  },
  {
    "objectID": "posts/2020-12-11-Naughty-by-Numbers.html#head-over-to-httpshedges.belmont.edushawleynaughty-to-see-this-post.",
    "href": "posts/2020-12-11-Naughty-by-Numbers.html#head-over-to-httpshedges.belmont.edushawleynaughty-to-see-this-post.",
    "title": "Naughty by Numbers: Classifications at Christmas",
    "section": "",
    "text": "(It uses enough JavaScript that this blogging platform will mangle it.)\n\n\n\npreview image"
  },
  {
    "objectID": "posts/2020-07-01-Citations-Via-Bibtex.html",
    "href": "posts/2020-07-01-Citations-Via-Bibtex.html",
    "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
    "section": "",
    "text": "(N.B.: The following was created as a Markdown file in _posts/. For Jupyter notebooks, the same things apply; one simply enters the Liquid codes into Markdown cells. Quick Jupyter example.)"
  },
  {
    "objectID": "posts/2020-07-01-Citations-Via-Bibtex.html#how-to-cite",
    "href": "posts/2020-07-01-Citations-Via-Bibtex.html#how-to-cite",
    "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
    "section": "How to Cite",
    "text": "How to Cite\nFor demonstration purposes, I’ll take the liberty of citing a couple of my recent papers, namely the first SignalTrain paper {% cite signaltrain %} and the new one by Billy Mitchell {% cite billy_signaltrain2 %}. Instead of using the LaTeX code {% raw %}\\cite{ &lt;whatever&gt; }{% endraw %}, I use the Liquid code {% raw %}{% cite &lt;whatever&gt; %}{% endraw %}. For example, the first citation above is written as “{% raw %}{% cite signaltrain %}{% endraw %}” in the Markdown file that generates this HTML page.\nThe two citation markings above point to the References section at the end of this post where the full references are printed out in the bibliography style of my choice."
  },
  {
    "objectID": "posts/2020-07-01-Citations-Via-Bibtex.html#drawing-from-the-bibliography",
    "href": "posts/2020-07-01-Citations-Via-Bibtex.html#drawing-from-the-bibliography",
    "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
    "section": "Drawing from the Bibliography",
    "text": "Drawing from the Bibliography\nIn the main blog directory, create a new directory called _bibliography/, and place your BibTeX file there as references.bib. In the case of this demo, the references file looks like this:\n@conference{signaltrain,\n  title = {Profiling Audio Compressors with Deep Neural Networks},\n  author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis},\n  booktitle = {Audio Engineering Society Convention 147},\n  month = {Oct},\n  year = {2019},\n  url = {http://www.aes.org/e-lib/browse.cfm?elib=20595}\n}               \n\n@article{billy_signaltrain2, \n  title={Exploring Quality and Generalizability in Parameterized Neural Audio Effects},\n  author={William Mitchell and Scott H. Hawley},\n  journal={ArXiv},  \n  year={2020},\n  volume={abs/2006.05584} \n  url = {https://arxiv.org/abs/2006.05584}\n} \nNote that this (single) references file is for your entire blog. The great thing about this is that all your Jupyter notebooks and Markdown posts will draw from this same file, which could be hundreds of references long, and jekyll-scholar will only include the ones you need for each post.\nFinally, at the end of your post, you signal the creation of the list of references by using the Liquid tag\n{% raw %}{% bibliography --cited %}{% endraw %}\n…so I’ll put that at the very bottom of this file. (Currently that’ll generate an error, because we haven’t enabled jekyll-scholar yet, but we’ll do that next.) The optional argument --cited means it’ll only list the references cited in your post."
  },
  {
    "objectID": "posts/2020-07-01-Citations-Via-Bibtex.html#enabling-jekyll-scholar",
    "href": "posts/2020-07-01-Citations-Via-Bibtex.html#enabling-jekyll-scholar",
    "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
    "section": "Enabling Jekyll-Scholar",
    "text": "Enabling Jekyll-Scholar\nTo enable jekyll-scholar, all we need to do is make the following two changes, and perhaps a third.\n\nIn _config.yml, add “- jekyll-scholar” to the list of plugins:.\nEdit the Gemfile to include gem 'jekyll-scholar' where the other plugins are listed.\nOptional: The default citation format is “apa”. If you want to change that, you can add the following to your _config.yml file:\nscholar:\n    style: &lt;name&gt;\n…naming one of the styles in the CSL style repository (but leaving off the .csl ending). Tip from the CSL maintainers:\n\nTo quickly search the styles in the GitHub CSL style repository by file name, press “t” to activate GitHub’s File Finder and start typing.\n\nNote however that the csl-styles Gem package used by jekyll-scholar lags behind the official CSL style repository, so some names you choose may not work. In that case, you can supply a CSL file yourself. For this demo, I found the file physical-review-d.csl, added it to my main blog directory, and then specified the style name physical-review-d in _config.yml. This produced the bracketed-number citation markers above, and the reference format you see below in the References section. (EDIT: Actually I customized the CSL file a bit after that, so my new file is custom.csl.)\n\nThe convenience of this BibTeX/jekyll-scholar approach is that instead of having to manually edit references on each individual page – say, if you wanted to change citation formats (or alternatively, update information about a paper cited in multiple posts) – now you only change one line in _config.yml (or update one spot in references.bib) and the system “builds out” the change “everywhere.”\nHappy blogging!"
  },
  {
    "objectID": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html",
    "href": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html",
    "title": "Scottergories: Making GPT-3 Generate 3 of a Kind",
    "section": "",
    "text": "{{ page.content | number_of_words | minus: 26}} words, {{ page.content | number_of_words | divided_by: 180 | append: ” minute read” }}, {{ page.content | number_of_words | divided_by: 275 | append: ” paperback pages” }}\nI’m a computational physicist who spent most of his life only interested in curve-fitting, i.e. “regression.” Recently I’ve become fascinated in “classification” to the point where I’m in the process of writing a popular-level book on classification and categories. (For a couple recent works that include some of this, see {% cite hawleywhoserules %} and {% cite hawleykruger %}.)\nIn June of 2020, I applied for OpenAI API “GPT-3” {% cite gpt3 %} access immediately upon its announcement… and just heard back eight months later (shortly after I tweeted something nice about OpenAI’s “CLIP” paper). My proposal was to explore the GPT-3 language model’s capacity for handling categories, something I’ll refer to loosely by the shorthand of “categorical reasoning” (but don’t take this to mean that I assert GPT-3 actually “reasons” anything). On the form to request access, I wrote:\nFor examples of just how poorly GPT-2—the older, smaller model—performs at this task, see the “Discussion” below. Believe me when I say the results were disappointing.\n“GPT-2 had 1.5 billion parameters. GPT-3 has 175 billion.”\nNow, with GPT-3 access, we can begin to see how this newer, bigger model performs. The essence of using the API is the “Prompt”. So, my job is to craft prompts, and then see how well the model responds. My first experiment with the API involves getting the model to generate “three of a kind”:\nUpdate: Oh, hey, there’s a new paper on GPT-3 prompt-writing that came out the same day I wrote this post! Guess I’ll read that soon."
  },
  {
    "objectID": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#three-of-a-kind",
    "href": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#three-of-a-kind",
    "title": "Scottergories: Making GPT-3 Generate 3 of a Kind",
    "section": "Three of a Kind",
    "text": "Three of a Kind\nLet’s see if GPT-3 can expound in threes. I’ll write my prompt in bold, and then underlined text will show the model’s completion/output. (Additional commentary from me will be in italics.)\nThis application generates example members of a category. Examples of fruits are apples, oranges and bananas. Examples of birds are quails, crows and sparrows. (Whoa! Not bad! This is with the “davinci” model, with Response Length = 10 tokens, Temperature = 0.7, Top P = 1, etc..all defaults.)\n(Ok, I wasn’t always getting three examples when I’d re-try, so let’s make the “three” explicit:)\nThis application generates three example members of a category. Three examples of fruits are apples, oranges and bananas. Three examples of dogs are Labrador, Rottweiler and Husky. (Had to increase response length to 15 and then truncate the extra text which read “The input is”)\n(For the following, just assume that I’m truncating unwanted extra text. Also, to save space, I’ll refrain from printing the entire prompt I use, and just list the categories…)\nfish: salmon, bass and tuna.\nU.S. Presidents: George Washington, Franklin Delano Roosevelt and Ronald Reagan (Whoah, this one triggered the “This completion may contain sensitive content” warning!)\nphilosophers: Plato, Descartes and Kant.\nthings you’d find in a refrigerator: milk, orange juice and apple juice. (Sure. But since it’s a random sample, let’s go again:) milk, cheese and vegetable salsa. (Not bad!)\nthings you’d find under a bed: a mouse, a handkerchief and a ball. (Ok, I guess…why not? Again:) socks, shoes and books. (ok…I can buy that.)\n80’s power ballads: “Faith” by George Michael, “I Want to Know What Love Is” by Foreigner and “The Power of Love” by Huey Lewis and the News. (uh…if you say so. At least these are all in the 80s!)\nhousehold chores: cleaning, cooking and sweeping. (or again:) washing, cleaning and ironing. Three examples of sports are swimming, running and cycling. (The model added that second sentence all by itself! Note all of these are gerunds, i.e. -ing words; it didn’t say, for example, something odd/wrong like “clean, mop, broom”)\nthings you don’t want your mom to know about you:: a. how drunk you were at the last party b. that you smoke pot c. that you plagiarized on that paper for your English class. (Whoa, have we reached AGI? ;-) And extra points for the use of “a. b. c.”)\nitems you take on a trip: a car, a suitcase and a book. (You CAN “take” a car, can’t you? hah! Nice linguistic anomaly.)\nfruits of the Holy Spirit: faith, hope and love. (I mean, I want share this on @FaithTechHub, so…right? LOL)\nOK, I am sufficiently impressed and have now gone to get my official Hasbro(TM) Scattorgories game…The following prompts are from the game\nthings that are sticky: gum, tape and bubble gum. (Nooice!)\nthings you add water to: eggs, rice and pasta. (eggs? Hmm…go again:) tea, coffee and milk. (nice. …wait, milk?. Again:) urns, cups, and buckets. (whoa!)\nterms of measurement: kilometers, centimeters and grams. Three examples of colors are red, blue and green. Three examples of adjectives are red, blue and green. Three examples of verbs are run, jump (See? It will keep generating categories if you set Response Length longer. Let me make the Response Length even longer, to 200…)\nterms of measurement: 3, 4 and 5 meters. Three examples of angles are 20 degrees, 40 degrees and 60 degrees. Three examples of countries are Japan, France and Germany. (Ok and beyond that it just repeats itself and/or repeats similar sentences, like: “The application generates the following sentences. This application generates verbs. The application generates the following sentences. This application generates pronouns.” and so on)\nthings found in the ocean: fish, coral and whales. Three examples of things found in the sky are clouds, stars and planets. (that second sentence was generated by the model)\nthings that are bright: the sun, the moon and the stars. Three examples of things that are made of wood are tables, chairs and fences. (Again, the second sentence was generated. Just in case it’s got a heavenly-body fixation, let’s go again:) things that are bright: the moon, the sun and an electric bulb. Three examples of furniture that a household might have are a sofa, a chair and a table. (Good job! “furniture” is a tricky category as described by George Lakoff {% cite Lakoff87 %})\nleisure activities: simple relaxation, snowboarding and surfing. Three examples of countries are USA, Canada and Mexico. (Not bad)\nreptiles/amphibians: lizards, frogs and crocodiles. (Ok, that “slashed” construction of “reptiles/amphibians” was literally what was printed on the card. I had not expected GPT-3 to pull that off, but mixing reptiles (lizards & crocodiles) with amphibians (frogs) is pretty cool. Let’s try just reptiles:) reptiles: lizards, snakes and turtles. Three examples of mammals are cows, sheep and giraffes. In each example, the category is written in bold letters and (Ok, now that’s just cheeky!)\nthings you do online: search, chat and email. (Note how it generates only things that are…arguably..verbs. Again:) checking email, shopping and playing games. (My goodness, it’s full of gerunds!)\n(Yea, let’s try gerunds, and let’s bump the Response Length up to 145…)\ngerunds: eating, running and swimming. Three examples of adjectives are large, strong and important. Three examples of verbs are write, jump and try. Three examples of adverbs are quickly, happily and carefully. Three examples of adjectives are big, wide and dangerous. Three examples of prepositions are to, in and on. This application generates a category. All examples are of the same gender, number and case. Three examples of nouns are dog, cat and bird. Three examples of adjectives are tall, small and large. Three examples of prepositions are by, for and under. Three examples of adverbs are quickly, slowly and (SHOW OFF!)\nterms of endearment: honey, sweetheart and dear. (awww)\nOk, I’ve now gone and grabbed my boxed copy of The Game of Things. Now, keep in mind, some of these things can be of a “suggestive” nature…\nthings you would like to do with chocolate: eat it, cook with it and share it with your friend. (Uh…ok, that was not at all suggestive, haha!)\nthings you shouldn’t do on your honeymoon: drink, drive and shop. Three examples of good economic policies are low interest rates, an independent central bank and low taxes. (COMPLETION MAY CONTAIN SENSITIVE CONTENT) (All very sensible, though not likely to be what you’ll hear if you play in a group of humans. Note that the “goal” of the humans would be to make each other laugh, whereas GPT-3 is just trying to generate high-probability matches. It’s basically playing Family Feud.)\nthings that warrant an apology: spilling a drink on someone, saying something rude to someone, and being late for an appointment. (Yeah, good job GPT-3!)\nthings that would get you fired: stealing, sleeping on the job and talking back to your boss. (Sure!)\nthings you would do if you had super-human powers: play baseball, run in the Olympics and fly to the moon. (again:) catch a falling satellite, fly from New York to Tokyo, and make a million dollars. (Ok. Sensible answers, and note that all the verb-usage matches.)\nthings you shouldn’t try to hold on to: a pen, a balloon and a cork. The idea is that you decide whether the example is an example of the category, or not. You click (Ok, wait a sec. Not sure about the pen, balloon and cork??, but it seems like it’s scraped some aspect of category-naming-games and included it in here! Let’s re-do a bunch)\nthings you shouldn’t try to hold on to: * a taxi, a dead rat and a Volkswagen. * a hot potato, a hot date and a hot temper. * a rock, a bird and a balloon. * a ball, a cat and a balloon. Three examples of things you should be careful with are an axe, a knife and a gun. (2nd sentence is part of the output) * a balloon, a basketball and a cactus. In the example, the Generate action is used to randomly generate a member of the category. This (Ok, kudos on the cactus. Not sure where it’s getting its category-gaming info from)\n(Speaking of Something To Hold On To, let’s give it an obscure music task just to see how it does:)\nsongs by Trevor Rabin: “Racing in the Street”, “Big Generator” and “Can I Play With Madness”. (hahaaaha. The first is a Springsteen tune, the 2nd the title track off Yes’ Big Generator album when Rabin was in the band and supposedly they all shared writing credits, and I would PAY MONEY to see Trevor Rabin cover Iron Maiden’s “Can I Play With Madness”!)\n(BTW, if you want to hear an incredible cover of “Something To Hold On To,” please give my good friend Rex Schnelle’s version a listen. Moving on…)\nthings you shouldn’t share: toothbrushes, clothes and pencils. (I did this over & over and “toothbrushes” was in every output, no matter how high I made the Temperature)\nthings you shouldn’t try to do in the dark: play tennis, play football and drive a car. (All my retries on this were similarly boring; GPT-3 rarely surprises one with its off-the-wall hilarity)\n(Speaking of lack of originality, it’s amazing how many cards in The Game of Things start with “Things you shouldn’t…”)\nthings you wouldn’t want to find in your sandwich: a brick, a piece of glass and a dead rat. (Spot on)\n(As a resident of Nashville, I’m curious to see if we can make it write a country song; many country songs are just lists of nostalgia for bygone rural America…)\nthings you remember about growin’ up on the farm: mud, worms, and tractors. (ok, sure)\nthings Daddy used to tell you: “Don’t do that!”, “Be nice.”, and “Look out!”. Three examples of things you are going to do when you grow up are: “travel to the moon.”, “write a book”, and “learn how to dance”. The full Python code is shown below: def make_things(): fruits = [“apple”, “orange”, “banana”] people = [“Daddy”, “Mommy”, “Grandpa”] things = [“Don’t do that!”, “Be nice.”, “Look out!”] things = [“travel to the moon.”, “write a book.”, “learn how to dance.”] return fruits, people, things What is the purpose of this application (Whooooaaa! What IS this thing? Is there a human being on the other end of this? ;-) .)\nthings you like about your truck: leather seats, size, and the sound system. (Eh. Technically valid answers, sensible, high-probability examples. Nothing “creative” though, of course. Again, only this time I’ll crank the Temperature up to 0.97:) safety rating, engine, and CD player. (The “trying to win at Family Feud” analogy is apt. Ok, forget the country song. Moving on.)\nthings you wouldn’t want a doctor to say to you:\n\n“you don’t have anything to worry about”, “one more time” and “we’ll see how it goes”. (The model knew to use quotation marks! First example seems wrong.)\nI have good news and bad news, surgery is now a day care procedure and it’s not cancer, it’s a tumor. (Uh.. huh?)\n, “You’re dead”, “You’re pregnant”, “You’re infected”. Three examples of things you wouldn’t want a doctor to say to your wife are (Nice comma usage, and Whoa! Let’s do that last one…)\n\nthings you wouldn’t want a doctor to say to your wife: * “I’m sorry” “I love you” and “I’m sorry I love you.” (hahaha!) * , “I’m afraid you have cancer”, “I’m afraid you have AIDS” and “I’m afraid you have syphilis.” Three examples of names of big cities are London, Paris and Los Angeles. Three examples of books to read are “War and Peace”, “Mein Kampf” and “The Catcher in the Rye”. (Ok, I think it’s got the hang of this 3’s-generating now! LOL)\nLast up: I mentioned this to my friend Heather and she asked me how long I get to ‘keep playing with it’. I said “My free trial tokens expire May 15, 2021, and I’ve already used 6000 out of 300k tokens. So it’s basically like joining the Chuck E. Cheese club.” She said, “Nice! I wonder if you can teach it to win at skee ball 😅” Which means now I have to do…\nthings you’d find at Chuck E. Cheese: video games, a ball pit and tokens. Three examples of dinosaurs are Tyrannosaurus Rex, Triceratops and Velociraptor. (NOT BAD! I’m going to go again, and if it mentions skee ball, WE’RE DONE.) * tokens, pizza and soda. (good) * Chuck E. Cheese himself, his brother, and a clown. (Ohhh-kay that’s enough for now! haha)"
  },
  {
    "objectID": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#discussion-preliminary-thoughts",
    "href": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#discussion-preliminary-thoughts",
    "title": "Scottergories: Making GPT-3 Generate 3 of a Kind",
    "section": "Discussion (Preliminary Thoughts)",
    "text": "Discussion (Preliminary Thoughts)\n“Surely this is not a very ‘intelligent’ task,” you say. “All it does is generate words that have some suitably high probability of being associated with the word in the prompt.” “Since it was trained on the internet, such lists-of-things-in-a-category probably already exist as web pages somewhere anyway”\nRight, one could imagine scraping a ton of web pages for text and then training some kind of non-Transformer-based method, maybe even a Bayesian bag-of-words model (like in a simple spam detector) to rank “related words” and pick the top choices.\nThat’s not exactly what’s happened here. Yes, GPT-3 was trained on tons of web pages, yes it “knows” (has a probabilistic model for) word associations, but also…\nIt is a “few shot learner” so it essentially instantly starts giving me outputs like what I want, without having been pre-trained on the task I want it to do. The fact that it gets singular vs. plural right, and verbs vs. nouns, and gerunds correct in its answers is noteworthy.\nSo far, this is just my first night of access to the API, and it blows away what I could get GPT-2 to do. One can try out various GPT-2 model sizes using the friendly interfaces at HuggingFace.com. For the smallest GPT-2 model, I got\ndogs: cats.” (Womp womp!)\nThe largest GPT-2 model size, however, with suitably adjusted parameters, could generate 3 dog breeds or 3 U.S. Presidents correctly. But any of the more “challenging,” “speculative” categories (e.g. “things you might…”) give even the largest GPT-2 at HuggingFace a hard time:\nthings that would get you fired: the fire alarm, the bathroom sink and the bathroom mirror. (?? The other various completions generated by GPT-2 when I re-ran it were similarly irrelevant.)\nOne further possible remark to my post here: “You realize you aren’t really playing Scattergories(tm), right?” Yes of course. Scattergories involves rolling the dice and getting a letter and having to name a bunch of things that start with that letter. Pace. I was never actually wanting to replicate the Scattergories(tm) game engine itself, rather the name of the game was a placeholder for the idea of “categorical reasoning.” Now…could we get GPT-3 to actually play Scattergories? …I’m not sure I even …care? I’m not sure how you’d write the prompt for that, but I’m open to trying it. For now, at just my first night of trying things out, this seems remarkably encouraging.\nWhich means….based on my experience with AI models: If the initial trial is encouraging, that means there’s a TON of failure modes you just haven’t discovered yet! ;-) We can probe for those too, another time – so far I’ve only used up 1.6% of my free tokens. There’s a lot more that one can do to use the API, and so far I haven’t used any of the advanced prompt-making utilities. Still, for the first foray into the OpenAI API, this was a GREAT experience!\nP.S.- OH GEEZ I JUST NOTICED THAT I TAUGHT GPT-3 TO NOT USE THE OXFORD COMMA! May a large millstone be tied around my neck… Wait wait! : All that was needed to get it to start generating Oxford-comma-style examples was to add the comma to my one example: “apples, oranges, and bananas.” Wow."
  },
  {
    "objectID": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#references",
    "href": "posts/2021-02-15-Making-GPT3-Generate-3-Of-A-Kind.html#references",
    "title": "Scottergories: Making GPT-3 Generate 3 of a Kind",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html",
    "title": "Crash Course On GANs",
    "section": "",
    "text": "header-img\nImage credit: Dev Nag\nThis post is not necessarily a crash course on GANs. It is at least a record of me giving myself a crash course on GANs. Adding to this as I go."
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#intromotivation",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#intromotivation",
    "title": "Crash Course On GANs",
    "section": "Intro/Motivation",
    "text": "Intro/Motivation\nI’ve been wanting to grasp the seeming-magic of Generative Adversarial Networks (GANs) since I started seeing handbags turned into shoes and brunettes turned to blondes…  …and seeing Magic Pony’s image super-resolution results and hearing that Yann Lecun had called GANs the most important innovation in machine learning in recent years.\nFinally, seeing Google’s Cat-Pig Sketch-Drawing Math… \n…broke me, and so…I need to ‘get’ this.\nI’ve noticed that, although people use GANs with great success for images, not many have tried using them for audio yet (Note: see SEGAN paper, below). Maybe with already-successful generative audio systems like WaveNet, SampleRNN (listen to those piano sounds!!) and TacoTron there’s less of a push for trying GANs. Or maybe GANs just suck for audio. Guess I’ll find out…"
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#steps-i-took",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#steps-i-took",
    "title": "Crash Course On GANs",
    "section": "Steps I Took",
    "text": "Steps I Took\n\nDay 1:\n\nGathered list of some prominent papers (below).\nWatched video of Ian Goodfellow’s Berkeley lecture (notes below).\nStarted reading the EBGAN paper (notes below)…\n…but soon switched to BEGAN paper – because wow! Look at these generated images: \nGoogled for Keras-based BEGAN implementations and other code repositories (below)…Noticed SEGAN…\n…Kept reading BEGAN, making notes as I went (below).\nFinished paper, started looking through BEGAN codes from GitHub (below) & began trying them out…\n\nCloned @mokemokechicken’s Keras repo, grabbed suggested LFW database, converted images via script, ran training… Takes 140 seconds per epoch on Titan X Pascal.\n\n\nMain part of code is in models.py\n\n\nCloned @carpedm’s Tensorflow repo, looked through it, got CelebA dataset, started running code.\n\nLeaving codes to train overnight. Next time, I’d like to try to better understand the use of an autoencoder as the discriminator.\n\n\n\nDay 2:\n\nMy office is hot. Two Titan X GPUs pulling ~230W for 10 hours straight has put the cards up towards annoyingly high temperatures, as in ~ 85 Celsius! My previous nightly runs wouldn’t even go above 60 C. But the results – espically from the straight-Tensorflow code trained on the CelebA dataset – are as incredible as advertised! (Not that I understand them yet. LOL.) The Keras version, despite claiming to be a BEGAN implementation, seems to suffer from “mode collapse,” i.e. that too many very similar images get generated.\nFished around a little more on the web for audio GAN applications. Found an RNN-GAN application to MIDI, and found actual audio examples of what not to do: don’t try to just produce spectrograms with DCGAN and convert them to audio. The latter authors seem to have decided to switch to a SampleRNN approach. Perhaps it would be wise to heed their example? ;-)\nSince EBGAN implemented autoencoders as discriminators before BEGAN did, I went back to read that part of the EBGAN paper. Indeed, section “2.3 - Using AutoEncoders” (page 4). (see notes below)\nOk, I basically get the autoencoder-discriminator thing now. :-)\n\n\n\nDay 3:\n“Life” intervened. :-/ Hope to pick this up later."
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#papers",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#papers",
    "title": "Crash Course On GANs",
    "section": "Papers",
    "text": "Papers\nHaven’t read hardly any of these yet, just gathering them here for reference:\n\nOriginal GAN Paper: ” Generative Adversarial Networks” by GoodFellow (2014)\nDCGAN: “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” by Radford, Metz & Chintala (2015)\n“Image-to-Image Translation with Conditional Adversarial Networks” by Isola et al (2016)\n“Improved Techniques for Training GANs” by Salimans et al (2016).\nDiscoGAN: “Learning to Discover Cross-Domain Relations with Generative Adversarial Networks” by Kim et al. (2017)\nEBGAN: “Energy-based Generative Adversarial Network by Zhao, Matheiu & Lecun (2016/2017).\n\nRemarks/Notes:\n“This variant [EBGAN] converges more stably [than previous GANs] and is both easy to train and robust to hyper-parameter variations” (quoting from BEGAN paper, below).\nIf it’s energy-based, does that mean we get a Lagrangian, and Euler-Lagrange Equations, and Lagrange Multipliers? And thus can physics students (& professors!) grasp these networks in a straightforward way? Should perhaps take a look at Lecun’s Tutorial on Energy-Based Learning.\n“The energy is the resconstruction error [of the autoencoder]” (Section 1.3, bullet points)\n\n\n\nebgan-pic\n\n\n\nImage credit: Roozbeh Farhoodi + EBGAN authors\n\n“…256×256 pixel resolution, without a multi-scale approach.” (ibid)\nSection 2.3 covers on the use of the autoencoder as a discriminator. Wow, truly, the discriminator’s “energy”/ “loss” criterion is literally just the reconstruction error of the autoencoder. How does that get you a discriminator??\nIt gets you a discriminator because the outputs of the generator are likely to have high energies whereas the real data (supposedly) will produce low energies: “We argue that the energy function (the discriminator) in the EBGAN framework is also seen as being regularized by having a generator producing the contrastive samples, to which the discrim- inator ought to give high reconstruction energies” (bottom of page 4).\n\n“Wasserstein GAN (WGAN) by Arjovsky, Chintala, & Bottou (2017)\n“BEGAN: Boundary Equilibrium Generative Adversarial Networks” by Berthelot, Schumm & Metz (April 2017).\n\nRemarks/Notes:\n“Our model is easier to train and simpler than other GANs architectures: no batch normalization, no dropout, no transpose convolutions and no exponential growth for convolution filters.” (end of section 3.5, page 5)\nThis is probably not the kind of paper that anyone off the street can just pick up & read. There will be math.\nUses an autoencoder for the discriminator.\nI notice that Table 1, page 8 shows “DFM” (from “Improving Generative Adversarial Networks with Denoising Feature Matching” by Warde-Farley & Bengio, 2017) as scoring higher than BEGAN.\npage 2: “Given two normal distributions…with covariances \\[C_1, C_2\\],…”: see “Multivariate Normal Distribution”.\nSection 3.3, Equilibrium: The “\\[\\mathbb{E}[\\  ]\\]” notation – as in \\[\\mathbb{E}\\left[\\mathcal{L}(x)\\right]\\] – means “expected value.” See https://en.wikipedia.org/wiki/Expected_value\nIntroduces the diversity ratio: \\[\\gamma=\\frac{\\mathbb{E}\\left[\\mathcal{L}(G(z))\\right]}{\\mathbb{E}\\left[\\mathcal{L}(x)\\right]}\\]. “Lower values of \\[\\gamma\\] lead to lower image diversity because the discriminator focuses more heavily on auto-encoding real images.”\n“3.5 Model architecture”: Did not actually get the bit about the autoencoder as the discriminator: “How does an autoencoder output a 1 or a zero?”\n\nOk, done. Will come back later if needed; maybe looking at code will make things clearer…\n\n“SEGAN: Speech Enhancement Generative Adversarial Network” by Pascual, Bonafonte & Serra (April 2017). Actual audio GAN! They only used it to remove noise."
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#videos",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#videos",
    "title": "Crash Course On GANs",
    "section": "Videos",
    "text": "Videos\n\nIan Goodfellow (original GAN author), Guest lecture on GANs for UC Berkeley CS295 (Oct 2016). 1 hour 27 minutes. NOTE: actually starts at 4:33. Watch at 1.25 speed.\n\nRemarks/Notes:\nThis is on a fairly “high” level, which may be too much for some viewers; if hearing the words “probability distribution” over & over again makes you tune out, and e.g. if you don’t know what a Jacobian is, then you may not want to watch this.\nHis “Taxonomy of Generative Models” is GREAT!\nThe discriminator is just an ordinary classifier.\nSo, the generator’s cost function can be just the negative of the discriminator’s cost function, (i.e. it tries to “mess up” the discriminator), however that can saturate (i.e. produce small gradients) so instead they try to “maximize the probability that the discriminator will make a mistake” (44:12).\n“KL divergence” is a measure of the ‘difference’ between two PD’s.\n“Logit” is the inverse of the sigmoid/logistical function. (logit&lt;–&gt;sigmoid :: tan&lt;–&gt;arctan)\nJensen-Shannon divergence is a measure of the ‘similarity’ between two PD’s. Jensen-Shannon produces better results for GANs than KL/maximum likelihood."
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#web-poststutorials",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#web-poststutorials",
    "title": "Crash Course On GANs",
    "section": "Web Posts/Tutorials",
    "text": "Web Posts/Tutorials\n\n“Machine Learning is Fun Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art” by Adam Geitgey, skip down to “How DCGANs Work” (2017)\nPost on BEGAN: https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/\nAn introduction to Generative Adversarial Networks (with code in TensorFlow)\n“Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)” by Dev Nag (2017)\n“Stability of Generative Adversarial Networks” by Nicholas Guttenberg (2016)\n“End to End Neural Art with Generative Models” by Bing Xu (2016)\nKording Lab GAN Tutorial by Roozbeh Farhoodi :-). Nicely done, has code too."
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#code",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#code",
    "title": "Crash Course On GANs",
    "section": "Code",
    "text": "Code\n\nKeras:\n\n‘Basic’ GAN with MNIST example: https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html\nGAN, BiGAN & Adversarial AutoEncoder: https://github.com/bstriner/keras-adversarial\nKording Lab’s GAN tutorial, Jupyter Notebook https://github.com/KordingLab/lab_teaching_2016/blob/master/session_4/Generative%20Adversarial%20Networks.ipynb. (Code is short and understandable.)\nKeras BEGAN:\n\nhttps://github.com/mokemokechicken/keras_BEGAN: Only works on 64x64 images; BEGAN paper shows some 128x128\nhttps://github.com/pbontrager/BEGAN-keras: No documentation, and I don’t see how it could run. I notice local variables being referenced in models.py as if they’re global.\n\nKeras DCGAN (MNIST): https://github.com/jacobgil/keras-dcgan\nAuxiliary Classifier GAN: https://github.com/lukedeo/keras-acgan\n\n\n\nTensorflow:\n\nBEGAN-Tensorflow: https://github.com/carpedm20/BEGAN-tensorflow\nEBGAN.Tensorflow: https://github.com/shekkizh/EBGAN.tensorflow\nSEGAN: https://github.com/santi-pdp/segan\nDCGAN-Tensorflow: https://github.com/carpedm20/DCGAN-tensorflow\n\n\n\nPyTorch:\n\nTutorial & simple implementation: https://github.com/devnag/pytorch-generative-adversarial-networks"
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#datasets",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#datasets",
    "title": "Crash Course On GANs",
    "section": "Datasets",
    "text": "Datasets\n\nCelebA: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\nMNIST: https://yann.lecun.com/exdb/mnist/\nSpeech enhancement: https://datashare.is.ed.ac.uk/handle/10283/1942\n“Labelled Faces in the Wild” https://vis-www.cs.umass.edu/lfw/"
  },
  {
    "objectID": "posts/2017-05-05-Crash-Course-On-GANs.html#more-references-lists",
    "href": "posts/2017-05-05-Crash-Course-On-GANs.html#more-references-lists",
    "title": "Crash Course On GANs",
    "section": "More References (Lists)",
    "text": "More References (Lists)\n\n“Delving deep into Generative Adversarial Networks (GANs): A curated list of state-of-the-art publications and resources about Generative Adversarial Networks (GANs) and their applications.”"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html",
    "href": "posts/2019-01-30-my-first-neural-network.html",
    "title": "My First Neural Network, Part 1",
    "section": "",
    "text": "Links to lessons: Part 0, Part 1, Part 2, Part 3\nWe will be reproducing work from Andrew Trask’s excellent tutorial “A Neural Network in 11 lines of Python”, albeit with a different emphasis, and in a different way. You may regard this treatment and his original treatment as complimentary, and feel free to refer to both. This lesson is written with the intent of building on the lesson about linear regression – which we might call “Part 0” – at the link “Following Gravity - ML Foundations Part Ia.”"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#the-sample-problem",
    "href": "posts/2019-01-30-my-first-neural-network.html#the-sample-problem",
    "title": "My First Neural Network, Part 1",
    "section": "The Sample Problem",
    "text": "The Sample Problem\nConsider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we’ll show each set of 3 inputs as a row of a matrix \\(X\\), and each output as the corresponding row of \\(Y\\):\n\\[ \\overbrace{\n\\left[ {\\begin{array}{ccc}\n   0 & 0 & 1 \\\\\n   0 & 1 & 1\\\\\n   1 & 0 & 1\\\\\n   1 & 1 & 1\\\\\n  \\end{array} } \\right]\n}^{X} \\rightarrow\n\\overbrace{\n\\left[ {\\begin{array}{c}\n   0   \\\\\n   0  \\\\\n   1  \\\\\n   1 \\\\\n  \\end{array} } \\right]\n  }^Y.\n\\]\nEven though this system has an exact solution (namely, \\(Y\\) equals the first column of \\(X\\)), usually we’ll need to be satisfied with a system that maps our inputs \\(X\\) to some approximate “prediction” \\(\\tilde{Y}\\), which we hope to bring closer to the “target” \\(Y\\) by means of successive improvements.\nThe way we’ll get our prediction \\(\\tilde{Y}\\) is by means of a weighted sum of each set of 3 inputs, and some nonlinear function \\(f\\) which we call the “activation function” (or just “activation”). Pictorially, the process looks like the following, for each row \\(i\\) of \\(X\\) and \\(Y\\), (where the columns of \\(X\\) are shown arranged vertically instead of horizonally):\n\n\n\nimage of NN\n\n\nIn terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values \\(w_0\\), \\(w_1\\) and \\(w_2\\). The calculation can be written as:\n\\[\nf\\left(\n  \\overbrace{\n\\left[ {\\begin{array}{ccc}\n   0 & 0 & 1 \\\\\n   0 & 1 & 1\\\\\n   1 & 0 & 1\\\\\n   1 & 1 & 1\\\\\n  \\end{array} } \\right]\n}^\\text{X}\n\\overbrace{\n   \\left[ {\\begin{array}{c}\n   w_0  \\\\\n    w_1\\\\\n   w_2\\\\\n  \\end{array} } \\right]\n}^{w}\n\\right)\n  =\n  \\overbrace{\n\\left[ {\\begin{array}{c}\n   0   \\\\\n   0  \\\\\n   1  \\\\\n   1 \\\\\n  \\end{array} } \\right]\n}^{\\tilde{Y}}\n\\]\nOur nonlinear activation function \\(f\\) is taken to operate on each row element one at a time, and we’ll let \\(f_i\\) denote the \\(i\\)th row of this completed activation, i.e.:\n\\[\nf_i = f\\left( \\sum_j X_{ij}w_j \\right) = \\tilde{Y}_i .\n\\]\nThe particular activation function we will use is the “sigmoid”,\n\\[\nf(x) = {1\\over{1+e^{-x}}},\n\\] – [click here to see a plot of this function](https://www.google.com/search?q=plot+1%2F(1%2Bexp(-x) – which has the derivative\n\\[\n{df\\over dx} = {e^{-x}\\over(1 + e^{-x})^2}\n\\] which can be shown (Hint: exercise for “mathy” students!) to simplify to \\[\n{df\\over dx}= f(1-f).\n\\]\nThe overall problem then amounts to finding the values of the “weights” \\(w_0, w_1,\\) and \\(w_2\\) so that the \\(\\tilde{Y}\\) we calculate is as close to the target \\(Y\\) as possible.\nTo do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e. all 4 rows. The loss function \\(L\\) we will choose is the mean square error loss, or MSE (note: later in Part 2 we will use a ‘better’ loss function for this problem):\n\\[\nL = {1\\over N}\\sum_{i=0}^{N-1} \\left[ \\tilde{Y}_i - Y_i\\right]^2,\n\\] or in terms of the activation function \\[\nL = {1\\over N}\\sum_{i=0}^{N-1} \\left[ f_i - Y_i\\right]^2.\n\\]\nEach of the weights \\(w_j\\) (\\(j=0..2\\)) will start with random values, and then be updated via gradient descent, i.e. \n\\[\nw_j^{new} = w_j^{old} - \\alpha{\\partial L\\over \\partial w_j}\n\\]\nwhere \\(\\alpha\\) is the learning rate, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is\n\\[\n{\\partial L\\over \\partial w_j} = {2\\over N}\\sum_{i=0}^{N-1} \\left[ \\tilde{Y}_i - Y_i\\right]{\\partial f_i \\over \\partial w_j}\\\\\n= {2\\over N}\\sum_{i=0}^{N-1} \\left[ \\tilde{Y}_i - Y_i\\right]f_i(1-f_i)X_{ij}.\n\\]\nAbsorbing the factor of 2/N into our choice of \\(\\alpha\\), and writing the summation as a dot product, and noting that \\(f_i = \\tilde{Y}_i\\), we can write the update for all the weights together as\n\\[\nw = w - \\alpha  X^T \\cdot \\left( [\\tilde{Y}-Y]*\\tilde{Y}*(1-\\tilde{Y})\\right)\n\\] where the \\(\\cdot\\) denotes a matrix-matrix product (i.e. a dot product for successive rows of \\(X^T\\)) and \\(*\\) denotes elementwise multiplication.\nTo clarify the above expression in terms of matrix dimensions, we can see that \\(w\\), a 3x1 matrix, can be made by multipyting \\(X^T\\) (a 3 x4 matrix) with the term in parentheses, i.e. the product of elementwise terms involving \\(\\tilde{Y}\\), which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix."
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#actual-code",
    "href": "posts/2019-01-30-my-first-neural-network.html#actual-code",
    "title": "My First Neural Network, Part 1",
    "section": "Actual Code",
    "text": "Actual Code\nThe full code for all of this is then…\n\n# Source: Slightly modified from Andrew Trask's code posted at \n#         https://iamtrask.github.io/2015/07/12/basic-python-network/\nimport numpy as np\n\n# sigmoid activation\ndef sigmoid(x,deriv=False): \n  if(deriv==True):\n    return x*(1-x)\n  return 1/(1+np.exp(-x))\n    \n# input dataset\nX = np.array([  [0,0,1],\n                [0,1,1],\n                [1,0,1],\n                [1,1,1] ])\n    \n# target output dataset            \nY = np.array([[0,0,1,1]]).T\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nnp.random.seed(1)\n\n# initialize weights randomly with mean 0\nw = 2*np.random.random((3,1)) - 1\n\nalpha = 1.0   # learning rate\n\nloss_history = []    # keep a record of how the loss proceeded, blank for now\nfor iter in range(1000):\n\n    # forward propagation\n    Y_pred = sigmoid(np.dot(X,w)) # prediction, i.e. tilde{Y}\n\n    # how much did we miss?\n    diff = Y_pred - Y\n    loss_history.append((diff**2).mean())   # add to the history of the loss\n\n    # update weights\n    w -= alpha * np.dot( X.T, diff*sigmoid(Y_pred, deriv=True))\n\nprint(\"Output After Training:\")\nprint(\"Y_pred = (should be two 0's followed by two 1's)\\n\",Y_pred)\nprint(\"weights =\\n\",w)\n\nOutput After Training:\nY_pred = (should be two 0's followed by two 1's)\n [[0.03178421]\n [0.02576499]\n [0.97906682]\n [0.97414645]]\nweights =\n [[ 7.26283009]\n [-0.21614618]\n [-3.41703015]]\n\n\nNote that, because of our nonlinear activation, we don’t get the solution \\(w_0=1, w_1=0, w_2=0\\).\nPlotting the loss vs. iteration number, we see…\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.loglog(loss_history)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#change-the-activation-function",
    "href": "posts/2019-01-30-my-first-neural-network.html#change-the-activation-function",
    "title": "My First Neural Network, Part 1",
    "section": "Change the activation function",
    "text": "Change the activation function\nAnother popular choice of activation function is the rectified linear unit or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e. a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise.\nClick here to see a graph of ReLU\nModifying our earlier code to use ReLU activation instead of sigmoid looks like this:\n\ndef relu(x,deriv=False):   # relu activation\n  if(deriv==True):\n    return 1*(x&gt;0) \n  return x*(x&gt;0)\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nnp.random.seed(1)\n\n# initialize weights randomly  (but only &gt;0 because ReLU clips otherwise)\nw = np.random.random((3,1))\n\nalpha = 0.3   # learning rate\n\nnew_loss_history = []    # keep a record of how the error proceeded\nfor iter in range(1000):\n\n    # forward propagation\n    Y_pred = relu(np.dot(X,w))\n\n    # how much did we miss?\n    diff = Y_pred - Y\n    new_loss_history.append((diff**2).mean())   # add to the record of the loss\n\n    # update weights\n    w -= alpha * np.dot( X.T, diff*relu(Y_pred, deriv=True))\n\nprint(\"Output After Training:\")\nprint(\"Y_pred = (should be two 0's followed by two 1's)\\n\",Y_pred)\nprint(\"weights =\\n\",w)\n\nOutput After Training:\nY_pred = (should be two 0's followed by two 1's)\n [[-0.]\n [-0.]\n [ 1.]\n [ 1.]]\nweights =\n [[ 1.01784368e+00]\n [ 8.53961786e-17]\n [-1.78436793e-02]]\n\n\n\n# Aside/Trivia (you can skip this cell): I find it interesting that apparently w2 = 1-w0\nprint( w[2] - (1-w[0]) )\n\n[-3.46944695e-17]\n\n\nPlot old results with new results:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.loglog(loss_history,label=\"sigmoid\")\nplt.loglog(new_loss_history,label=\"relu\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\nLooks like ReLU may be a better choice than sigmoid for this problem!"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#exercise-read-a-7-segment-display",
    "href": "posts/2019-01-30-my-first-neural-network.html#exercise-read-a-7-segment-display",
    "title": "My First Neural Network, Part 1",
    "section": "Exercise: Read a 7-segment display",
    "text": "Exercise: Read a 7-segment display\nA 7-segment display is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled \\(a\\) through \\(g\\) according to the following diagram:\n\n\n\n7-segment diagram\n\n\n\nDiagram of the network\nThe 7 inputs “a” through “g” will be mapped to 10 outputs for the individual digits, and each output can range from 0 (“false” or “no”) to 1 (“true” or “yes”) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function \\(f\\)):\n\n\n\ndiagram of 7-seg network\n\n\n…where again, this network operates on a single data point at a time, datapoints which are rows of X and Y. What is shown in the above diagram are the columns of \\(X\\) and \\(Y\\) for a single row (/ single data point).\n\n\nCreate the dataset\nLet the input X be the segments \\(a\\) through \\(g\\) are the columns of the input \\(X\\), and are either 1 for on or 0 for off. Let the columns of the target \\(Y\\) be the digits 0-9 themselves arranged in a “one hot” encoding scheme, as follows:\n\n\n\n\nDigit\n\n\nOne-Hot Encoding for \\(Y\\)\n\n\n\n\n0\n\n\n1,0,0,0,0,0,0,0,0,0\n\n\n\n\n1\n\n\n0,1,0,0,0,0,0,0,0,0\n\n\n\n\n2\n\n\n0,0,1,0,0,0,0,0,0,0\n\n\n\n\n…\n\n\n…\n\n\n\n\n9\n\n\n0,0,0,0,0,0,0,0,0,1\n\n\n\n\nThe values in the columns for \\(Y\\) are essentially true/false “bits” for each digit, answering the question “Is this digit the appropriate output?” with a “yes”(=1) or “no” (=0) response.\nThe input \\(X\\) will be a 10x7 matrix, and the target \\(Y\\) will be a 10x10 matrix. Each row of \\(X\\) will be the segments to produce the digit for that row. For example, the zeroth row of \\(X\\) should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0].\nDefine numpy arrays for both \\(X\\) and \\(Y\\) (Hint: for \\(Y\\), check out np.eye()):\n\n# Students: fill these out completely for what the X and Y *should* be\n# for the 7-segment display.  The following is just a \"stub\" to get you started.\nX = np.array([ [1,1,1,1,1,1,0],\n               [],\n               [],\n               [] ])\nY = np.array([ [1,0,0,0,0,0,0,0,0,0],\n               [],\n               [] ])\n\n\n\nInitialize the weights\nPreviously the dimensions of the weight matrix \\(w\\) were 3x1 because we were mapping each row of 3 elements in \\(X\\) to each row of 1 element of \\(Y\\). For this new problem, each row of \\(X\\) has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of \\(Y\\), so what should the dimensions of the weights matrix \\(w\\) be?\nWrite some numpy code to randomly initialize the weights matrix:\n\nnp.random.seed(1)  # initial RNG so everybody gets similar results\n\nw = np.random.random(( , ))    # Students, fill in the array dimensions here\n\nSyntaxError: ignored\n\n\n\n\nTrain the network\nHaving created an \\(X\\) and its matching \\(Y\\), and initalized the weights \\(w\\) randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e. train it to recognize digits on 7-segment displays. Do this below.\n\n# Students, copy training code from above and paste it here. \n#  Use sigmoid activation, and 1000 iterations, and learning rate of 0.9\n#    Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice?\n\n\n# And then print out your Y_pred & weights matrix, and limit it to 3 significant digits\nprint(\"Output After Training:\")\nnp.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) # 3 sig figs\nprint(\"Y_pred=\\n\",Y_pred)\nprint(\"weights =\\n\",repr(w))  # the repr() makes it so it can be copied & pasted back into Python code"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#final-check-keras-version",
    "href": "posts/2019-01-30-my-first-neural-network.html#final-check-keras-version",
    "title": "My First Neural Network, Part 1",
    "section": "Final Check: Keras version",
    "text": "Final Check: Keras version\nKeras is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset:\n\n# Just a demo of how one might similarly train a network using Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nmodel = Sequential([\n    Dense(10, input_shape=(7,)),\n    Activation('sigmoid') ])\nmodel.compile(optimizer='adam',   # We'll talk about optimizer choices and loss choices later\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(X, Y, epochs=200, batch_size=1)\nprint(\"\\nY_tilde = \\n\", model.predict(X) )"
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#optional-if-you-want-to-go-really-crazy",
    "href": "posts/2019-01-30-my-first-neural-network.html#optional-if-you-want-to-go-really-crazy",
    "title": "My First Neural Network, Part 1",
    "section": "Optional: If you want to go really crazy",
    "text": "Optional: If you want to go really crazy\nHow about training on this dataset: \\[ \\overbrace{\n\\left[ {\\begin{array}{cc}\n   0 & 0 \\\\\n   0 & 1 \\\\\n   1 & 0 \\\\\n   1 & 1 \\\\\n  \\end{array} } \\right]\n}^{X} \\rightarrow\n\\overbrace{\n\\left[ {\\begin{array}{c}\n   0   \\\\\n   1  \\\\\n   1  \\\\\n   0 \\\\\n  \\end{array} } \\right]\n  }^Y.\n\\] Good luck! ;-)\n(Hint 1: This problem features prominently in the history of Neural Networks, involving Marvin Minsky and “AI Winter.”\nHint 2: This whole lesson could instead be entitled “My First Artificial Neuron.”)\nNext time, we will go on to Part 2: Bias and CE Loss."
  },
  {
    "objectID": "posts/2019-01-30-my-first-neural-network.html#additional-optional-exercise-binary-math-vs.-one-hot-encoding",
    "href": "posts/2019-01-30-my-first-neural-network.html#additional-optional-exercise-binary-math-vs.-one-hot-encoding",
    "title": "My First Neural Network, Part 1",
    "section": "Additional Optional Exercise: Binary Math vs. One-Hot Encoding",
    "text": "Additional Optional Exercise: Binary Math vs. One-Hot Encoding\nFor the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false “bits” for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations.\n\nConstruct the target output matrix \\(Y\\) for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s).\nUsing this \\(Y\\) array, train the network as before, and plot the loss as a function of iteration.\n\nQuestion: Which method works ‘better’? One-hot encoding or binary encoding?"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html",
    "href": "posts/2022-01-24-multidim-dotproducts.html",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "",
    "text": "One sometimes sees claims like “within high dimensional spaces, if you choose random vectors, they tend to be orthogonal to each other,” but is this true?\nThe answer depends on whether you’re talking about unit vectors or not.\nMore to the point, it depends on the context in which the above nugget of wisdom is being applied, and on not so much orthogonality per se as what we might call “near orthogonality”: Are we interested in cases when the dot product is nearly zero, or when the cosine of the angle between the vectors is nearly zero? (In the case of exact orthogonality, the dot product and cosine are both exactly zero.) The latter statement is equivalent to restricting attention to unit vectors, and also is isomorphic to considering the Pearson correlation coefficient between two random signals.\nLet’s do some direct computations. We’ll vary the number of dimensions and compute a bunch of dot products between random vectors, and then plot histograms of these dot product values. The sharper the distribution is around the value of zero, the more likely it is for vectors to be orthogonal.\n#all_slow"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#general-setup",
    "href": "posts/2022-01-24-multidim-dotproducts.html#general-setup",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "General Setup",
    "text": "General Setup\nImports and global settings\n\n# General Setup\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport scipy.stats as ss\nfrom IPython.display import display\n\nnp.random.seed(42)\n\nn = 50000  # number of random pairs of vectors to try\ndims = [2,3,5,10,100,1000]  # different dimensionalities to try. Same as \"D\" in plots below\nresults = []"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#utility-routines",
    "href": "posts/2022-01-24-multidim-dotproducts.html#utility-routines",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Utility Routines",
    "text": "Utility Routines\nDefine some functions that we’re likely to use multiple times.\n\n\nShow the code\n# Utility routines \n\ndef norm_rows(arr):   \n    \"normalize vectors which exist as rows with components as columns\"\n    arrT = arr.T                               # .T is to make broadcasting work easily \n    mags = np.sqrt( (arrT*arrT).sum(axis=0) )  # vector magnitudes\n    return (arrT/mags).T\n\ndef uniform_rand(size=(10,10)):\n    \"random number on [-1..1]\"\n    return np.random.uniform(low=-1.0, high=1.0, size=size)\n\ndef softmax(x):\n    \"good ol' softmax function, for a bunch of row vectors\"\n    xT = x.T  # .T's are just to make broadcasting work out.\n    vec_max = np.max(xT,axis=0)\n    e_x = np.exp(xT - vec_max)   \n    return (e_x / e_x.sum(axis=0)).T\n\n\ndef make_plot(n=n, dims=dims, title='', rand_func=uniform_rand, normalize=False,\n             color='#1f77b4', xlim=None, softmax_them=False, scale_x_by_sqrtd=False):\n    \"\"\"A function we'll call again & again to make plots with\"\"\"\n    fig, axes = plt.subplots(2,3, figsize=(12,6))\n    fig.suptitle(title, fontsize=\"x-large\")\n    axes = axes.ravel()\n    sds = []                      # list of standard devations for different dims\n    for i,ax in enumerate(axes):\n        dim = dims[i]\n        a = rand_func(size=(n,dim))\n        b = rand_func(size=(n,dim))\n        if softmax_them: a, b = softmax(a), softmax(b)\n        if normalize: a, b = norm_rows(a), norm_rows(b)\n        dots = (a*b).sum(axis=-1)\n        if scale_x_by_sqrtd: dots /= np.sqrt(dim)\n        std = np.std(dots,axis=0)   # measure standard dev of distribution of dot product\n        ax.hist(dots, density=True, bins=100, label=f'D={dim}, $\\sigma$={std:3.2g}', color=color)  \n        sds.append(std)\n        if xlim!=None: ax.set_xlim(xlim)\n        ax.legend(loc=4)   # legend will show dimensions and std dev\n        if i%3==0: ax.set_ylabel('Probability')\n        if i&gt;2: ax.set_xlabel('Dot product value')\n    return sds, axes, fig   # return some axes & figure info too in case we want to replot\n\n\nNow we can get started by looking at distributions of dot products for various kinds of vectors. Starting with…"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#uniform-distribution-non-unit-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#uniform-distribution-non-unit-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Uniform distribution, Non-Unit Vectors",
    "text": "Uniform distribution, Non-Unit Vectors\nThese will be vectors that fill a hypercube, i.e. each component can take on a value on [-1…1].\n\ntitle = \"Uniform distribution, Non-Unit Vectors\"\nsds, axes, fig =  make_plot(title=title)\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\n\n\nFrom the above graph, we see that as the dimensionality D increases, it’s still the case that the most common dot product is zero, but the probability distribution becomes wider & flatter, i.e. the chances that a pair of random vectors are not orthogonal becomes increasingly more likely as D increases.\n\n\n\n\n\n\nNote: Normalizing by sqrt(D)\n\n\n\nOoo, ooo! Some of you may have seen how in the “scaled dot product attention” used in Transformer models, they normalize by the square root of the number of dimensions? Let’s try that on the above graphs and see if the variance (i.e. the scale on the horizontal axis) stays roughly the same:\n\ntitle = \"Uniform distribution, Non-Unit Vectors, Scaled by 1/sqrt(D)\"\nsds, axes, fig =  make_plot(title=title, scale_x_by_sqrtd=True)\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\n\n\nSee how the variance stays constant as D increases? Cool, right? ;-)"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#uniform-distribution-unit-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#uniform-distribution-unit-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Uniform distribution, Unit vectors",
    "text": "Uniform distribution, Unit vectors\n\ntitle = \"Uniform distribution, Unit vectors\"\nsds, axes, fig = make_plot(title=title, normalize=True, color='orange', xlim=[-1,1])\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\n\n\nWait, back up! Those orange graphs remind me of a beta distribution. Can we fit that? Let’s try…\n\ndef fit_beta(axes):\n    x = np.linspace(0,1,100)\n    for i,ax in enumerate(axes):\n        dim = dims[i]\n        alpha = (dim-1)/2                     # this seems to work quite well\n        y = 0.5*ss.beta.pdf(x, alpha, alpha)  # just messing with beta distribution\n        ax.plot(2*x-1, y, color='green')\n        \nfit_beta(axes)\nfig\n\n\n\n\n(Turns out that that beta distribution can be derived symbolically. Lucky guess on my part. ;-) )\nInteresting that for D=2, the vectors are more likely to be parallel or antiparallel than orthogonal, but that changes as D increases.\nIn the above graphs we see the opposite trend from the previous non-unit-vector case: the distribution gets narrower as D increases, meaning that a given pair of random unit vectors are more likely to be orthogonal as D increases.\nWhat if we draw from a normal distribution of components instead of a uniform one?"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#normal-distribution-non-unit-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#normal-distribution-non-unit-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Normal Distribution, Non-Unit Vectors",
    "text": "Normal Distribution, Non-Unit Vectors\n\ntitle = \"Normal distribution, Non-Unit Vectors\"\nsds, axes, fig =  make_plot(title=title, rand_func=np.random.normal)\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\n\n\nOK, same trend as before: the distribution gets wider as dimensionality increases. What about for unit vectors?"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#normal-distribution-unit-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#normal-distribution-unit-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Normal distribution, Unit Vectors",
    "text": "Normal distribution, Unit Vectors\n\ntitle = \"Normal distribution, Unit vectors\"\nsds, axes, fig = make_plot(title=title, rand_func=np.random.normal,\n                           normalize=True, color='orange', xlim=[-1,1])\nresults.append({'label': title, 'sds': sds })  # save for later\nfit_beta(axes)\n\n\n\n\nLooks like the normal distribution case is the same – wow, exactly the same – as the uniform distribution only more extreme: for unit vectors, the distribution gets narrower (around 0) as the dimension increases.\n\n\n\n\n\n\nNote\n\n\n\nIf the vectors have only positive components (e.g. what you get from softmax) then you will never have orthogonal vectors (because they all exist in the “positive subspace”). But lets see what happens with such “softmax vectors”:"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#softmax-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#softmax-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "“Softmax Vectors”",
    "text": "“Softmax Vectors”\nIf the vectors all have positive coefficients, e.g. because they are the output of a softmax function, then there’s no way they can be orthogonal. We might ask, what’s the most common value for the dot product?\n(Note that these are not unit vectors according to the L2/“Euclidean” norm, rather they are unit vectors according to an L1/“Manhattan distance” norm).\n\ntitle = \"Softmax vectors, not unit in L2\"\nsds, axes, fig = make_plot(title=title, normalize=False, softmax_them=True, color='red')\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\nfor i,ax in enumerate(axes):  # seems the mean is 1/D; let's show that:\n    dim = dims[i]\n    ax.plot([1/dim,1/dim],[0,ax.get_ylim()[1]], label='vert bar at 1/D')\n\n\n\n\nGiven that the dot product / cosine similarity for these vectors seems to consistently have an expectation value of \\(1/D\\) for dimensions \\(D\\), then as \\(D\\) increases this value will go to zero, and thus the vectors will be increasingly “orthogonal”."
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#cosine-similarity-for-softmax-vectors",
    "href": "posts/2022-01-24-multidim-dotproducts.html#cosine-similarity-for-softmax-vectors",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Cosine Similarity for Softmax Vectors",
    "text": "Cosine Similarity for Softmax Vectors\nWhat about the cosine between the vectors we just did? That’s equivalent to normalizing the softmax vectors according to an L2/Eucliean norm. Here we go:\n\ntitle = \"Cosine Similarity for Softmax\"\nsds, axes, fig = make_plot(title=title, normalize=True, softmax_them=True, color='purple')\nresults.append({'label': title, 'sds': sds})  # save for later\n\n\n\n\n…yeah I dunno ’bout that. Seems to be approaching a number a bit larger than 0.76. ?? I can’t immediately think any “special” number that matches that (e.g., it’s too low to be \\(\\pi\\)/4 and much too high to be \\(1/\\sqrt{2}\\)). Let’s keep going to higher dimensions and see what the mean converges to:\n\n# let's keep going higher\nnew_dims = [10000,20000,50000]\nfor dim in new_dims:\n    a = 2*np.random.uniform(size=(n,dim)).astype(np.float32) - 1 # gotta watch for mem overruns\n    b = 2*np.random.uniform(size=(n,dim)).astype(np.float32) - 1\n    a, b = softmax(a), softmax(b) \n    a, b = norm_rows(a), norm_rows(b)  # normalize to get cosine\n    dots = (a*b).sum(axis=-1)\n    std = np.std(dots,axis=0)\n    print(dim, dots.mean(), std)\n\n10000 0.7616262 0.002711203\n20000 0.7616077 0.0019324023\n50000 0.76159537 0.001209799\n\n\nLikely candidate special number is the hyperbolic tangent of 1:\n\nnp.tanh(1)\n\n0.7615941559557649\n\n\n…though I’m not sure how that arises. Hmmm!"
  },
  {
    "objectID": "posts/2022-01-24-multidim-dotproducts.html#summary",
    "href": "posts/2022-01-24-multidim-dotproducts.html#summary",
    "title": "Likelihood of Vector Orthogonality in High-Dimensional Spaces",
    "section": "Summary",
    "text": "Summary\nIn all cases, the mode of the distribution is still at zero dot product. So we can say with confidence that vectors are most likely to be (near-)orthogonal regardless of the dimensionality. However we see that although for unit vectors the distribution of dot product values gets sharper (around zero) as the number of dimensions increases, for non-unit vectors it gets flatter, i.e. betting on orthogonality becomes increasinly less of a safe bet as you go to higher dimensions.\nThe following graph summarizes our results, in terms of the standard deviation of the distribution of dot product values:\n\nfig, ax = plt.subplots()\nfor k in range(len(results)):\n    marker, markersize = ('s-',12) if k==1 else ('o-',11)\n    ax.loglog(dims, results[k]['sds'], marker, markersize=markersize, label=results[k]['label'])\nplt.legend()\nplt.xlabel('Dimensions')\nplt.ylabel('Std Dev')\nplt.title(\"For dot products of random vectors...\")\nplt.show()\n\n\n\n\n(where the orange line and the red line are right on top of each other)\nThus we see that the dot products of unit vectors, “softmax vectors” (positive definite with unit L1 norm), as well as the cosine similarity for the latter, are all more likely to be zero as the dimensionality increases, whereas for other vectors the dimensionality trend goes the other way.\nAlso: can I just say that I really like the scaling laws apparent in that last plot."
  },
  {
    "objectID": "posts/2022-11-15-multidim-shells.html",
    "href": "posts/2022-11-15-multidim-shells.html",
    "title": "Shell Volumes and Distances in High-Dimensional Spaces",
    "section": "",
    "text": "Some of us are still not accustomed to thinking in terms of very high numbers of dimensions – myself included. Earlier this year, I made a post on vector orthogonality in high-dimensional spaces.\nThis post is about the volume of high-dimensional spaces. Even in two dimensions, you may be aware of facts such as that a 20-inch pizza has more than twice the area of a 14-inch pizza:\nprint(\"Areas =\",[3.14*r**2 for r in [14, 20]])\n\nAreas in square inches = [615.44, 1256.0]\nAs we go to higher dimensions, such observations become more stark, for example that a sphere of radius 4 has eight times the volume of a sphere of radius 2."
  },
  {
    "objectID": "posts/2022-11-15-multidim-shells.html#intuition-from-geometry-volumes-of-n-balls",
    "href": "posts/2022-11-15-multidim-shells.html#intuition-from-geometry-volumes-of-n-balls",
    "title": "Shell Volumes and Distances in High-Dimensional Spaces",
    "section": "Intuition from Geometry: Volumes of n-balls",
    "text": "Intuition from Geometry: Volumes of n-balls\nLet’s consider the volume of a multidimensional “n-ball” of dimension \\(n\\) and radius \\(r\\), which is given by this recurrence relation: \\[ {\\displaystyle V_{n}(r)={\\begin{cases}1&{\\text{if }}n=0,\\\\[0.5ex]2r&{\\text{if }}n=1,\\\\[0.5ex]{\\dfrac {2\\pi }{n}}r^{2}\\times V_{n-2}(r)&{\\text{otherwise}}.\\end{cases}}} \\]\nSo, for example, \\(V_2(r) = \\pi r^2\\), \\(V_3(r) = {2\\pi\\over 3}r^2\\times 2r = {4\\over 3}\\pi r^3\\),…and so on.\nLet’s consider the percentage of the total volume for a shell of thickness \\(\\Delta r\\), as a function of both \\(r\\) and \\(n\\). In the following we’re going to plot \\(V_n(r+\\Delta r) - V_n(r)\\) as we vary \\(n\\), and let’s take steps of size \\(\\Delta r\\) = r/10 and let \\(n=2,3,4\\)…\n\nimport matplotlib.pyplot as plt\nimport numpy as np \n\ndef volume(n, r):\n    \"a recursive formula for the volume of an n-ball of radius r\"\n    if 0==n: return 1\n    if 1==n: return 2*r\n    return 2*np.pi/n * r**2 * volume(n-2, r)\n\ndef plot_fractions(ns, dr=1/10):\n    rs = np.linspace(0,1-dr,num=int(1/dr)) #we'll let r go from 0 to 1\n    #print(\"rs = \",rs)\n    for n in ns:\n        v_dr = (volume(n, rs+dr) - volume(n, rs))  # volume of the shell\n        fractions = v_dr/volume(n, 1)              # fraction of total volume\n        plt.plot(rs, fractions, label=f'n={n}')\n        assert 1-np.sum(fractions) &lt; 1e-6, \"Error: fractions don't add up to 1\"\n    plt.legend()\n    plt.xlabel('r')\n    plt.ylabel('fraction of total volume')\n    plt.show()\n    \nplot_fractions([2,3,4])\n\n\n\n\nIf we go to larger numbers of dimensions, we see that almost the entire volume is made up of a thin shell near the perimeter:\n\nplot_fractions([10,25,50,100], dr=1/50)\n\n\n\n\nIn Deep Learning applications, it’s not uncommon to have dimensions \\(n\\) which are 64, 256, even over 1000 dimensions. For data points which populate such a high dimensional n-ball with uniform density, essentially all of them will lie “on the surface” of the n-ball.\nWe may want to know about the distance between such points. If they’re inside a “volume”, we might think than a Euclidean-type distance would be the best. But if they pretty much all lie along the surface of a sphere, would that really be any better or worse that something like an angular distance or the frequently used cosine similarity?\nHmmm. Good question. We should consider something else first before we go on."
  },
  {
    "objectID": "posts/2022-11-15-multidim-shells.html#hold-up.-an-n-ball-of-uniform-density-how-relevant-is-that",
    "href": "posts/2022-11-15-multidim-shells.html#hold-up.-an-n-ball-of-uniform-density-how-relevant-is-that",
    "title": "Shell Volumes and Distances in High-Dimensional Spaces",
    "section": "Hold up. “An n-ball of uniform density”: How relevant is that?",
    "text": "Hold up. “An n-ball of uniform density”: How relevant is that?\nThat depends.\nPreviously we were imagining just the volume of the n-ball itself, then we imagined it being populated “uniformly” by data points. But is that likely to happen? In other words, if we’ve got \\(n\\) random variables that are each uniformly distributed on \\([0..1]\\), then are we saying that the average radius (or distance from the origin) is going to be close to 1? Uh…. let’s check:\n\nfor n in [2,3,4,10,20]:\n    x = np.random.rand(n)  # some random vector\n    print(f\"n = {n}, distance = {np.sqrt(np.sum(x**2))}\")\n\nn = 2, distance = 0.4531853366702091\nn = 3, distance = 1.0826001130628344\nn = 4, distance = 1.2805569079817343\nn = 10, distance = 1.965822638795274\nn = 20, distance = 2.9421432188892176\n\n\n…so, no. We can actually get greater distances than 1 this way. That means that such points are not within the unit n-ball.\nBut what if these are the kinds of points we’re mostly likely to get in our Data Science and Deep Learning investigations? That depends on our data, and how it may (or may not) be normalized.\nIf we have such random, independent variables, which may lie outside the unit n-ball, what can we say about their distances? Well, the above list of outputs were were just one vector for each dimension. Let’s take a look a a number (…ah we can’t use the letter \\(n\\), let’s say) \\(m\\) random vectors in \\(n\\) dimensions…\n\nns =  [2,3,4,10,32,64,128,256,512,1024,2048] # number of dimensions\nm = 1000  # number of random vectors in each dimension\navg_distances = []\nfor n in ns:\n    x = np.random.rand(m,n)\n    distance = np.sqrt(np.sum(x**2, axis=1))\n    avg_distance = np.mean(distance)\n    print(f\"n = {n}, avg distance from origin = {avg_distance}\")\n    avg_distances.append(avg_distance)\n\nn = 2, avg distance from origin = 0.7481821823467171\nn = 3, avg distance from origin = 0.9691494533397342\nn = 4, avg distance from origin = 1.124886358111731\nn = 10, avg distance from origin = 1.8035216690887792\nn = 32, avg distance from origin = 3.261533859767904\nn = 64, avg distance from origin = 4.607289814395833\nn = 128, avg distance from origin = 6.527956044174336\nn = 256, avg distance from origin = 9.238162970649888\nn = 512, avg distance from origin = 13.071335971436879\nn = 1024, avg distance from origin = 18.464564627923217\nn = 2048, avg distance from origin = 26.135526554018735\n\n\nIt kind of looks like a (sideways) parabola!\n\nplt.plot(ns,avg_distances, label='avg_distance')\nplt.plot(ns, np.sqrt(np.array(ns)/3), '--', label='$\\sqrt{n}/3$') # what if we also plot sqrt(n)* a constant? \nplt.legend()\nplt.ylabel('avg distance')\nplt.xlabel('$n$')\nplt.show()\n\n\n\n\nNow, if we were to normalize these points so that they had some similar magnitude, that’d be fine, although if we normalized using the Euclidean norm that would force the points to lie on the surface of the n-ball, i.e. an n-sphere. That’s kind of boring, right? ;-)\nMore interesting would probably be those points which have been normalized according to the frequently-used softmax function,\n\\[ {\\rm softmax}(x_i) = {e^{x_i} \\over \\sum_j e^{x_j}},\\]\nwhich will force them to have unit magnitude according to the L1 norm or “Manhattan Distance” which was the subject of earlier investigations in this blog.\nSo, what we’ll do is we’ll choose random variables on some interval – but will they be uniform, Gaussian, or some other choice? – and then softmax them, and then have a look at their distance from the origin, and then, more importantly, their distances from each other.\n\ndef softmax(x):\n    xmax = np.max(x) # this little trick can help with numerical accuracy\n    return np.exp(x-xmax) / np.sum(np.exp(x - xmax))\n\n\nns =  [2,3,4,10,32,64,128,256,512,1024,2048] # number of dimensions\nm = 1000  # number of random vectors in each dimension\navg_distances = []\nfor n in ns:\n    x = 2*np.random.rand(m,n)-1  # let each value vary on -1..1 before we softmax them\n    x = softmax(x)               # softmax will force everything to have L1 norm = 1\n    distance = np.sqrt(np.sum(x**2, axis=1)) # measure distance according to L2/Euclidean norm\n    avg_distance = np.mean(distance)\n    print(f\"n = {n}, avg distance from origin = {avg_distance}\")\n    avg_distances.append(avg_distance)\n\nns, avg_distances = np.array(ns), np.array(avg_distances)\nfit = 11e-4/ns**(1/2)\nfig, ax = plt.subplots(1,2, figsize=(10,4))\nax[0].plot(ns,avg_distances, label='data points')\nax[0].plot(ns,fit, '--', label='rough fit$\\sim 1/\\sqrt{n}$')\nax[1].plot(np.log10(ns),np.log10(avg_distances), label='data points')\nax[1].plot(np.log10(ns),np.log10(fit), '--', label='rough fit$\\sim 1/\\sqrt{n}$')\n\nax[0].set_ylabel('avg radius')\nax[0].set_xlabel('$n$')\nax[1].set_ylabel('log10(avg radius)')\nax[1].set_xlabel('log10($n$)')\nfor i in [0,1]: \n    ax[i].legend()\n    ax[i].set_title(\"Softmaxxed uniform vectors\")\nplt.show()\n\nn = 2, avg distance from origin = 0.0007511397255709152\nn = 3, avg distance from origin = 0.0006308267488890856\nn = 4, avg distance from origin = 0.0005510334461844213\nn = 10, avg distance from origin = 0.00035750458935601627\nn = 32, avg distance from origin = 0.00020173684564505517\nn = 64, avg distance from origin = 0.00014270354448691296\nn = 128, avg distance from origin = 0.00010118972374124984\nn = 256, avg distance from origin = 7.159922929099627e-05\nn = 512, avg distance from origin = 5.064894541153353e-05\nn = 1024, avg distance from origin = 3.579446707831397e-05\nn = 2048, avg distance from origin = 2.5317769100231054e-05\n\n\n\n\n\nSo does this mean that for larger numbers of dimensions, the “softmaxxed” points tend to get closer to the origin, rather than the perimeter? Well, not necessarily from these graphs alone. These are showing the average, which could be distorted by large values, for example. So let’s do a series of plots for different values of \\(n\\).\n….TODO. I’ll need to come back and fill this in later.\nFor now, it’s an exercise for the student/professor. ;-)"
  },
  {
    "objectID": "posts/2021-09-15-Handy-Bash-Functions.html",
    "href": "posts/2021-09-15-Handy-Bash-Functions.html",
    "title": "Handy Bash Functions for ML & Data Science",
    "section": "",
    "text": "Once I figure out a complicated command, I make a bash function or alias for it. Here are some that I find useful:\n# set up python environment.\n# usage: $ makeenv &lt;env_name&gt;\n# Note: the pip update is b/c sometimes I've gotten stuck with an ancient pip where nothing works!\nmakeenv() { python3 -m venv ~/envs/\"$1\"; source ~/envs/\"$1\"/bin/activate; python3 -m pip install pip -Uqq; }\n\n# loads python environment.  \n# usage: $ loadenv &lt;env_name&gt;\nloadenv() { source ~/envs/\"$1\"/bin/activate; }\n# same thing different name\ngimme() { source ~/envs/\"$1\"/bin/activate; }\n\n# downloads colab notebook.  [Writes to colab.ipynb]\n# usage: $ grabcolab &lt;sharing_URL&gt;    \ngrabcolab() { fileid=$( echo \"$1\" | sed -E 's/.*drive\\/(.*)\\?.*/\\1/' ); wget -O colab.ipynb 'https://docs.google.com/uc?export=download&id='$fileid; }\n\n# runs a notebook on command line, as if it were a python script.  \n# usage: $ nbrun colab.ipynb  \n# [Note for student nbs, run this from an access-reduced account!]\n# On my personal page I remove the pip install but put it in to make it handy for students\nnbrun() { pip install -Uqq jupytext; jupytext --to py \"$1\";  mv  \"${1%.*}\".py run_this.ipy; ipython run_this.ipy;}\n\n# creates .tgz file from directory, runs in parallel\n# $ usage: $ partar &lt;dir_name&gt;  [no / on end.   requires pigz: $ sudo apt install pigz\npartar() { tar -I pigz -cf \"$1.tgz\" \"$1\";  }\n\n# uncompresses .tgz file into directory, runs in parallel.\n# usage $ paruntar &lt;.tgz file&gt;\nparuntar() { tar -I pigz -xf \"$1\";  }  \n\n# long-list sorted by timestamp, with more-style pagination.  $@ is all arguments.\n# usage: ltt *.ipynb\nllt() { ls --color=auto -p -lth --color=always \"$@\" | less -R -X --quit-if-one-screen; }\n\n# which alias is this?  includes functions too.  \n# example: $ wa partar\nwa() { type \"$1\";}\n…these are all in my ~/.bash_aliases file, which is invoked at the bottom of my ~/.bashrc file in the line: source ~/.bash_aliases.\nNote that \"$1\" just refers to the first argument of the command.\n\nI used to use Anaconda & conda but found it “bulky” and “invasive”, so now I just use python venv.\n\nImage via VQGAN+CLIP, prompt=‘computer code | bash shell scripting | data science | by james gurney | trending on artstation’"
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html",
    "title": "Mac => Linux Switch Notes",
    "section": "",
    "text": "Mac and I aren’t breaking up, we’re just seeing other people."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#intro",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#intro",
    "title": "Mac => Linux Switch Notes",
    "section": "Intro",
    "text": "Intro\nI’d been jonesing for a laptop with a real GPU for a couple years. Granted, most of my computation occurs on remote servers, but sometimes when I’m traveling, those connections can be spotty. And heck, I might want to play the occasional video game (without NVIDIA GeForce NOW).\nTwenty years ago, I switched from Linux/Windows dual-boot to Mac so I could get shiny apps, and to not have to worry about device drivers, etc. But with Apple dragging their feet re. hardware upgrades, I finally took the plunge – before my Mac died, so I’d have plenty of time to transition.\nGot a System76 Oryx Pro. With Pop!_OS 19.10 — I’m used to Ubuntu, so this is a bit new. I’m so impressed with the System76 folks that I want to try to stay with their Pop!_OS because I know they’ve been doing the tweaking so it’ll work well with their laptops, and also so I can legit bug them when I hit problems. ;-) Like Ubuntu, Pop!_OS is a Debian variant, so much of the same actions apply.\nSimilar posts by others:\n\nhttps://www.theportlandcompany.com/2017/08/14/switching-mac-ubuntu-gnome-17-04/\nhttp://www.matthewbuscemi.com/blog/2019-06-05-the-great-linux-migration-of-2019.html"
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#syncing-with-apple-ecosystem",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#syncing-with-apple-ecosystem",
    "title": "Mac => Linux Switch Notes",
    "section": "Syncing with Apple Ecosystem",
    "text": "Syncing with Apple Ecosystem\n(from within Linux. See below for alleged VirtualBox installation of MacOS)\nI still have an iPad & iPhone, so finding a solution that could sync with what I already have (rather than, e.g., going with Google Calendars) was helpful.\nOne could simply use iCloud.com via a web browser, but I'd like to integrate a bit more with the Linux ecosystem.\n\nMail:\nI tried Geary, Thunderbird & Evolution. Found more online instructions for using Evolution. Evolution doesn’t to do like Mac’s Mail.app does, namely integrate all your inboxes into one unified Inbox – you have to go an check each inbox for each account. But here’s a workaround to fake it. (Thunderbird supports a nice unified inbox view. Note that Thunderbird’s font size is minuscule, but you can rescale it.)\nMy account types: Gmail and Microsoft Exchange.\nCalendar:\nEvolution will integrate with Apple: https://ar.al/2018/08/05/using-icloud-calendars-on-gnu-linux/\nNote that accounts added in Evolution will propagate to default Gnome Calendar app too, but there’s no way to get Gnome to add new Apple calendars; only Evolution provides this functionality, it seems.\nContacts:\nEvolution again: https://ar.al/2018/08/05/getting-your-icloud-contacts-on-gnu-linux/\nSadly, unlike the Mac, when you try to use the contacts, for example when you start addressing an email, it won’t prioritize the search/matching list by the most recent people you’ve actually contacted. Rather the names of people you haven’t interacted with in years will dominate the list of names.\nNotes (wrapper to web app):\niCloud Notes Snap: https://www.addictivetips.com/ubuntu-linux-tips/use-icloud-on-linux/\nAll this thing is is a web browser that connects to Apple's iCloud notes page. So no using this offline. Note that you have to log out & back in before you see Snap apps. Also note that Hide/Minimize on this window will make it almost impossible to recover. (??) Thus I tend to move the window to another desktop.\nMessages (SMS)\nTL/DR: It’s a no-go.\nThere is no iCloud version of Messages. Apple won't allow 3rd party apps.\nGnome Phone Manager was supposed to let you pair with your phone via Bluetooth and then send & receive messages, but it’s obsolete now and I couldn’t get it to work.\nKDE has a \"Connect\" app, but I'm not abandoning all the Gnome setup I’ve done to switch to KDE! And it probably only works with Android.\nWammu is unable to connect to my (Bluetooth-paired) phone.\nYou could configure your old Mac as a remote Messages server, but this implementation is insecure: https://github.com/cvincent/umessage\nVerizon doesn't offer a Linux version of their Message+ app, but you can send/receive via their web site.\n…One could always use some alternate messaging ‘ecosystem’ like WhatsApp’s web app or Signal, but I was looking for ways to get ‘regular’ SMS messages for my existing number. :-/\nFiles:\nHonestly Dropbox is fine! I turned on ssh ‘sharing’ on my Mac so I can scp files if I need to, but Dropbox on Linux is as easy as one would imagine.\nMusic:\nFor actual audio production, I’ll need to keep the Mac around for Logic & Pro Tools. As far as casual listening to music,…eh. I’m fine with Pandora. (I’m not a Spotify user.) Note: Did get studio audio working under Linux though! See below under “Audio”.\nBackups:\nI have an Apple “Time Capsule” WiFi-cum-backup server. It’s not going to be possible to back up both the Mac and the Linux machine to the same device. I’ll need to choose which machine to back up there. Some have suggested getting a Raspberry Pi and mount a $30 2TB HD to it for backing up the Linux machine. TODO."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#cuda-tensorflow-pytorch",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#cuda-tensorflow-pytorch",
    "title": "Mac => Linux Switch Notes",
    "section": "CUDA / Tensorflow / PyTorch",
    "text": "CUDA / Tensorflow / PyTorch\nTL/DR: this was the grand hope of buying this computer. Still not able to run yet, but I’ll never give up. ;-)\nThis main challenge, as with any system really, is grabbing a version of CUDA that will allow you to grab at least a few binaries:\n\nLatest CUDA via system76's distro is 10.2. They don’t offer 10.1 anymore.\nPyTorch builds only support up to 10.1. To get CUDA 10.2 support, I built PyTorch from source. My research code isn’t running…I suspect because something in PyTorch changed re. multiprocessing support. :-(.\nSystem76 made a \"tensorman\" docker util. I confess to my shame that I still find docker to be rather confusing. Using tensorman, I was able to run their tiny demo but not my own research codes. I tried building my own docker image but….didn't enjoy it. Went back to what I know: Anaconda. Still no success. TODO: Will update.\nTensorflow binaries go up to CUDA 10.0. I'd built tensorflow-gpu from source multiple times on Ubuntu systems...but I spent 3 days on this thing and still haven't gotten it to build. Some kind of error with grpc at gettid. :-/ OOoo! Got it! Uh, at least, it compiles."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#document-editing",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#document-editing",
    "title": "Mac => Linux Switch Notes",
    "section": "Document Editing",
    "text": "Document Editing\n\nMarkdown: Over the years, I’ve tried a number of Markdown editors – Haroopad, Mou, Ghost, StackEdit. Then today I discovered Typora. It is my favorite by far!\nCode: I like Atom or VSCode for code (same as I did on the Mac).\nOffice: LibreOffice seems, so far, to be able to read & write my Microsoft Office files without mangling them! It’s uglier, but that’s par for the course.\n(Nonfiction) Book authorship:\n\nI’d been working on a book using Scrivener 3. Scrivener released a(n unofficial) Linux port but it’s old. Linux users now typically either run the Windows or Mac versions however they can. See “MacOS on Linux via VirtualBox”, below. (Score! Mac license key successfully-hypothetically accepted in VirtualBox guest OS.)\nAlternatives: Manuskript (cf. novelist Matthew Buscemi’s notes on switching from Scrivener to Manuskript), or Sigil for EPUBs, Scribus for print layout,…? …Well and of course LaTeX! ;-)"
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#peripherals",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#peripherals",
    "title": "Mac => Linux Switch Notes",
    "section": "Peripherals",
    "text": "Peripherals\nOne doesn’t hope for much with Linux, but I’ve succeeded with more than I’d hoped for. As follows…\n\nAudio\n\nBluetooth pairing to a Bose Soundlink works fine: Ooo, and the volume buttons on the keyboard control the Bluetooth speaker volume!\nMy Focusrite 2i2 USB box works right away – sort of. The OS recognizes it but then ignores it and just keeps using the laptop defaults regardless of what you set for the System Settings. Yet Audacity will actually use it correctly. ¯\\_(ツ)_/¯. Officially, Focusrite claims no Linux support, but allows that you might get lucky since they are USB class-compliant. UPDATE: If you manually set the Sound setting to Internal Speakers and then set it to the Focusrite, then the Focusrite gets used for everything. (Seems that the Plug’n’Play is a bit messed up.)\nMy Steinberg UR44 USB audio interface… was not recognized at first. There’s a thread from 2014 on the Steinberg forums for setting up both the UR22 and UR44 on Linux. They say you need to flip the switch on the back of the device to “Class Compliant (CC)” mode. Then turn the Steinberg off & on (i.e. unplug the power cable & plug it in) and…voila! It now shows up! I have studio-quality audio I/O!!\n\n\n\nVideo\n\nExternal Displays (HDMI): One thing that will require getting used to will be the fact that you can only use the HDMI port on the Oryx Pro when the NVIDIA GPU is fully enabled. You can’t use it with the Intel Graphics or Hybrid Graphics. And switching graphics modes requires a reboot. :-/ But after rebooting, it works! Too bad the battery life isn’t so hot with the NVIDIA card, but that’s ok. ;-)\n….Uh, and apparently it refuses to route audio through HDMI even though it should. Hmm. Found this out the hard way when teaching a university class to audio engineering students. :-(\n\n\n\nApple Wireless Keyboard\n\nThe Pop!_OS/Gnome GUI-based Bluetooth pairing method wasn’t working super great, so I started out following these command-line instructions and then switched over to to Pop!_OS GUI once the scan was in progress.\n\nCurrently typing these words on my wireless keyboard, with the laptop in the closet, looking at my external monitor connected via HDMI, listening to external monitor speakers controlled by the Steinberg UR44! :)"
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#customization",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#customization",
    "title": "Mac => Linux Switch Notes",
    "section": "Customization",
    "text": "Customization\n\nGnome:\n\nSystem76 posted a list of handy Gnome extensions.\nGnome Tweaks, e.g. for adding Minimize/Maximize buttons to the windows! The default Gnome desktop…I do not understand what those developers are thinking, other than “Let’s waste screen real estate.”\n\nKeyboard Colors (on Oryx Pro): There are a few utilities out there. I went with this one. Note that there are only three \"zones\" of color definable, not controls for each key. Also had to use sudo to enable user writing to appropriate /sys/ devices.\nScrolling: Unlike the Mac, the trackpad scrolling behavior seems to be app-specific. As I write this, Typora doesn’t offer any inertia – also called “coasting” – whereas a few Gnome apps (except Terminal!) offer near-perfect Mac-like inertia scrolling, whereas Thunderbird, Atom & VSCode offer none. SOLUTION: Install the “Synaptics” (legacy) driver:\nsudo apt install xserver-xorg-input-synaptics\n…then log out & log back in. Now things are much better; it’s a pretty buggy and sometimes the kinetic scrolling won’t kick in, and other times it will. But this is a vast improvement. (Note: I did not uninstall libinput but rather left it in.)\nOh but wait! On reboot the trackpad was dead! The only solution was to uninstall the synaptics driver :-( and now the trackpad works again."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#macos-on-linux-via-virtualbox",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#macos-on-linux-via-virtualbox",
    "title": "Mac => Linux Switch Notes",
    "section": "MacOS on Linux via VirtualBox",
    "text": "MacOS on Linux via VirtualBox\n(Given Apple’s EULA, the following is purely a hypothetical scenario for discussion purposes only...)\nSometimes one may simply need a particular Mac app. This “semi-automated install script” described on the VirtualBox Wikibooks page is…I am told, hypothetically…awesome! It will – allegedly, mind you – automatically grab the MacOS ISO and set everything up! (Note that the first time that hypothetical-me might have tried this, hypothetical-me may have missed a bunch of the prompts from the script because the VirtualBox window had obscured the Terminal. Don’t be like said hypothetical-me – hypothetically you should follow the script, and just press enter at the appropriate moments.) And it allegedly mostly “just works.”\nAlso: Catalina won’t work yet; only Mojave for now.\n\nMain Issues:\n\nGuest Additions don’t get set up. For a long time, VirtualBox didn’t have Guest Additions for Mac guests, and then it did, and now the newest VirtualBox (version 6.1) doesn’t. But the above script installs version 6.0.14, and for versions 6.0.12 to 6.0.14 there were Guest Additions for Mac! Still available for download here. Then following these instructions, one might download this .iso file from within the macOS guest, then mount it (via the Finder), then double-click on the file VBoxDarwinAdditions.pkg to install them. Doing this will enable shared clipboards and…\nShared Folders – not. Once the Guest Additions are up, one may try to create a directory on the Mac client, e.g. mkdir ~/linux as a mount point, and set up a shared folder in Virtual Box that could auto-mount to this location…. and then one can hypothetically waste HOURS AND HOURS and never it get it to work, at which point one may give up and just use Dropbox or some ****. Really I just wanted this for Scrivener, so syncing ~/Dropbox/Documents would be sufficient.\nAlternatively, you could use “File Sharing” to setup a Samba server, but…Not gonna.\nInsanely Slow Execution. Lots of issues with the Mac guest, one of which is that it’ll only utilize 1 CPU!\nLow Display Resolution by default. But fixable by changing the EFI resolution, such as by running on the host system the command\n$ VBoxManage setextradata \"macOS\" VBoxInternal2/EfiGraphicsResolution 1600x900\n\nFor some reason, iMessages won’t hypothetically login, even though other aspects of iCloud & AppleID link. No errors, just…doesn’t seem to work….so I’m told.\n“Obviously”, hardware-stuff like the Facetime camera probably will never work. Don’t care."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#dual-boot---adding-windows",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#dual-boot---adding-windows",
    "title": "Mac => Linux Switch Notes",
    "section": "Dual Boot - adding Windows",
    "text": "Dual Boot - adding Windows\n(Let’s be honest: this is just for playing video games.)\nThe System76 instructions seem to start in the middle of something; pretty confusing. And there’s a second set of System76 instructions with almost the same title. Turns out the former is for adding Pop!_OS to Windows, and the latter is for adding Windows to Pop!_OS. The latter is what I wanted… but… wasn’t able to follow what they did. So….here’s what I did:\n\nYou can’t repartition your hard drive while it’s being used. So create a bootable USB image of either all of Pop!_OS or else just the GParted partitioning tool. The Popsicle utility is good for flashing USBs.\nRestart the machine, hold F7, and select the USB from the boot menu. Then you can run GParted. My version looked like the following, where /dev/sda3 is the main partition that started as all Pop!_OS:\n\nAll you need to do is resize the big partition to a smaller size. I made mine such that it would free up 120 GB. You don’t need to create a new partition in the newly-unallocated space or format it; Windows will do that.\nGet a legit bootable Windows 10 ISO somehow, e.g. by downloading from Microsoft or (mumble mumble). Mine was super-old which made what follows extra-fun. (I used a second USB stick by the way). At least it still had a valid Activation Key. :-)\nRestart with with Windows ISO USB inserted, hold F7, boot to the Windows installer. In the installer, when choosing a partition, choose the empty space that was leftover after you shrunk (shrank?) your main Linux partition. Install Windows there.\nWindows installs itself at the front of the machine’s rudimentary boot-loading menu (the F7 thing), meaning that, congratulations, your machine now boots to Windows by default, and I haven’t figured out how to change that. Good news is, the Windows install process will require restarting a bunch of times, so at least that will happen automatically (without you having to choose not-Linux over & over).\nWhen Windows finally came up, it couldn’t detect the WiFI device on the laptop! That was a bit scary, and made it rather challenging to download any sort of updates. Thankfully the Ethernet jack worked, and so I was able to download enough updates so that eventually (after many incremental update-installs and reboot cycles) the WiFi device started working.\nManaged to install everything I wanted, included Steam and Uplay. Got “Rise of the Tomb Raider” for $12 and played it on max settings at 80 to 100 FPS with no trouble. :-) (Well, note that the machine gets super-hot and the fan runs like crazy, but what do you expect?)\nNow, every time I want to boot to Linux, I have to remember to hold F7. …Oh well."
  },
  {
    "objectID": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#trouble",
    "href": "posts/2020-01-13-Mac-Linux-Switch-Notes.html#trouble",
    "title": "Mac => Linux Switch Notes",
    "section": "Trouble!",
    "text": "Trouble!\nWhen I came back to Linux from Windows, the system started hanging intermittently. I though maybe it was limited to the trackpad but it seems there are many reports of freezing with Ubuntu 19.10 (which Pop!_OS inherits from) for certain video drivers, and in fact I was in “full NVIDIA mode” when I came back to Linux from Windows. Tonight I’ve been typing the rest of this in Hybrid Graphics mode with not a single hang. So…maybe that’s all I can hope for for now.\nI’ll get by.\n….How about we stop there for now. I’ll post another edit of this once PyTorch & Tensorflow are working to my satisfaction.\nP.S.- Oh hey. I was going to end that last line with a fingers-crossed emoji, only to find out that Ubuntu and most Linux distributions don’t supoort emoji by default, but there is a Gnome Characters package, which after installing, will let you do this: 🤞😃👍\n\n-SH, 1/13/2020."
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "",
    "text": "Links to lessons: Part 0, Part 1, Part 2, Part 3"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#what-is-backpropagation",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#what-is-backpropagation",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "What is Backpropagation?",
    "text": "What is Backpropagation?\nFirst watch this 5-minute video on backprop by Siraj Raval.\nEDIT 2/20/2020: ^Ravel was later revealed to be plagiarizing content. I will look for an alternative link. The format in my (“flipped”) ML course last year involved reading things, watching brief videos, and modifying code. Siraj’s video fit the bill last year as being brief & good."
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#a-multi-layer-network",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#a-multi-layer-network",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "A Multi-Layer Network",
    "text": "A Multi-Layer Network\nBetween the input \\(X\\) and output \\(\\tilde{Y}\\) of the network we encountered earlier, we now interpose a “hidden layer,” connected by two sets of weights \\(w^{(0)}\\) and \\(w^{(1)}\\) as shown in the figure below. This image is a bit more complicated than diagrams one might typically encounter; I wanted to be able to show and label all the different “parts.” We will explain what the various symbols mean as we continue after the figure. \nAs before, the combined operation of weighted sum and nonlinear activation is referred to as a(n artificial) neuron; in the diagram above there are 4 “hidden neurons” — or equivalently “4 neurons in the hidden layer” — as well as one neuron for output. (The activations \\(f^{(0)}\\) and \\(f^{(1)}\\) may be the same, or they may be different.)"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#semantics-what-is-a-layer",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#semantics-what-is-a-layer",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Semantics: What is a Layer?",
    "text": "Semantics: What is a Layer?\nThe term “layer” in regards to neural network is not always used consistently. You may find it used in different senses by different authors.\n\nSome users of the term will only use it with repect to weight matrices, (since these are the parts of the network which are adjusted in learning).\nOthers will refer to the input and (predicted) output as layers, and may or may not include the weights as layers..\nOthers will only count additional “hidden layers” between the inputs and outputs, and these “layers” are connected by multiple weight matrices.\n\nSome will speak of “activation layers.” In software libraries like Keras, many different types of operations and storage are referred to as layers.\n\nFor the work we’ve done so far, we’ve had inputs and outputs connected by one weight matrix, subject to a nonlinear activation function. Is this a two-layer network made of input and output “layers,”” or is it a single-layer network, because there is only one weight matrix? What about the activation layer? This is to some degree a semantic issue which one does not need to get hung up on.\nFor our purposes it is convenient to refer to the inputs \\(X\\), the ‘activated’ hidden states \\(H\\), and the output \\(\\tilde{Y}\\) as “layers”, numbering them 0, 1, and 2 respectively, and using the script notation \\(\\mathcal{L}^l\\) to denote each layer, where the layer index \\(l=0..2\\), so that\n\\[\n\\mathcal{L}^{(0)} = X, \\ \\ \\ \\ \\mathcal{L}^{(1)} = H,\\ \\ \\ \\ \\mathcal{L}^{(2)} = \\tilde{Y}\n\\]\nThis makes it easy to write the value of higher-numbered layers in terms of lower-numbed layers, i.e., \\[\n\\mathcal{L}^{(l+1)} = f^{(l)}\\left( {\\mathcal{L}^{(l)}}^T \\cdot w^{(l)} \\right),\n\\] where the dot \\(\\cdot\\) denotes a matrix product. This is often referred to as a “feed foward” operation because values are fed from left to right in the above diagram, “forward” through the network. (Backpropagation will involve feeding values from right to left.)\nResponse to student question(s):  “What are neurons? Like, what does this mean in terms of matrices?”\nThis will serve as a review of the Part 1 lesson. Using the above notation, the operations from the input to the hidden layer look like this in matrix form:\n …where the lines in dark red and cyan are simply to indicate sample calculations which are part of the matrix multiplication."
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#figuring-out-dimensions-of-the-weights",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#figuring-out-dimensions-of-the-weights",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Figuring out dimensions of the weights",
    "text": "Figuring out dimensions of the weights\nWhen we learned about matrix multipliation, we remarked that most of the time in machine learning, “the trick is to get the inner dimensions to match.”\nLet’s say there are \\(N\\) different input data “points” consisting of \\(M\\) values each. So the input \\(X\\) is an \\(N\\times M\\) matrix. And let the output \\(\\tilde{Y}\\) be a \\(NxP\\) matrix (in our example, \\(P=1\\)). If we were just connecting \\(X\\) and \\(\\tilde{Y}\\) with no hidden layer, the single weights matrix would be a \\(M\\times P\\) matrix:\n\\[\n(\\color{blue}N\\times \\color{red}M)\\cdot(\\color{red}M\\times \\color{green}P) = (\\color{blue}N\\times \\color{green}P)\n\\] (The nonlinear activation doesn’t change the dimensions of the matrices.)\nAdding a hidden layer with \\(Q\\) number of neurons means we will still have \\(N\\) different activations for each neuron (i.e. for each datapoint), so that \\(H\\) is a \\(N\\times Q\\) matrix. Thus the dimensions of \\(w^0\\) must “match” between these two matrices, and so \\(w^0\\) must be a \\(M\\times Q\\) matrix:\n\\[\n(\\color{blue}N\\times \\color{red}M)\\cdot(\\color{red}M\\times \\color{purple}Q) = (\\color{blue}N\\times \\color{purple}Q)\n\\] Similarly \\(w^1\\) must be a \\(Q\\times P\\) matrix, and the full operation in terms of matrix dimensions is\n\\[\n(\\color{blue}N\\times \\color{red}M)\\cdot(\\color{red}M\\times \\color{purple}Q)\\cdot(\\color{purple}Q\\times \\color{green}P) = (\\color{blue}N\\times \\color{green}P).\n\\] Compare this to the diagram above for \\(P=1\\), \\(Q=4\\).\nNote: If you add bias terms to your model, you may need to remember that the number of columns in both the input \\(X\\) and hidden layer \\(H\\) are greater by one, i.e. \\(\\color{red}{M}\\rightarrow \\color{red}{M+1}\\), etc.\n##..a bit of code\nThe layers \\(\\mathcal{L}^l\\) can be represented in Python a list called layers which has a of length 3. Similarly, our weights can be items in a list called weights. Returning to our first sample problem from Part 1:\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Training data: input and target \nX = np.array([  [0,0,1],\n                [0,1,1],\n                [1,0,1],\n                [1,1,1] ])\nY = np.array([[0,0,1,1]]).T\n\n# define auxiliary variables\nN, M, P  = X.shape[0], X.shape[1], Y.shape[1]    # infer matrix shape variables from training data\nY_tilde = np.zeros((N,P))                        # setup storage for network output \n\n# Hidden layers\nQ = 4                     # number of hidden neurons, i.e. \"size of hidden layer\"\nH = np.zeros((N,Q))\n\n# weight matrices\nw0 = 2*np.random.random((M,Q)) - 1\nw1 = 2*np.random.random((Q,P)) - 1\n\n# Make lists for layers and weights\nlayers = [X, H, Y_tilde]\nweights = [w0, w1]\n\n# Just try a sample calculation with random intialization to see how this works \n# Feed-forward (with linear activation):\nfor l in range(2):\n  layers[l+1] = np.dot(layers[l], weights[l])\n  print(\"layers [\",l+1,\"] =\\n\", layers[l+1], sep=\"\")  # sep=\"\" just omits spaces\n\nlayers [1] =\n[[-0.58431094  0.24256333  0.85640655  0.25641678]\n [-0.45815989  0.80598459  1.63132867 -0.10938617]\n [-0.38151492  0.81129189  0.8919347   0.03921142]\n [-0.25536386  1.37471315  1.66685682 -0.32659153]]\nlayers [2] =\n[[-0.97388449]\n [-2.37430685]\n [-1.56570389]\n [-2.96612624]]\n\n\nGeneralizing this so it will do the full feed-forward will take a bit more code. We’ll leave a placeholder routine for backpropagation for now.\n\n# Activation choices\ndef sigmoid(x, deriv=False): \n  f = 1/(1+np.exp(-x))\n  return f*(1-f) if deriv else f\n\ndef relu(x, deriv=False):   \n  return 1*(x&gt;0) if deriv else x*(x&gt;0)\n\n# Placeholder routine to perform backprop.  Will fill in later\ndef update_weights(weights, layers, Y, alpha, activ):\n  return weights                   # for now, it's a no-op\n\n\ndef fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000):\n  \"\"\"\n  Routine for training using a multi-layer network\n    layers:    list of layer values, i.e. layers =  [X, H, Y_tilde]\n    Y:         target output\n    activ:     list of activation functions. default = list of 2 sigmoids\n    use_bias:  Whether to include a constant offset in weighted sums\n    alpha:     learning rate\n    maxiter:   number of iterations to run\n  \"\"\"\n  lmax = len(layers)-1             # max index of layers, also = # of weights\n  \n  if use_bias:           # add a column of 1's to every layer except the last\n    for el in range(lmax):\n        new_col = np.ones((layers[el].shape[0],1)) \n        layers[el] = np.hstack((new_col, layers[el])) \n  \n  # Define weights\n  np.random.seed(1)                # for reproducibility\n  weights = [None]*lmax            # allocate slots in a blank list\n  for el in range(lmax):           # \"el\" because \"l\" and \"1\" may look similar\n    weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1])) - 1\n        \n  loss_hist = []                   # start with an empty list\n  for iter in range(maxiter):\n\n    # Feed-forward pass\n    for el in range(lmax):\n      layers[el+1] = activ[el](np.dot(layers[el], weights[el]))\n      \n    # Loss monitoring\n    diff = layers[lmax] - Y\n    loss_hist.append( (diff**2).mean()  )    # use MSE loss for monitoring\n          \n    # Backprop code will go here\n    weights = update_weights(weights, layers, Y, alpha, activ)\n\n  return weights, layers[lmax], loss_hist        \n\n\n# Test this just to make sure it runs\nlayers = [X,H,Y_tilde]\nweights, Y_tilde, loss_hist = fit(layers, Y, maxiter=1)\nfor el in range(len(weights)):\n  print(\"weights[\",el,\"] = \\n\",weights[el], sep=\"\")\n\nweights[0] = \n[[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822]\n [-0.81532281 -0.62747958 -0.30887855 -0.20646505  0.07763347]\n [-0.16161097  0.370439   -0.5910955   0.75623487 -0.94522481]\n [ 0.34093502 -0.1653904   0.11737966 -0.71922612 -0.60379702]]\nweights[1] = \n[[ 0.60148914]\n [ 0.93652315]\n [-0.37315164]\n [ 0.38464523]\n [ 0.7527783 ]]\n\n\n…Now that we’ve achieved feed-foward operation of the network, in order to make it ‘learn’ or ‘train’, we need to compare the output value \\(\\tilde{Y}\\) (which is the same as layers[2] by the way) to the target value, compute the gradients of the loss function, and then backpropagate in order to update all the weights!"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#backpropagating-theory",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#backpropagating-theory",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Backpropagating: Theory",
    "text": "Backpropagating: Theory\nTL/DR: You can skip down to the last boxed equation of this section if math scares you. You will not be required to reproduce this. I do want to show you “where this stuff comes from”, but if you find the derivation too intimidating, you can still progress in the course just fine.\nLet’s review how we got the gradients for \\(w^{(1)}\\) in Part 2, denoting weighted sums by “\\(S\\)”, e.g. \\(S^l = \\mathcal{L}^l\\cdot w^l\\), we just used the Chain Rule: \\[\n{\\partial L\\over\\partial w^{(1)}} =\n\\color{blue}\n{\\partial L \\over\\partial \\mathcal{L}^{(2)}}\n\\color{green}\n{\\partial \\mathcal{L}^{(2)} \\over\\partial S^{(1)}}\n\\color{red}\n{\\partial S^{(1)} \\over\\partial w^{(1)}}\n\\] We’ll define the first partial derivative to be \\(\\delta^{(2)}\\), which works out (given our choice of \\(L\\) from Part 2) to be \\[\\color{blue}{\n{\\partial L \\over\\partial \\mathcal{L}^{(2)}}\n=\\delta^{(2)} = \\tilde{Y}-Y},\\] i.e., it is the error in the final ouput. The next partial derivative (in green) is just the derivative of the activation function \\(f\\), and the last partial derivative is just \\(\\color{red}{\\mathcal{L}^{(1)}}\\), so as we saw in the previous lesson, we can write this ‘schematically’ (i.e. not quite as a properly-set-up matix equation yet) as \\[\n{\\partial L\\over\\partial w^{(1)}} =\n\\color{blue}{\\delta^{(2)}}\n\\color{green} {f^{(1)\\prime} }\n\\color{red}{\\mathcal{L}^{(1)}}\n\\] whereas in proper form it will take on this ordering as a matrix equation: \\[\n\\boxed{\n{\\partial L\\over\\partial w^{(1)}} =\n{\\mathcal{L}^{(1)}}^T \\cdot\n{\\delta^{(2)}}\n{f^{(1)\\prime}}\n}.\n\\]\nTo get the gradients for \\(w^{(0)}\\), we can make use of a similar “\\(\\delta\\)” notation if we’re careful in how we define a new \\(\\delta^{(1)}\\). Let’s write out the chain rule, and put parentheses around a particular group of terms for later:\n\\[\n{\\partial L\\over\\partial w^{(0)}} =\n\\color{blue}{\n\\left(\n{\\partial L \\over\\partial \\mathcal{L}^{(2)}}\n{\\partial \\mathcal{L}^{(2)} \\over\\partial S^{(1)}}\n{\\partial S^{(1)} \\over\\partial \\mathcal{L}^{(1)}}\n\\right)}\n\\color{green}\n{\\partial \\mathcal{L}^{(1)} \\over\\partial S^{(0)}}\n\\color{red}\n{\\partial S^{(0)} \\over\\partial w^{(0)}}\n\\] In a manner similar to what we did above, this can be written as \\[\n{\\partial L\\over\\partial w^{(0)}} =\n\\color{blue}{\n\\left(\\delta^{(2)}f^{(1)\\prime}w^{(1)}\\right)}\n\\color{green}{f^{(0)\\prime}}\n\\color{red}{\\mathcal{L^{(0)}}}\n\\] We now define the terms in parentheses as \\(\\delta^{(1)}\\)\n\\[\n\\color{blue}{\n\\delta^{(1)} =  \\delta^{(2)}f^{(1)\\prime}w^{(1)}\n},\n\\] …which is kind of like “the error in the hidden layer,” or like the final solution error projected backward into the hidden layers via our (momentarily fixed) weights \\(w^{(1)}\\).\nThen our gradients for \\(w^{(0)}\\) take on a similar form as the gradients for \\(w^{(1)}\\). ‘Schematically’ this looks like \\[\n{\\partial L\\over\\partial w^{(0)}} =\n\\color{blue}{\\delta^{(1)}}\n\\color{green}{f^{(0)\\prime}}\n\\color{red}{\\mathcal{L^{(0)}}}\n\\] and in proper matrix form this is \\[\n\\boxed{\n{\\partial L\\over\\partial w^{(0)}} =\n{\\mathcal{L}^{(0)}}^T \\cdot\n{\\delta^{(1)}}\n{f^{(0)\\prime}}\n},\n\\] i.e., the same form as the preceding layer, just “back” one layer. We are backpropagating the errors \\(\\delta^{(l)}\\) from one layer to another in order to update the weights.\nThe weights are then updated as before, except now we will write this ‘generically’ for all weights and layers using the index \\(l\\): \\[\n\\boxed{\nw^{(l)} := w^{(l)} - \\alpha {\\mathcal{L}^{(l)}}^T \\cdot\n{\\delta^{(l+1)}}\n{f^{(l)\\prime}}\n},\n\\] where \\[\n\\delta^{(l+1)} = \\left\\{ \\begin{array}{l}\n\\tilde{Y}-Y,\\ \\ \\  &\\ &l+1=l_{max} \\ \\ \\ \\ ({\\rm e.g.} \\ l_{max}=2)\\\\\n\\delta^{(l+2)}f^{(l+1)\\prime}\\cdot {w^{(l+1)}}^T, &\\ &l+1 &lt; l_{max}\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#writing-the-backprop-code",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#writing-the-backprop-code",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Writing the Backprop Code",
    "text": "Writing the Backprop Code\nNow we’ll use the above analysis to replace the update_weights() function from earlier.\n\ndef update_weights(weights, layers, Y, alpha, activ):\n  \"\"\"\n  Backprop routine, for arbitrary numbers of layers, assuming weights & \n  activations are defined\n  \n  Inputs:\n    weights: list of arrays of weights between each layer\n    layers:  list of arrays of layer values (post-activation function)\n    Y:       target output\n    alpha:   learning rate\n    activ:   list of activation functions for each (non-input) layer\n  Outputs:\n    weights (updated)\n  \"\"\"\n  lmax = len(layers) - 1                        # a useful variable\n  assert len(weights)==lmax                     # make sure number of weights match up\n  assert len(activ) &gt;= lmax                     # make sure we defined enough activations for the layers\n    \n  delta = layers[lmax] - Y                      # error between output and target\n  \n  for el in range(lmax-1, -1, -1):              # Count backwards to layer zero\n    fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True)   # deriv of activation\n    weights[el] -= alpha * np.dot( layers[el].T, delta*fprime )       # gradient descent step\n    delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None   # setup delta for next pass in loop\n\n  return weights           \n\n# Let's run it!\nlayers = [X, H, Y_tilde]\nalpha = 0.1\nmaxiter=5000\nweights, Y_tilde, loss_hist_2weights = fit(layers, Y, alpha=alpha, maxiter=maxiter)\n\n# compare against a 1-weight (no hidden layer) network:\nlayers = [X, Y_tilde]\nweights, Y_tilde, loss_hist_1weight = fit(layers, Y, alpha=alpha, maxiter=maxiter)\n\n# Plot the loss history\nplt.semilogy(loss_hist_1weight, label=\"No hidden layers\")\nplt.semilogy(loss_hist_2weights, label=\"Hidden layer\")\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n\n\n\n# Let's add more hidden neurons\nQ = 50                     # number of hidden neurons, i.e. \"size of hidden layer\"\nH = np.zeros((N,Q))\n\nlayers = [X, H, Y_tilde]\nweights, Y_tilde, loss_hist_many = fit(layers, Y, alpha=alpha, maxiter=maxiter)\n\n# try a relu activation for the hidden layer (leave output activ as sigmoid!)\nweights, Y_tilde, lhm_relu = fit(layers, Y, alpha=alpha, activ=[relu,sigmoid], maxiter=maxiter)\n\nplt.semilogy(loss_hist_1weight, label=\"0 hidden neurons\")\nplt.semilogy(loss_hist_2weights, label=\"4 hidden neurons\")\nplt.semilogy(loss_hist_many, label=\"many hidden neurons\")\nplt.semilogy(lhm_relu, label=\"many, relu on hidden\")\n\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#solving-xor",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#solving-xor",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Solving XOR",
    "text": "Solving XOR\nNow let’s revisit the “XOR” problem that a single neuron couldn’t handle.\n\\[ \\overbrace{\n\\left[ {\\begin{array}{cc}\n    0 & 0 \\\\\n    0 & 1 \\\\\n    1 & 0 \\\\\n    1 & 1 \\\\\n  \\end{array} } \\right]\n}^{X} \\rightarrow\n\\overbrace{\n\\left[ {\\begin{array}{c}\n   0   \\\\\n   1  \\\\\n   1  \\\\\n   0 \\\\\n  \\end{array} } \\right]\n  }^Y.\n\\] With our multi-layer network, we can solve this. Note that while an exact solution to the XOR problem exists using only 2 hidden neurons and linear activations, a program can still have a hard time finding a good approximation via gradient descent, and we use 20 hidden neurons to assist, as follows:\n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY = np.array([[0,1,1,0]]).T\nY_tilde = 0*Y                         # Just allocate some storage\nH = np.zeros((N,20))                  \n\nweights, Y_tilde, loss_hist_xor = fit([X,H,Y_tilde], Y, activ=[relu,sigmoid], alpha=0.5)\nprint(\"Prediction Y_tilde =\",Y_tilde.T)\nprint(\"Target Y (correct answer)  =\",Y.T)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.loglog(loss_hist_xor)\n#print(\"weights = \",weights)\n\nPrediction Y_tilde = [[0.00480128 0.9962425  0.9963109  0.00374649]]\nTarget Y (correct answer)  = [[0 1 1 0]]"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#keras-version",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#keras-version",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Keras version",
    "text": "Keras version\nKeras is so simple to set up, it’s easy to get started. This is what the previous example for XOR looks like “in Keras”:\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam\n\n# training data\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY = np.array([[0,1,1,0]]).T\n\n# specify model\nn_hidden = 20\nmodel = Sequential([\n    Dense(n_hidden, input_shape=(X.shape[1],), activation='relu'),\n    Dense(1, activation='sigmoid')])\n\n# choices for loss and optimization method\nopt = Adam(lr=alpha)   # We'll talk about optimizer choices later\nmodel.compile(optimizer=opt, loss='binary_crossentropy',metrics=['binary_accuracy'])\n\n# training iterations\nmodel.fit(X, Y, epochs=maxiter, batch_size=1, verbose=0)\n\nprint(\"\\nY_tilde = \\n\", model.predict(X) )\n\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\nWARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n\n\n1/1 [==============================] - 0s 28ms/step\n\nY_tilde = \n [[2.1021294e-03]\n [9.9999869e-01]\n [9.9999863e-01]\n [2.0069590e-06]]\n\n\nKeras can get a better appoximation than we did because of the choice of optimizer algorithm. We’ll talk about optimization algorithms (refinements to gradient descent) another time."
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#pytorch-version",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#pytorch-version",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "PyTorch version",
    "text": "PyTorch version\nUnlike Keras, PyTorch does not have any “training wheels.” You have to specify a number of the operations yourself. It’s helpful to have a template to start from, such as the following example for our XOR problem.\n\nimport torch                  # it's 'PyTorch' but the package is 'torch'\ndevice = torch.device('cpu')  # handy for changing to 'cuda' in GPU runtimes later!\n\ntorch.manual_seed(1)  # for reproducibility\n\n# training data\nX = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\nY = np.array([[0,1,1,0]],dtype=np.float32).T\n\n# re-cast data as PyTorch variables, on the device (CPU or GPU) were calc's are performed\nx, y = torch.tensor(X).to(device), torch.tensor(Y).to(device)   \n\n# specify model (similar to Keras but not quite)\nn_hidden = 20                           # number of hidden neurons\nmodel = torch.nn.Sequential(\n          torch.nn.Linear(X.shape[1], n_hidden),\n          torch.nn.ReLU(),\n          torch.nn.Linear(n_hidden, 1),\n          torch.nn.Sigmoid()\n        ).to(device)\n\n# choices for loss and optimization method\nloss_fn = torch.nn.BCELoss()      # binary cross-entropy loss\noptimizer = torch.optim.Adam([{'params': model.parameters()}], lr=alpha)\n\n# training iterations\nloss_hist_pytorch = []\nfor iter in range(maxiter):\n  optimizer.zero_grad()                  # set gradients=0 before calculating more\n  y_tilde = model(x)                     # feed-forward step\n  loss = loss_fn(y_tilde, y)             # compute the loss\n  loss_hist_pytorch.append(loss.item())  # save loss for plotting later\n  loss.backward()                        # compute gradients via backprop\n  optimizer.step()                       # actually update the weights\n\n# print and plot our results\nprint(\"\\nY_tilde = \\n\", y_tilde.cpu().data.numpy() )\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.loglog(loss_hist_pytorch)\n\n\nY_tilde = \n [[2.9900986e-07]\n [9.9999976e-01]\n [9.9999964e-01]\n [3.4445571e-07]]"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#more-with-the-7-segment-display",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#more-with-the-7-segment-display",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "More with the 7-segment display",
    "text": "More with the 7-segment display\nUsing the \\(X\\) and \\(Y\\) arrays for from previous exercices with the 7-segment display, we’ll explore the effects of adding hidden neurons and different activation functions. Using the code template that follows below,…\n\nA. Explore hidden layer sizes & activations\n\nWrite code for a new activation function: \\(\\tanh(x)\\) and its derivative. Note: there is already placeholder code for this in the template below\nSet training data to be that of the 7-segment display.\nChoose (for yourself) a single learning rate (e.g. $=$0.5), and a standard number of iterations (e.g. 10000).\n\nThen compare results for multiple networks (all with softmaxsigmoid activation on the end): 1. A single hidden layer with 20 neurons and (for the hidden layer)… - sigmoid activation - relu activation - tanh activation\n\nA single hidden layer with 100 neurons and (for the hidden layer)…\n\n\nsigmoid activation\nrelu activation\ntanh activation\n\n\n\nB. Explore multiple hidden layers\n\nNow use two hidden layers, H and H2 with 10 neurons each, and experiment to find the best combination of activations, and best choice of learning rate that gives you the lowest loss at the end of your chosen number of iterations. Check that your predicted output is as you expect.\n\n\n\nAssignment:\nUpload a text file of the code for your “winning” entry for #3 to Blackboard. Use the code below as a template.\n\n## TEMPLATE CODE. Scroll down to \"MAKE YOUR CHANGES BELOW\", below\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n### LEAVE THIS UNCHANGED\n# First, let's repeat the sigmoid(), relu(), update_weights() and fit() routines\n# already defined, so we have a'standalone' code and can easily make changes\n\n# Activation choices\ndef sigmoid(x, deriv=False): \n  f = 1/(1+np.exp(-x))\n  return f*(1-f) if deriv else f\n\ndef relu(x, deriv=False):   \n  return 1*(x&gt;0) if deriv else x*(x&gt;0)\n\n# Backpropagation routine\ndef update_weights(weights, layers, Y, alpha, activ):\n  lmax = len(layers) - 1                        # a useful variable\n  assert len(weights)==lmax                     # make sure number of weights match up\n  assert len(activ) &gt;= lmax                     # make sure we defined enough activations for the layers\n    \n  delta = layers[lmax] - Y                      # error between output and target\n  \n  for el in range(lmax-1, -1, -1):              # Count backwards to layer zero\n    fprime = activ[el](np.dot(layers[el], weights[el]), deriv=True)   # deriv of activation\n    weights[el] -= alpha * np.dot( layers[el].T, delta*fprime )       # gradient descent step\n    delta = np.dot(delta*fprime, weights[el].T) if (el&gt;0) else None   # setup delta for next pass in loop\n\n  return weights           \n\n# Routine for training via gradient descent\ndef fit(layers, Y, activ=[sigmoid]*2, use_bias=True, alpha=1.0, maxiter=10000):\n  lmax = len(layers)-1             # max index of layers, also = # of weights\n  \n  if use_bias:           # add a column of 1's to every layer except the last\n    for el in range(lmax):\n        new_col = np.ones((layers[el].shape[0],1)) \n        layers[el] = np.hstack((new_col, layers[el])) \n  \n  # Define weights\n  np.random.seed(1)                # for reproducibility\n  weights = [None]*lmax            # allocate slots in a blank list\n  for el in range(lmax):           # \"el\" because \"l\" and \"1\" may look similar\n    weights[el] = 2*np.random.random((layers[el].shape[1], layers[el+1].shape[1]))-1 \n            \n  loss_hist = []                   # start with an empty list\n  for iter in range(maxiter):\n\n    # Feed-forward pass\n    for el in range(lmax):\n      layers[el+1] = activ[el](np.dot(layers[el], weights[el]))\n    Y_tilde = layers[lmax]\n      \n    # Loss monitoring\n    diff = Y_tilde - Y\n    loss_hist.append( (diff**2).mean()  )    # use MSE loss for monitoring\n          \n    # Backprop code will go here\n    weights = update_weights(weights, layers, Y, alpha, activ)\n\n  return weights, Y_tilde, loss_hist \n\n\n##### END OF PART TO LEAVE UNCHANGED\n\n\n#####---------------  MAKE YOUR CHANGES BELOW ------------##############\n\n# define the tanh activation function\ndef tanh(x, deriv=False):\n  if deriv:\n    pass # *** Students: replace 'pass' with what the derivative should be\n  return np.tanh(x)\n\n\n## Students: replace X, Y with 7-segment data instead\nX = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\nY = np.array([[0,1,1,0]],dtype=np.float32).T\n\n\nY_tilde = np.copy(Y)                     # Just allocates some storage for Y_tilde\n\n##  Hidden layers: Students: Change Q, the number of hidden neurons, as needed\nQ = 10\nN = X.shape[0]                           # this just grabs the number of rows in X\nH = np.zeros((N,Q))                  \nH2 = np.zeros((N,Q))                     # extra hidden layer, might not be used\n\n## Students: change this as instructed\nlayers = [X, H, Y_tilde]              # later, add another layer H2 when instructed\nactiv = [sigmoid, sigmoid, sigmoid]   # change the first (2) activation(s) as instructed\nalpha = 0.5                           # play around with this\n\n\n## LEAVE THIS PART UNCHANGED\nweights, Y_tilde, loss_hist = fit(layers, Y, activ=activ, alpha=alpha)\nnp.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)}) # 2 sig figs\nprint(\"Prediction Y_tilde =\\n\",Y_tilde.T)\nprint(\"Target Y (correct answer)  =\\n\",Y.T)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.loglog(loss_hist)\n\nPrediction Y_tilde =\n [[0.01 0.99 0.99 0.01]]\nTarget Y (correct answer)  =\n [[0.00 1.00 1.00 0.00]]"
  },
  {
    "objectID": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#preview-of-next-lesson-mnist",
    "href": "posts/2019-02-08-my-1st-nn-part-3-multi-layer-and-backprop.html#preview-of-next-lesson-mnist",
    "title": "My First NN Part 3. Multi-Layer Networks and Backpropagation",
    "section": "Preview of next lesson: MNIST",
    "text": "Preview of next lesson: MNIST\nNow that you’ve built up some experience with reading digits, let’s move to handwritten digits! This is a problem usually solved with an architecture called a Convolutional Neural Network, but our ordinary feed-forward network can do it too.\nThe MNIST database of handwritten digits is a classic dataset that every ML student works on. It consists of a large number images of handwritten digits only 28x28 pixels in size. We will “flatten” these into a row of 784 columns, and output a \\(\\tilde{Y}\\) of one-hot-encoded vectore just like we did for the output of the 7-segment display (same digits, 0 to 9!)."
  },
  {
    "objectID": "posts/2017-02-03-How-To-Port-Forward-Jupyter-Notebooks.html",
    "href": "posts/2017-02-03-How-To-Port-Forward-Jupyter-Notebooks.html",
    "title": "How to Port-Forward Jupyter Notebooks",
    "section": "",
    "text": "Jupyter notebooks and their related iTorch notebooks seem to be popular for tutorials, e.g the Deep Learning with Torch tutorial I started today, but I hated them and avoided them – not just for the reasons on this list, but mainly because I keep my “machine learning machine” inside my university’s firewall and this has made it a pain to run “notebooks” if you’re off-site.\nWhen I work from home, I do so via a couple ssh hops, and then copy and paste my script files into the terminal window. But these “notebook” things require a web GUI, and X11 forwarding over mutliple ssh sessions is prohibitively slow.\nI didn’t want to configure a NAT system, and was considering some kind of homegrown CGI-script system (which has “security breach” written all over it), but thankfully I stumbled upon this post on Coderwall, where the process was spelled out. For my set of systems, I needed an additional layer of port-forwarding. So for me it goes like this…\n(incidentally, I find ‘localhost’ sometimes doesn’t resolve, which is why I use 127.0.0.1 explicitly)\nWohoo!"
  },
  {
    "objectID": "posts/2017-02-03-How-To-Port-Forward-Jupyter-Notebooks.html#extra-remotely-editing-files-via-rmate-sublime-text",
    "href": "posts/2017-02-03-How-To-Port-Forward-Jupyter-Notebooks.html#extra-remotely-editing-files-via-rmate-sublime-text",
    "title": "How to Port-Forward Jupyter Notebooks",
    "section": "Extra: Remotely Editing Files via rmate & Sublime Text",
    "text": "Extra: Remotely Editing Files via rmate & Sublime Text\nOn laptop, in Sublime Text 3: Tools &gt; Command Pallete &gt; Install package &gt; rsub\nOn internal (server): sudo apt-get install ruby; sudo gem install rmate\nMake two reverse-SHH tunnel hops from laptop to doorkeeper to internal:\n\nssh -R 52698:127.0.0.1:52698 doorkeeper\n(on doorkeeper) ssh -R 52698:127.0.0.1:52698 internal\n(then on internal) rmate [whatever file you want to edit]\n\n…and suddenly, your file appears in your Sublime Text window on your laptop!\nTo automate this, either alias ssh to be “ssh -R 52698:127.0.0.1:52698” or modify your ~/.ssh/config file(s)"
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "",
    "text": "(This blog post implements the concepts illustrated in my previous post, “Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D”. Note that the “3D” part was because of its utility as a teaching tool for gaining intuition; realistic applications usually involve hundreds of “dimensions.”)"
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#intro",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#intro",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "Intro",
    "text": "Intro\nIn Part 1, I described how contrastive loss-based “metric learning” methods are similar to and different from traditional “one-hot” neural network classifiers in a few ways: similar in that both types of methods can be viewed as embedding methods (not just metric learning), different in that the latter involve a fixed set of classes, whereas the former rely on a similarity measure.\n\nConfession: I didn’t fully disclose my motivation last time: I simply got sick of not knowing what “contrastive losses” meant in many important paper titles – notably OpenAI’s “CLIP” and Eduardo Fonseca et al’s “Unsupervised Contrastive Learning of Sound Event Representations” – and decided “I need to learn this.” And (as one often finds in the rapidly-advancing and fragmented world of ML) it turned out I already knew what contrastive losses and metric learning were (e.g., I took a 4-course NLP Specialization last year!), just not by those names. So hey, I might actually be qualified to teach on this after all! ;-) Ok, enough with the autobiography.\n\nHere’s an annotated outline of the sections of this post:\n\nTraditional One-Hot Classification Demo, using 3 and 4 classes. We’ll be following Jeremy Howard and Sylvain Guggers’s “bears” example from Chapter 2 of the fastai book. But we’ll focus on concepts and outcomes, saving FastAI details for Section 4\nContrastive Loss Demo using the same data as as Section 1.\nDiscussion\nFastAI Details, including “how I learned to write my own Callback” for progress-tracking and data visualization. Note that these coding details will be present earlier in the post and viewable via the “Display Code” buttons, but the explanations of the code won’t appear until Section 4."
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#traditional-one-hot-classification-demo-three-and-4-types-of-bears",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#traditional-one-hot-classification-demo-three-and-4-types-of-bears",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "1. Traditional One-Hot Classification Demo: Three (and 4) Types of Bears",
    "text": "1. Traditional One-Hot Classification Demo: Three (and 4) Types of Bears\nWe could do the “cats vs. dogs vs. horses” example from the previous Part 1 post, but it turns out that because we’re going to do Transfer Learning using a model pre-trained on ImageNet (which already has categories for cats, dogs, and horses), it’s too easy to even see anything “interesting” happen! I want to be able to show you “dots moving around” as the model trains.\nInstead, we’ll copy Jeremy Howard…\n\nAside: “We’ll copy Jeremy Howard” is something more members the ML community could probably stand to do, or admit they’re doing.\n\n…and do three types of bears: grizzly, brown, and teddy. ;-) And then later we’ll throw in one more “natural kind” of bear. ;-)\n\nScraping for Images\nUsing the DuckDuckGo API, let’s grab ~100 images of each kind of bear.\n\n\nCode\ndef scrape_for_me(dl_path, labels, search_suffix, erase_dir=True, max_n=100):\n    if erase_dir:\n        !rm -rf {dl_path}           # clear out last time's downloads\n    path = Path(dl_path)\n    if not path.exists(): path.mkdir()\n    for o in labels:            # scrape images off the web\n        search_term = (f'{o} {search_suffix}').strip()\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        print(f\"Getting urls for search_term = {search_term}..\")\n        urls = search_images_ddg(f'{search_term}', max_n=max_n) \n        urls = [ x for x in urls if (\"magpies\" not in x) and (\"charliebears\" not in x) ]   # kludge for now to keep download_images from hanging\n        print(f\"{search_term}: Got {len(urls)} image URLs. Downloading...\")\n        print(\"   urls = \",urls)\n        download_images(dest, urls=urls, preserve_filename=False)\n        print(\"    Images downloaded.\")\n        \n    fns = get_image_files(path)     # list image filenames\n    failed = verify_images(fns)     # check if any are unloadable\n\n    # remove what's unloadable\n    failed.map(Path.unlink);\n    if failed != []:\n        _ = [fns.remove(f) for f in failed]\n\n    # Extra: To avoid Transparency warnings, convert PNG images to RGBA, from https://forums.fast.ai/t/errors-when-training-the-bear-image-classification-model/83422/9\n    converted = L()\n    for image in fns:\n        if '.png' in str(image):\n            im = Image.open(image)\n            converted.append(image)  # old file name before resaving\n            im.convert(\"RGBA\").save(f\"{image}2.png\")    \n    converted.map(Path.unlink); # delete originals\n    print(f\"After checking images for issues, {len(get_image_files(path))} (total) images remain.\")\n    return path     # and return a pathlib object pointing to image dir\n\n\n\ndl_path = 'scraped_images'  # where we're saving to\nlabels = 'grizzly','black','teddy'  \nsearch_suffix = 'bear'\npath = scrape_for_me(dl_path, labels, search_suffix, max_n=100)\n\nGetting urls for search_term = grizzly bear..\n\n\n\n“Only 100(x3) images?! How can you hope to train a sophistical computer vision model with such a small amount of data?” Transfer Learning, my friend.\n\n\n\nLoad the Data and Take a Look\nHere are some examples of the images we grabbed:\n\n\nCode\ndef setup_dls(path):\n    data = DataBlock(        # define fastai data structure\n        blocks=(ImageBlock, CategoryBlock), \n        get_items=get_image_files, \n        splitter=RandomSplitter(valid_pct=0.2, seed=42),\n        get_y=parent_label,\n        item_tfms=RandomResizedCrop(224, min_scale=0.5),\n        batch_tfms=aug_transforms())\n    dls = data.dataloaders(path)    # define dataloader(s)\n    return dls \n\ndls = setup_dls(path)\ndls.show_batch()                # take a look\n\n\n\n\n\n\n\nDefine and Train the Model\n…With my special “triangle plot” visualization ( as demonstrated in the Part 1 blog post) called from my VizPreds fastai Callback (described below). (If you run this notebook on Colab, you’ll need to authorize Google Drive when prompted below, in order for the triangle plot image thumbnails to show).\nIn the plots that follow, mouse over the dots to see our bear images “move” as the model trains! You’ll see one epoch where the “body” network’s weights are frozen, and then four epochs where the full model trains.\n\n\nCode\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate])\nlearn.fine_tune(4, cbs=VizPreds)  # fine_tune does 1 epoch 'frozen', then as many more epochs you specify\n\n\nGenerating (URLS of) thumbnail images...\nDrive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\nThumbnails saved to Google Drive in /gdrive/My Drive/scraped_images_thumbs/\nWaiting on Google Drive until URLs are ready.\n\nEpoch 0: Plotting 51 (= 51?) points:\nEpoch 0: Plotting 51 (= 51?) points:\nEpoch 1: Plotting 51 (= 51?) points:\nEpoch 2: Plotting 51 (= 51?) points:\nEpoch 3: Plotting 51 (= 51?) points:\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.788116\n0.159182\n0.019608\n00:13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.327038\n0.050930\n0.000000\n00:13\n\n\n1\n0.231127\n0.067547\n0.019608\n00:13\n\n\n2\n0.170447\n0.047806\n0.019608\n00:13\n\n\n3\n0.134644\n0.027070\n0.019608\n00:13\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nDepending on which bear images get scraped on which day, even this may be trivially easy. When I ran this above, there was only one error: a tiny brown-colored bear cub that was classified by the model as grizzly but that was labeled (via DuckDuckGo) as a black bear. All the other points correctly “collapsed” very quickly nearly the poles.\nIf we look at the confusion matrix, we’ll see (for the time I ran this), only the one error mentioned above:\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nAnd if we plot the top losses, we’ll see (based on mousing-over in the triangle plot above) the images for the points furthest from the poles: first the mislabeled cub, then a grizzly mother and cub, then a teddy bear:\n\n\nCode\ninterp.plot_top_losses(6, nrows=1)\n\n\n\n\n\nWhen I ran this, the top loss image was actually a small dog! I coulda/shoulda checked that above in the triangle plot!\nWe’re not going to train this more or clean the dataset, instead let’s just remember that in this case, for 3 classes our error rate was 0.019608 (= 1/51, i.e. 1 error out of 51 images in the validation set), i.e. we were ~98% accurate.\n\n\nFour Classes\nLet’s add one more bear to the mix, the “spectacled bear”, and repeat this process, and visualize in 3D (sadly no image thumbnail mouseovers for 3D plots). First we’ll download and look at what we’ve got..\n\n\nCode\nlabels = 'grizzly','black','teddy','spectacled'\nscrape_for_me(dl_path, [labels[-1]], search_suffix, erase_dir=False) # download one more class\ndls = setup_dls(path)\ndls.show_batch()  \n\n\nspectacled bear: Got 100 image URLs. Downloading...\n   urls =  ['http://4.bp.blogspot.com/-6xZLUXHh8Uc/UUxmEy9QZBI/AAAAAAAAXcY/TMXg3WwS3GA/s1600/Spectacled+Bear+2.jpg', 'https://www.coolearth.org/wp-content/uploads/2016/06/Spectacled-Bear-2.jpg', 'http://3.bp.blogspot.com/-Sg8TmR5qh88/UUxmNY4d0HI/AAAAAAAAXcw/dpL2qFHZvp0/s1600/Spectacled+Bear+5.jpg', 'https://i.natgeofe.com/n/804d9cdb-f8da-449a-93b0-aac7a448b7c0/spectacled-bear_thumb.JPG?w=1200', 'https://www.activewild.com/wp-content/uploads/2018/03/spectacled-bears-1024x683.jpg', 'http://www.rainforest-alliance.org/sites/default/files/2016-09/spectacled-bear-header.jpg', 'https://lh6.googleusercontent.com/-Sptgd7fYIqY/TYk4YVRbndI/AAAAAAAAB7E/MYqqcMu-isw/s1600/Spectacled_Bear_Tennoji_2.jpg', 'https://www.vanwageningen.net/i/upload/2014/10/29/20141029132015-bc5e40e5-me.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-1002804-spectacled-bear-face.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear.jpg', 'https://colombiareports.com/wp-content/uploads/2017/07/spectacled_bear.jpg', 'http://tenrandomfacts.com/wp-content/uploads/2016/07/Spectacled-Bear.jpg', 'https://static1.squarespace.com/static/56a1a14b05caa7ee9f26f47d/t/57390f5520c6471cf068c6cc/1463357302812/', 'http://theamazonianassignment.weebly.com/uploads/1/2/1/3/12136910/944137409_orig.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-ecuadorian-andes-mountains-328x562.jpg?c14113', 'http://i1.treknature.com/photos/2959/bear_0111111.jpg', 'http://2.bp.blogspot.com/-vdZv8wGWMdo/UUxmNCvsvhI/AAAAAAAAXco/H5Pzgk5dtOk/s1600/Spectacled+Bear+6.jpg', 'https://media.sciencephoto.com/image/c0175188/800wm/C0175188-Spectacled_Bear.jpg', 'http://www.stlzoo.org/files/1014/0017/1158/ANDEANBEAR9410WINKELMAN2.jpg', 'https://www.tourradar.com/days-to-come/wp-content/uploads/2016/01/Spectacled-bear.jpg', 'https://www.zoochat.com/community/media/spectacled-bear.250657/full?d=1393011952', 'http://2.bp.blogspot.com/-1jh4WfVEyx4/T_A-OoNHGFI/AAAAAAAAMHM/Wm41l9WFfgs/s1600/Spectacled+Bear3.jpg', 'http://www.rainforest-alliance.org/sites/default/files/styles/750w_585h/public/2016-09/spectacled-bear-header.jpg?itok=0V7_KGpe', 'https://www.sustainability-times.com/wp-content/uploads/2019/06/AndeanBear_ZN.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Habitat.jpg', 'http://seethewild.org/wp-content/uploads/2016/05/features.jpeg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/spectacled-bear-portrait-rose-santuci-sofranko.jpg', 'http://kids.sandiegozoo.org/sites/default/files/2017-07/andean-bear-tree.jpg', 'https://southafricatoday.net/wp-content/uploads/2018/07/1532665810_1-1024x1000.jpg', 'https://upload.wikimedia.org/wikipedia/en/c/ce/Spectacled-bear.jpg', 'http://4.bp.blogspot.com/-zHQPWNws7VQ/TwK7vuoleYI/AAAAAAAAEuI/iZJGb0R9CEQ/s1600/spectacled_bear-spectacled_bear.jpg', 'http://www.ecns.cn/hd/2020/05/18/e41ef93a186c40a38726189b8901f6f3.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Baby-Spectacled-Bear.jpg', 'http://www.mammalwatching.com/wp-content/uploads/2018/01/EC-square-fox.jpg', 'http://newyorktrendnyc.com/wp-content/uploads/2013/11/andean-bear.jpg', 'https://home.bt.com/images/spectacled-bears-and-snow-leopards-among-creatures-getting-funding-boost-136427641293802601-180606124211.jpg', 'https://nationalzoo.si.edu/sites/default/files/animals/andeanbear-006.jpg', 'https://www.pitara.com/wordpress/wp-content/uploads/2001/02/spectacled-bear.jpg', 'http://3.bp.blogspot.com/-EGI8NqbPcvo/UUxmUJ6t1qI/AAAAAAAAXdA/fDUqD60Tv00/s1600/Spectacled+Bear.jpg', 'https://lh6.googleusercontent.com/-cc05A4QkDxc/TYk4S3w1qFI/AAAAAAAAB7A/3vb0t85A7GU/s1600/spectacled_bear_03.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-rocks-820x545.jpg?c14113', 'https://i.ytimg.com/vi/VXFIYIQgEkg/maxresdefault.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-71192249-spectacled-bear.jpg', 'https://farm3.staticflickr.com/2913/14137750328_d101997a4c_z.jpg', 'http://www.realworldholidays.co.uk/blog/wp-content/uploads/2015/12/spectacled-bear1.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-2.jpg', 'http://upload.wikimedia.org/wikipedia/commons/6/62/Spectacled_Bear_059.jpg', 'https://media.nbcwashington.com/2019/09/4621594915_9022bfd814_b.jpg?fit=1024%2C702', 'https://philadelphiazoo.org/wp-content/uploads/2019/10/Andean.SpectacledBear_Rosie_3395.jpg', 'http://2.bp.blogspot.com/-4bGjmY9K4pk/T_A-Yq5aNGI/AAAAAAAAMHc/pv9gEJ8O6Uo/s1600/Spectacled+Bear5.jpg', 'https://resize.hswstatic.com/w_907/gif/types-of-spectacled-bear0.jpg', 'http://1.bp.blogspot.com/-xGs4-W-3iSY/TjaZG6_XnOI/AAAAAAAABUE/AkEJ97L1HQc/s1600/156521.jpg', 'https://www.robertharding.com/watermark.php?type=preview&im=RM/RH/HORIZONTAL/1127-12838', 'https://i.pinimg.com/736x/fa/40/33/fa403340878f175322c07b860b9d6426--spectacled-bear-bolivia.jpg', 'http://img.izismile.com/img/img3/20100513/640/cute_spectacled_bear_640_03.jpg', 'https://ourplnt.com/wp-content/uploads/2016/03/Spectacled-bear.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/3956278926_328d1c6a54_o.jpg', 'http://cincinnatizoo.org/system/assets/uploads/2014/02/spectaculed-bear.jpg', 'http://3.bp.blogspot.com/-brSuqjGOIG0/URGtpFtyo_I/AAAAAAAAARg/vLE13RVoCsk/s1600/spectacled-bear.jpg', 'https://c2.staticflickr.com/2/1501/24144684635_73b9e79d88_b.jpg', 'https://critter.science/wp-content/uploads/2019/05/sb1.jpg', 'https://srcnaut.com/cdn-cgi/image/f=auto,fit=crop,g=0.5x0.5,w=2000,h=1125,q=90,d=1/upload/32/02/df/shutterstock-1188627676.jpg', 'http://seethewild.org/wp-content/uploads/2016/05/dreamstime_15373008-1.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Spectacled_Bear_-_Houston_Zoo.jpg/1200px-Spectacled_Bear_-_Houston_Zoo.jpg', 'https://www.zoochat.com/community/media/spectacled-bear-at-chester-15-07-12.192796/full?d=1342391273', 'https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1440,f_auto/MA_00155387_i4leyu.jpg', 'http://2.bp.blogspot.com/-PQGmVUDZAhs/TwK5urRX1LI/AAAAAAAAEtw/s569QHCcx8Y/s1600/spectacled+bear.jpg', 'https://bw-1651cf0d2f737d7adeab84d339dbabd3-gallery.s3.amazonaws.com/images/image_2865567/db2b44df10ba2a567a8e5cc34ddfeed0_original.JPG', 'http://etc.usf.edu/clippix/pix/spectacled-bear-with-piece-of-food-in-its-mouth_medium.jpg', 'https://www.activewild.com/wp-content/uploads/2018/03/spectacled-bear-standing-768x1024.jpg', 'http://2.bp.blogspot.com/-1mdQ66udYmw/T_A-UjaJk5I/AAAAAAAAMHU/uJw0kPcz1vc/s1600/Spectacled+Bear4.jpg', 'https://www.worldatlas.com/upload/43/04/5c/shutterstock-92656222-min.jpg', 'https://www.si.edu/sites/default/files/newsdesk/photos/andean-bear.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-ogimage.jpg', 'https://images.freeimages.com/images/large-previews/e0f/spectacled-bear-1561333.jpg', 'https://kentattractions.co.uk/wp-content/uploads/2016/09/Spectacled-Bear.jpg', 'http://gringosabroad.com/wp-content/uploads/2013/11/Male-Andean-Bear-face.jpg', 'https://rangerplanet.com/wp-content/uploads/2019/10/spectacled-bear.jpg', 'https://www.wisebirding.co.uk/wp-content/uploads/Spectacled-Bear.jpg', 'https://i.ytimg.com/vi/2TmnhBSyQoo/maxresdefault.jpg', 'https://vistapointe.net/images/spectacled-bear-5.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-tree-820x513.jpg?d8bc0c', 'https://standfirst-whitleyaward-production.imgix.net/content/uploads/1996/12/18134457/Tremarctos_ornatus_portrait.jpg?auto=compress,enhance,format&crop=faces,entropy,edges&fit=crop&fm=pjpg&w=1866&h=1400', 'https://c1.staticflickr.com/7/6037/6304684676_8fe4be5717_b.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/5566818433_9c994ec063_o.jpg', 'https://www.bwallpaperhd.com/wp-content/uploads/2018/12/SpectacledBear.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-eating-fruit.jpg', 'https://images.fineartamerica.com/images-medium-large-5/andean-bear-spectacled-bear-houston-zoo-tessa-fairey.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-598140-spectacled-bear.jpg', 'http://magarticles.magzter.com/articles/8803/216564/58e38ecbbeeb4/Conservation-Insight-Andean-Bear.jpg', 'http://www.rainforest-alliance.org/sites/default/files/styles/large/public/2016-09/spectacled-bear.jpg?itok=BXWidYtz', 'https://bw-1651cf0d2f737d7adeab84d339dbabd3-gallery.s3.amazonaws.com/images/image_2865571/a3195154ccdac7c9efcbed5727462346_original.JPG', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-in-the-Wild.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/6862037661_30664cf53f_o.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Eating.jpg', 'http://2.bp.blogspot.com/-3UzekS1270M/TiAwko0VrQI/AAAAAAAAAKI/rOXYR8LXLHM/s1600/spectacled_bear_3.jpg', 'https://i.pinimg.com/736x/d2/99/fa/d299fa76c3d1a5c0e017514b086ddd9c--spectacled-bear-teddy-bears.jpg', 'https://c2.staticflickr.com/4/3685/8852797884_8f3a84d84c_b_d.jpg', 'http://www.kidzone.ws/lw/bears/images/spectacled-bear.jpg', 'https://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Images.jpg']\n    Images downloaded.\nAfter checking images for issues, 348 (total) images remain.\n\n\n\n\n\nThen we’ll train the model:\n\n\nCode\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate])\nlearn.fine_tune(4, cbs=VizPreds)  # fine_tune does 1 epoch 'frozen', then as many more epochs you specify\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.685202\n0.791340\n0.275362\n00:16\n\n\n\n\n\nEpoch 0: Plotting 69 (= 69?) points:\nEpoch 0: Plotting 69 (= 69?) points:\nEpoch 1: Plotting 69 (= 69?) points:\nEpoch 2: Plotting 69 (= 69?) points:\nEpoch 3: Plotting 69 (= 69?) points:\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.474520\n0.377549\n0.130435\n00:15\n\n\n1\n0.337095\n0.232552\n0.086957\n00:16\n\n\n2\n0.252857\n0.182564\n0.072464\n00:15\n\n\n3\n0.207571\n0.131986\n0.057971\n00:16\n\n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n…So we see that spectacled bears, being black-colored, can get mixed up with black bears sometimes.\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nThe biggest losses are usually when the spectacled bears’ “spectacles” are not fully in view (e.g. in a side shot), in which case either its black coat makes it seem like a black bear, or its shagginess & size make it seem like a grizzly:\n\n\nCode\ninterp.plot_top_losses(6, nrows=1)\n\n\n\n\n\n(Another time I ran this, the top loss was for an image of a fox that was mislabeled as ‘spectacled’! Clearly just scraping from DuckDuckGo is not fool-proof. LOL)\nNote that we had to retrain the model in order to handle the spectacled bear – there was no “slot” for it in the original 3-class model. We’re now going to switch to a contrastive loss model that uses a Siamese Network, which…we’ll see how it does on the spectacled bear after we train it on only the first 3 types of bear.\nI’m going to pause writing and come back in the morning, so that I can title the next section…"
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#contrastive-loss-demo-same-bears-different-day",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#contrastive-loss-demo-same-bears-different-day",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "2. Contrastive Loss Demo: Same Bears, Different Day",
    "text": "2. Contrastive Loss Demo: Same Bears, Different Day\nAs we described in blog post Part 1, contrastive loss systems achieve metric learning by employing a Siamese Network.\nLesson 2 of the FastAI book already has a very effective Siamese Network demo. It does not use contrastive loss, and yet it will (see Appenix below) outperform my contrastive loss version (in terms of accuracy, by about 5 to 6%). So why would we want contrastive losses? Well, because they are useful for learning “semantically meaningful” embeddings, something the aforementioned Siamese Network lesson did not set as a goal.\nThe FastAI lesson uses the Oxford PETS dataset which has a different format than our bear-images dataset, so the “good news” is I got an opportuny to learn more about writing my own fastai dataloader by altering Jeremy’s instructions in the Lesson.\n\nWriting My First DataLoader(s)\n…Actually it turns out that we can copy all we need from the fastai tutorial except the parts where we grab the image files and the label_func() (below) that gives us class names based on the file pathnames. What we need is a routine that grabs pairs of images at a time, and labels* them as either “similar” (denoted by a label value of 1) or not (denoted by a value of 0). .\n\nTerminology: By “labels” I refer exclusively to “ground truth” or “target” values, which I may use interchangeably (esp. “targets”) supplied by the dataset/user – we are doing Supervised Learning. Model outputs will be referred to as “predictions”.\n\nSo, adapting the FastAI Siamese Networks Tutorial to a slightly different (and slightly simpler) data file structure,… it turns out we won’t be writing a DataBlock, but rather writing a Transform. (I don’t yet understand why TBH.)\nBefore we do that, we’re going to define a custom input object called SiameseImage that our dataloader/Transform can operate on, and that we can “show” in the notebook – it pays to be able to look at your data! ;-)\nLet’s review our list of (3-bears) files, and we’ll leave out the spectacled bears for now\n\nlabels = 'grizzly','black','teddy'\npath = scrape_for_me(dl_path, labels, search_suffix, max_n=300)\n#labels = ['cat','dog','horse']\n#path = scrape_for_me(dl_path, labels, '', max_n=300)\nsubdirs, path, files = labels, Path(dl_path), []\nfor s in subdirs: files += get_image_files(path/s)\n\nprint(f'files has {len(files)} elements.')\nprint(f\"files[0] = {files[0]}, files[-1] = {files[-1]}\")\n\ngrizzly bear: Got 280 image URLs. Downloading...\n   urls =  ['https://i0.wp.com/www.commonsenseevaluation.com/wp-content/uploads/2013/08/Bear.jpg', 'https://blog.humanesociety.org/wp-content/uploads/2019/04/BEAR_GRIZZLY_A45T3555_468435.jpg', 'https://www.conservationnw.org/wp-content/uploads/2017/06/Grizzly-bear-family-Copyright-iStock.com_PhotosbyAndy-4.2015-e1507229404830.jpg', 'https://images.fineartamerica.com/images-medium-large/grizzly-bear-portrait-jim-guy.jpg', 'https://www.mtpr.org/sites/kufm/files/styles/x_large/public/201809/grizzly-bear-02_Istock.jpg', 'https://www.rmef.org/wp-content/uploads/2019/04/grizzly.jpg', 'https://images.dailyhive.com/20170509095127/bear-148-parks-canada-amar-atwal.jpeg', 'https://api.time.com/wp-content/uploads/2015/08/grizzly-bear.jpg?quality=85&w=1200&h=628&crop=1', 'https://a57.foxnews.com/a57.foxnews.com/static.foxnews.com/foxnews.com/content/uploads/2018/09/640/320/1862/1048/Grizzly-iStock.jpg?ve=1&tl=1?ve=1&tl=1', 'https://media.spokesman.com/photos/2014/09/15/jj-Grizzly_MT.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/grizzly-bear-in-snow-tendrel-images.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-Desktop-Wallpaper-.jpg', 'https://i.cbc.ca/1.4132188.1495755019!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/grizzly-bear.jpg', 'http://elelur.com/data_images/mammals/grizzly-bear/grizzly-bear-08.jpg', 'https://images.fineartamerica.com/images-medium-large-5/grizzly-bear-cub-phyllis-taylor.jpg', 'https://www.mtpr.org/sites/kufm/files/styles/x_large/public/201812/Grizzly-bear-family_LuCaAr-iStock.jpg', 'https://images.fineartamerica.com/images-medium-large/grizzly-bear-yukon-robert-postma.jpg', 'https://www.unilad.co.uk/wp-content/uploads/2020/02/close-up-photography-of-grizzly-bear-1068554.jpg', 'https://reidparkzoo.org/wp-content/uploads/2013/09/grzbear2812a.jpg', 'https://outsider.com/wp-content/uploads/2020/06/Grizzly-Bear-3.png', 'http://i.huffpost.com/gen/1227495/images/o-GRIZZLY-BEAR-facebook.jpg', 'https://i.ytimg.com/vi/Tue__Hlazh4/maxresdefault.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-Wallpapers-and-Backgrounds.jpg', 'https://get.wallhere.com/photo/animals-wildlife-bears-fur-wilderness-Grizzly-Bears-Grizzly-bear-brown-bear-bear-fauna-mammal-1920x1200-px-snout-carnivoran-organism-american-black-bear-terrestrial-animal-771353.jpg', 'http://traveltalesoflife.com/wp-content/uploads/Kicking-Horse-Resort-Grizzly-Close-Up.jpg', 'http://www.hickerphoto.com/images/1024/-56752.jpg', 'https://images.dailyhive.com/20161104114500/Grizzly-bears-TrophyLush.jpg', 'https://mtsgreenway.org/wp-content/uploads/2019/08/September-Grizzly-Large.png', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/1-grizzly-bear-art-wildlife-fine-art.jpg', 'https://www.livewaterjacksonhole.com/wp-content/uploads/2020/06/Grizzly-399-Grand-Teton-National-Park.jpg', 'https://i0.wp.com/rangerrick.org/wp-content/uploads/2018/04/RRMAR13_6-11Grizzlies.jpg?fit=1156%2C650&ssl=1', 'http://grizzlybearwatching.com/wp-content/uploads/2017/03/BlackBear1-1500x998.jpg', 'http://mediad.publicbroadcasting.net/p/kufm/files/styles/medium/public/201410/grizzlybear.jpg', 'https://www.vmcdn.ca/f/files/rmotoday/images/grizzly122_kalra.jpg;w=960', 'http://4.bp.blogspot.com/-iRDehywX024/UNMpj9s5kVI/AAAAAAAAADI/9sxChqYMV2c/s1600/Grizzly-Bear-Yellowstone.jpg', 'https://2.bp.blogspot.com/-d0DjXibvH8w/UQN3Xn4QWDI/AAAAAAAAFb0/2nUaH5eoapg/s1600/Grizzly+Bear-2013-0pic-0.jpg', 'https://www.gannett-cdn.com/presto/2020/07/21/PGRF/83612915-6ab2-4f0d-aeca-7088f219ecec-AP20191697142470.jpg?crop=2999,1687,x0,y0&width=2999&height=1687&format=pjpg&auto=webp', 'http://assets.nydailynews.com/polopoly_fs/1.3269663.1498159924!/img/httpImage/image.jpg_gen/derivatives/article_750/654079900.jpg', 'https://d.ibtimes.co.uk/en/full/1621655/grizzly-bears.jpg', 'https://www.idahoconservation.org/wp-content/uploads/2020/06/Grizzly-Bear-photo-by-Jim-Mellen-1.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Ursus_arctos_californicus%2C_Santa_Barbara%2C_Natural_History_Museum.jpg/1200px-Ursus_arctos_californicus%2C_Santa_Barbara%2C_Natural_History_Museum.jpg', 'https://patch.com/img/cdn20/users/22906546/20180731/095817/styles/raw/public/processed_images/image006_1-1533085927-8474.jpg', 'https://wallup.net/wp-content/uploads/2017/11/17/265280-bears-Grizzly_bear.jpg', 'https://keyassets.timeincuk.net/inspirewp/live/wp-content/uploads/sites/2/2019/06/GettyImages-525103104.jpg', 'https://www.tsln.com/wp-content/uploads/2018/10/bears-tsln-101318-3-1240x826.jpg', 'http://www.bearsmart.com/wp-content/uploads/2015/01/why-bears3-730x330.jpg', 'https://cdn.rbth.com/980x-/all/2016/10/07/grizzly_gettyimages-10188130_b.jpg', 'https://wildlife.org/wp-content/uploads/2015/08/Grizzly-Bear-Credit-Washington-State-University-620x264.jpg', 'http://agenda21news.com/wp-content/uploads/2014/12/Grizzly-bear.gif', 'https://bloximages.chicago2.vip.townnews.com/codyenterprise.com/content/tncms/assets/v3/editorial/c/75/c75e6ae4-5ab0-11e7-9823-3b1d2aaec452/595173d52fa4e.image.jpg?resize=1200%2C885', 'https://cdn11.bigcommerce.com/s-db59p/images/stencil/1280x1280/products/21092/42704/oversized_12162019_01__97480.1577021805.jpg?c=2', 'http://1.bp.blogspot.com/_VgXaDXiFvX4/TOLc_oEzGcI/AAAAAAAAA9c/Q7qtQ7R1ju8/s1600/grizzly-bear-1.jpg', 'https://devdenverzoo.com/wp-content/uploads/2018/09/Grizzly-Bear_03.jpg', 'https://miro.medium.com/max/1758/1*cBBVLzODaagrL7V2eNSjSQ.jpeg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/3/grizzly-bear-jerry-fornarotto.jpg', 'https://www.wildnatureimages.com/images/xl/170628-145-Grizzly-Bear-Sow-with-Cubs.jpg', 'http://www.telegraph.co.uk/content/dam/news/2016/06/21/99682788_Two_grizzly_bears_at_the_Woodland_Park_Zoo_eat_salmon_after_the_fish_was_tossed_to_them_by-xlarge_trans_NvBQzQNjv4BqDfnT11g5XZty1Nobnb9oJsN7dAtWJUhNbBLTzbEHUEY.jpg', 'https://bloximages.chicago2.vip.townnews.com/trib.com/content/tncms/assets/v3/editorial/c/af/caf539ee-9c28-5fda-819a-3aeaa1f029bc/582cffad235a4.image.jpg?resize=1200%2C800', 'https://wildlife.org/wp-content/uploads/2016/08/grizzly1.jpg', 'https://cdn.thefencepost.com/wp-content/uploads/sites/12/2017/06/grizzly-RFP-070317-1240x826.jpg', 'http://static.independent.co.uk/s3fs-public/styles/story_large/public/thumbnails/image/2015/10/06/09/Grizzly.jpg', 'http://mediad.publicbroadcasting.net/p/kufm/files/styles/x_large/public/201603/grizzly-bear_Nathan-Rupert-CC-BY-NC-ND_0.jpg', 'https://www.cbbulletin.com/wp-content/uploads/2020/04/griz.jpg', 'https://img.thedailybeast.com/image/upload/v1492108548/articles/2016/10/15/won-t-you-please-adopt-a-grizzly-bear-cub/161014-Andrews-Grizzly-cubs-tease__p9gwbc.jpg', 'https://onekindplanet.org/wp-content/uploads/2017/09/grizzlybear-edit.jpg', 'http://www.akronzoo.org/Data/Sites/1/media/GalleryImages/105/WebImages/grizzly-bear-hero-1.jpg', 'http://wallup.net/wp-content/uploads/2016/01/139369-bears-nature-animals-river-baby_animals-Grizzly_Bears-Grizzly_bear.jpg', 'https://weneedfun.com/wp-content/uploads/Grizzly-Bear-4.jpg', 'https://1.bp.blogspot.com/-l-Y6zD_KF6o/UQN3jPgigAI/AAAAAAAAFcU/HOKJWQmNjo4/s1600/Grizzly+Bear-2013-0pic-05.jpg', 'https://devdenverzoo.com/wp-content/uploads/2018/09/Grizzly-Bear_02.jpg', 'https://www.conservationnw.org/wp-content/uploads/2017/10/grizzly-bears-1280x950.jpg', 'https://s1.it.atcdn.net/wp-content/uploads/2013/07/Grizzly-bear.jpg', 'https://outsider.com/wp-content/uploads/2020/09/grizzly-bear-ferociously-protects-kill-while-tourists-watch-yellowstone-national-park-1170x702.jpg', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/wyoming-grizzly-bear-og_0.jpg', 'https://www.wildernesscommittee.org/sites/default/files/2018-05/grizzly_bear.jpg', 'http://www.wyofile.com/wp-content/uploads/2017/11/grizzly-pic-e1509641980482.jpg', 'http://averageoutdoorsman.com/wp-content/uploads/2013/02/grizzly-bear.jpg', 'https://www.publicdomainpictures.net/pictures/90000/velka/grizzly-bear-portrait.jpg', 'https://www.vancouverobserver.com/sites/vancouverobserver.com/files/images/article/body/grouse-grizzly_n3d3306-web.jpg', 'http://www.wild-facts.com/wp-content/uploads/2012/04/Grizzlybear55.jpg', 'http://kgmi-am.sagacom.com/wp-content/blogs.dir/70/files/2014/08/grizzly-bear.jpg', 'http://www.shetzers.com/wp-content/uploads/2017/06/W-Blonde-Bear-Cub-and-Mother.jpg', 'https://i2.wp.com/jakesnatureblog.com/wp-content/uploads/2016/06/baby-brown-bear.jpg', 'https://d.newsweek.com/en/full/400233/rtr4kseg.jpg', 'https://wallup.net/wp-content/uploads/2018/10/09/125286-grizzly-bear-forest.jpg', 'https://www.columbiatribune.com/storyimage/MO/20171003/NEWS/171009653/AR/0/AR-171009653.jpg', 'https://images.wallpapersden.com/image/download/bear-grizzly-bear-eyes_amhrZ5SZmpqtpaSklGZuZ2WtZmVtZQ.jpg', 'https://wallpapertag.com/wallpaper/full/a/1/1/437141-grizzly-bear-backgrounds-1920x1080-for-lockscreen.jpg', 'https://bloximages.chicago2.vip.townnews.com/idahostatejournal.com/content/tncms/assets/v3/editorial/6/45/64588e0b-55e7-5585-bc61-819ae3630cff/59b721e2eb964.image.jpg', 'https://devdenverzoo.com/wp-content/uploads/2018/09/Grizzly-Bear_05.jpg', 'https://www.washingtonpost.com/resizer/i6X-FE4FqlbaGDzh9CanT4YU99Y=/1440x0/smart/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/2C55XVXHAE5FTHUGC726432CYU.jpg', 'http://www.100hdwallpapers.com/wallpapers/3840x2160/two_grizzly_bears_in_water_4k-hd_wallpapers.jpg', 'https://www.uniquelynorthwest.com/wp-content/uploads/2014/12/grizzly-bear-alaska-incentive.jpg', 'https://www.conservationnw.org/wp-content/uploads/2020/03/JSW_6993-scaled.jpg', 'https://www.conservationnw.org/wp-content/uploads/2017/10/Grizzly-bear-in-mountain-meadow.-Photo-copyright-Jason-Verschoor.-iStockphoto.com_-e1507583569416.jpg', 'http://cdn.roaring.earth/wp-content/uploads/2016/03/Scratching-head-main-text.jpg', 'https://content.presspage.com/uploads/1979/1920_190416-oped-nielsen-grizzly-alex-taylor-banner-534230.jpg?10000', 'https://outsider.com/wp-content/uploads/2020/07/Grizzly-Bear.png', 'https://static01.nyt.com/images/2020/07/10/multimedia/10xp-bears-pix/10xp-bears-pix-videoSixteenByNineJumbo1600.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/grizzli-bear-cub-lyl-dil-creations.jpg', 'https://cowboystatedaily.com/wp-content/uploads/2020/05/bear-photo-not-cat-scaled.jpg', 'https://2.bp.blogspot.com/-p-QKKk3T8O0/UQN3a5YSALI/AAAAAAAAFcE/iTlNg3jee9A/s1600/Grizzly+Bear-2013-0pic-03.jpg', 'https://3.bp.blogspot.com/-_kQl-36lVL4/ThYEDrnAGBI/AAAAAAAABdM/sQkA6n28LJY/s1600/Grizzly+7+WA.jpg', 'https://fotos.perfil.com/2019/12/02/trim/1140/641/grizly-bear-811359.jpg', 'https://wildlifeimages.org/wp-content/uploads/2016/09/DSC1704.jpg', 'https://lakechelannow.com/wp-content/uploads/2019/08/Grizzly_Bear-Face-800x600.jpg', 'https://www.realtree.com/sites/default/files/content/images/open-graph/2020/cdenniswdonohue-shutterstock-griz.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-4K.jpg', 'http://2.bp.blogspot.com/-u2jNl3VZLpU/T5LoHWgdunI/AAAAAAAAFPo/DdDtaVJGTco/s1600/grizzly-bear_grizzly-bear.jpg', 'https://life.hawkeoptics.com/wp-content/uploads/2019/07/Keith-Crowley-Grizzly-Bears-03.jpg', 'https://img.theculturetrip.com/1440x807/wp-content/uploads/2018/01/carnivore-nature-wild-alaska-wildlife-grizzly-bear-861962.jpg', 'https://www.skinnymoose.com/outdoorsmorgasbord/files/2014/03/Grizzly-Bear-3.jpg', 'https://blog.humanesociety.org/wp-content/uploads/2020/07/grizzly-bear-iStock-647126690_369827.jpg', 'https://ewscripps.brightspotcdn.com/dims4/default/083faf0/2147483647/strip/true/crop/806x453+0+21/resize/1280x720!/quality/90/?url=https:%2F%2Fewscripps.brightspotcdn.com%2F00%2F7f%2F2489255d4d9ab8fc447f4b57ed66%2Fgrizzly-bear.jpg', 'https://www.lintelligencer.com/wp-content/uploads/2020/09/Alaska-Grizzly-kills-hunter-in-attack-at-national-park-Report.jpg', 'https://the-biggest.net/wp-content/uploads/2019/07/how-tall-was-the-biggest-grizzly-bear.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-for-desktop-background.jpg', 'https://www.shetzers.com/wp-content/uploads/2017/06/R-Grizzly-bear-cub-standing-tall-704x880.jpg', 'https://4.bp.blogspot.com/-7YmqPws-fF4/WhI9SBZv5dI/AAAAAAAAKik/KR02M342yKYwifxV3MgJQJYu1vhydOhFQCLcBGAs/s1600/IMG_3113.JPG', 'https://resize.hswstatic.com/w_1024/gif/grizzly-center.jpg', 'https://c.wallhere.com/photos/80/58/brown_bear_face_beautiful_background-629567.jpg!d', 'https://buckrail.com/wp-content/uploads/2018/05/Grizzly-bears-are-a-popular-attraction-for-visitors.-Jackson-Hole-EcoTour-Adventures.jpg', 'https://gephardtdaily.com/wp-content/uploads/2015/08/11754872_1109393535742666_5103459483633495575_o.jpg', 'http://www.outdoorhub.com/wp-content/uploads/sites/2/2015/08/outdoorhub-yellowstone-grizzly-bear-kills-hiker-in-apparent-predatory-attack-2015-08-12_15-18-35.jpg', 'https://www.whistler.ca/sites/default/files/styles/crop_min_663/public/2019/Apr/page/main-images/25900/grizzly.jpg?itok=NGH8XPEe', 'https://d.ibtimes.co.uk/en/full/1462246/grizzly-bear.jpg', 'https://cdn.rbth.com/all/2016/10/07/grizzly_gettyimages-dv1620005_b.jpg', 'https://www.rmef.org/wp-content/uploads/2020/05/wyoming-grizzly-bear.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/standing-grizzly-bear-cub-in-the-evening-light-daryl-l-hunter.jpg', 'https://www.wintersexpress.com/files/2019/05/animal-bear-cute-162340.jpg', 'https://weneedfun.com/wp-content/uploads/Grizzly-Bear-Picture-4.jpg', 'https://intercontinentalcry.org/wp-content/uploads/2019/08/grizzly-bear1.jpg', 'http://calgaryguardian.com/wp-content/uploads/2017/05/142_1289.jpg', 'https://cdn.abcotvs.com/dip/images/492803_012715-ss-grizzly-bears-img.jpg?w=1600', 'http://www.publicdomainpictures.net/pictures/90000/velka/grizzly-bear-profile.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-HD-Desktop.jpg', 'https://townsquare.media/site/101/files/2020/05/GrizzRS.jpg?w=1200', 'https://wallpapertag.com/wallpaper/full/e/b/8/437151-grizzly-bear-backgrounds-2560x1600-for-tablet.jpg', 'https://images.fineartamerica.com/images-medium-large-5/usa-alaska-grizzly-bear-cub-sits-brenda-tharp.jpg', 'https://external-preview.redd.it/bccjV-0WX40IgFdJ1FOy7_IE9nLPXR0wloCap7hXTSA.jpg?auto=webp&s=a17fd3832e57032a5b44af94010a189e4e906ea6', 'https://defenders.org/sites/default/files/styles/meta_image/public/2019-04/grizzly_bear_sow_jim_peaco_nps_header.jpg?itok=vTdBffpn', 'https://outsider.com/wp-content/uploads/2020/08/grizzly-bear-hiker-glacier-national-park-video.png', 'https://i.cbc.ca/1.4174820.1506608673!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_1180/grizzly-glares-at-viewer.jpg', 'https://ourplnt.com/wp-content/uploads/2020/12/grizzly-bear.jpg', 'https://spca.bc.ca/wp-content/uploads/WS2017_92846_Stephen-Lustig_Grizzly-Bear.jpg', 'https://braincharm.com/wp-content/uploads/2019/08/cropped-Grizzly-Bear-Bozeman-Grizzly-Montana-Bear-3483886.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-Wallpapers-HD.jpg', 'https://cdn.britannica.com/19/186719-050-887A6F2C/Grizzly-bear-Rocky-Mountains-Wyoming.jpg', 'http://www.pbs.org/wnet/nature/files/2018/07/Bear133.jpg', 'http://1.bp.blogspot.com/-CoOIzMhLQUI/UHzVx6IxyVI/AAAAAAAAA-c/RHMhnAXXniU/s1600/Grizzly+Bear+Sleepy.jpg', 'http://www.photos-public-domain.com/wp-content/uploads/2012/03/grizzly-bear.jpg', 'http://img.huffingtonpost.com/asset/1910_1000/58c0a5fe1d000027007cd5c0.jpg', 'http://blog.theclymb.com/wp-content/uploads/2014/09/angry-grizzly-bear.jpg', 'http://s3.amazonaws.com/Photo_of_the_Day/system/uploads/photo/image/12388/sized_Grizzly_Bear_Cubs.jpg', 'https://www.unilad.co.uk/wp-content/uploads/2015/08/UNILAD-mom-grizzly8.jpg', 'https://www.hoglezoo.org/wp-content/themes/hoglezoo_new/images/animal_finder/491283162GrizzlyBear.jpg', 'http://dawnsbrain.com/wp-content/uploads/2014/11/grizzly-bear-6.jpg', 'https://4.bp.blogspot.com/-2_RuGHXGm0Y/UQW3vwH37JI/AAAAAAAAEzc/Mt1seFmFrog/s1600/Grizzly-Bear-.jpg', 'https://i.ytimg.com/vi/zzJbkmOVgMI/maxresdefault.jpg', 'https://cdn.abcotvs.com/dip/images/1407687_062916-wtvd-file-grizzly-bear-img.jpg?w=1280&r=16:9', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-Background-.jpg', 'https://devdenverzoo.com/wp-content/uploads/2018/09/Grizzly-Bear_04.jpg', 'http://www.montanaoutdoor.com/wp-content/uploads/2018/02/Grizzly-King-FEAT.jpg', 'https://outsider.com/wp-content/uploads/2020/06/Grizzly-Bear-Unsplash.png', 'http://www.wallpapergeeks.com/wp-content/uploads/2014/03/Grizzly-Bear-Near-McNeil-River-Alaska.jpg', 'https://outsider.com/wp-content/uploads/2020/09/two-grizzly-bears-brutally-fight-over-fish-in-alaska-1170x702.png', 'https://lh3.googleusercontent.com/proxy/XiCQkPJB2tAag16S3wO0yNJnMQXTpTzEOZw-eWQKr6nOaJhlqeJzHrDhPowuHfLQREfxRPAN9kjW-6KrwU8HUeBesZsH-nLKkYV2lr5NE5sXcRy5xwMqv9cbYYYNpEJnhJs=w1200-h630-p-k-no-nu', 'https://cottagelife.com/wp-content/uploads/2020/07/shutterstock_546562111.jpg', 'https://images.fineartamerica.com/images-medium-large-5/grizzly-bear-6-thomas-woolworth.jpg', 'https://redtri.com/wp-content/uploads/2014/07/grizzly-bear-2.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/grizzly-bear-twenty-two-north-photography.jpg', 'https://a57.foxnews.com/static.foxnews.com/foxnews.com/content/uploads/2019/06/931/524/iStock-grizzly-bear-and-cubs.jpg?ve=1&tl=1', 'http://www.alaskaphotoworld.com/alaska365/wp-content/uploads/2013/06/Grizzly-Bear-Yukon-Alaska-c-Laurent-Dick-Wild-Alaska-Travel(pp_w1060_h657).jpg', 'http://mediad.publicbroadcasting.net/p/kufm/files/styles/x_large/public/201712/grizzly-bear-snow_pd.jpg', 'https://i.ytimg.com/vi/jR_U9AoAtJE/maxresdefault.jpg', 'http://www.marketwire.com/library/MwGo/2017/3/7/11G132324/Images/3_Bears_Approach_by_Jim_Lawrence-4f4d16b51311853286570ff0d3022a81.jpg', 'http://2.bp.blogspot.com/-e6e4YOB-K9I/UQEz1wt72TI/AAAAAAAAC0s/YZUgPnIwTks/s640/Grizzly+bear.jpg', 'https://media.spokesman.com/photos/2018/08/18/Grizzly_Bears_Montana.JPG.jpg', 'https://s3-assets.eastidahonews.com/wp-content/uploads/2015/10/03030033/0ad8593723.jpg', 'https://www.mtpr.org/sites/kufm/files/styles/x_large/public/201512/grizzly-bear_02-PD.jpg', 'https://www.indianz.com/News/2017/01/12/grizzlybearyellowstoneitingchiang.jpg', 'https://www.alloutdoor.com/wp-content/uploads/2019/08/Grizzly_Bear_Ursus_arctos_ssp..jpg', 'https://www.getwest.ca/site/assets/files/1049/uchuck-grizzly-bear.jpg', 'http://zooidaho.org/wp-content/uploads/2016/02/shonibear.jpg', 'http://www.cs.columbia.edu/~sedwards/photos/kyle200604/20060412-7108%20Grizzly%20bear.jpg', 'https://cdn.suwalls.com/wallpapers/animals/grizzly-bear-46852-1920x1200.jpg', 'http://wildlifewaystation.org/images/content/animals/Bear-Grizzly-2.JPG', 'http://2.bp.blogspot.com/-NjMTuklENdE/UHzVv_8dIxI/AAAAAAAAA-U/tNBsQDn8kFI/s1600/Grizzly+Bear+Pic.jpg', 'https://live.staticflickr.com/8078/8277276763_b4d8cff511_b.jpg', 'http://images.fineartamerica.com/images-medium-large/1-grizzly-bear-yukon-robert-postma.jpg', 'https://www.yourhikeguide.com/wp-content/uploads/scandinavian_wildlife_park_26-26995.jpg', 'http://bloximages.newyork1.vip.townnews.com/stltoday.com/content/tncms/assets/v3/editorial/b/99/b99156d5-0ad8-5b5c-a98a-d805c9a5d6bb/57eaaca236063.image.jpg?resize=1200%2C1369', 'http://www.tweedsmuirparklodge.com/assets/Uploads/_resampled/ScaleWidthWzEyMDBd/Grizzly-bear-with-fish-at-Tweedsmuir.jpg', 'https://1471793142.rsc.cdn77.org/data/images/full/27318/palm-beach-zoo-debuts-grizzly-bear-cubs.jpg', 'https://media.salon.com/2014/03/Grizzly-Bear.jpg', 'https://d3d0lqu00lnqvz.cloudfront.net/media/media/897b2e5d-6d4c-40fa-bbe8-6829455747e2.jpg', 'https://www.trbimg.com/img-5966ab20/turbine/ct-yellowstone-grizzly-scarface-killed-20170712', 'https://www.wyomingpublicmedia.org/sites/wpr/files/styles/x_large/public/202005/grizzly_and_cubs.png', 'https://cmkt-image-prd.freetls.fastly.net/0.1.0/ps/5129718/910/1365/m2/fpnw/wm1/mrkxj2x8a849nf6stvtxuofhydmjnf7slsysexqsuhadz3xaxi6ze1soz8huzzsg-.jpg?1538388129&s=48c555641b232ab7c64b9c76b97a67bc', 'http://www.my-photo-blog.com/wp-content/uploads/2017/07/Grizzly-Bear-Nursing.jpg', 'https://i0.wp.com/pgdailynews.ca/wp-content/uploads/2017/12/Grizzly.jpg', 'https://www.naturalworldsafaris.com/~/media/images/blogs/2018/01/grizzly-bear-hunting-ban/mmr_2015-08-06_7055.jpg', 'http://i.huffpost.com/gen/1747973/images/o-GRIZZLY-BEAR-RAINFOREST-facebook.jpg', 'https://bloximages.chicago2.vip.townnews.com/trib.com/content/tncms/assets/v3/editorial/b/6c/b6c62d25-f84e-51b0-95ae-223fcd943312/57cc8795a69a1.image.jpg?resize=1200%2C800', 'https://www.readersdigest.ca/wp-content/uploads/2018/09/grizzly-bear-cub.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/grizzly-bear-judy-vincent.jpg', 'http://2.bp.blogspot.com/-9JAlJayP2XU/UQN3Wffpa8I/AAAAAAAAFbs/eMP5BcuLetc/w1200-h630-p-k-nu/Grizzly+Bear-2013-0pic-02.jpg', 'https://townsquare.media/site/99/files/2018/12/Bear.jpg?w=1200&h=0&zc=1&s=0&a=t&q=89', 'http://www.hickerphoto.com/images/1024/alert-grizzly-bear-56745.jpg', 'https://s3.amazonaws.com/images.gearjunkie.com/uploads/2015/07/Grizzly-Bear.jpg', 'https://wildlifefriendly.org/wp-content/uploads/2015/09/Grizzly.jpg', 'https://www.thefencepost.com/wp-content/uploads/sites/12/2018/08/grizzlies-tsln-081118-1240x828.jpg', 'https://www.expeditionsalaska.com/wp-content/uploads/2017/04/16-brown-bear-photos-2377.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-Download.jpg', 'http://4.bp.blogspot.com/-QmwOWiR3ydM/USOj4DBjZSI/AAAAAAAAFBM/bvX4-MmH-tY/s1600/Grizzly_Bear_Info_Images+08.jpg', 'https://www.scitechnow.org/wp-content/uploads/2015/05/Ep27_Grizzly-Bears.jpg', 'https://whatthingsweigh.com/wp-content/uploads/grizzly_bear.jpg', 'https://www.huntingillustrated.com/wp-content/uploads/2014/03/Depositphotos_93855856_xl-2015-1-scaled.jpg', 'https://www.wildnatureimages.com/images/xl/080914-113-Grizzly-Bear.jpg', 'http://media1.s-nbcnews.com/j/newscms/2017_07/1905761/170217-grizzly-alaska-mn-1130_e938eae58cdfe33a8c5bd9af07356dd9.nbcnews-ux-2880-1000.jpg', 'https://i.ytimg.com/vi/-klfQbvz0Nw/maxresdefault.jpg', 'https://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/embed/public/2017/11/20/grizzly-bear.jpg', 'https://www.canadatravelspecialists.com/media/3362/bear-catching-salmon.jpeg', 'https://www.legendsofamerica.com/wp-content/uploads/2018/07/Grizzlybear-269x300.png', 'https://farm8.staticflickr.com/7026/6843888709_b6136360c8_z.jpg', 'https://i.cbc.ca/1.4775731.1533917302!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/eyes-of-the-grizzly.jpg', 'http://mediad.publicbroadcasting.net/p/kufm/files/styles/x_large/public/201606/grizzly_PD.jpg', 'https://i1.wp.com/backcountryjourneys.com/wp-content/uploads/2015/12/bigstock-Grizzly-Bear-9532331.jpg', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/washington-grizzly-bear-og.jpg', 'https://www.expeditionsalaska.com/wp-content/uploads/2016/09/09_SEP3502.jpg', 'https://www.shared.com/content/images/2017/08/Ordelheide_20160803-_D5A7701_bearfamily.jpg', 'https://detroitzoo.org/wp-content/uploads/2015/08/Grizzly-Boys.jpg', 'https://filson-life.imgix.net/2020/03/grizzly-bear-reintroduction-1.jpg?fit=scale&fm=pjpg&h=573&ixlib=php-1.2.1&w=860&wpsize=entry-main', 'https://cdn0.wideopenspaces.com/wp-content/uploads/2020/04/whatdogrizzlybearseat4-scaled.jpg', 'https://s3-assets.eastidahonews.com/wp-content/uploads/2019/05/06175149/grizzly-bear-860x571.jpg', 'https://cdn.s3-media.wbal.com/Media/2017/03/22/ac89e9aa-1e3f-4f88-bef3-aeba490d0059/original.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/GrizzlyBearJeanBeaufort.jpg/1200px-GrizzlyBearJeanBeaufort.jpg', 'https://gray-kmvt-prod.cdn.arcpublishing.com/resizer/XDqt0Wh9pw0QyVfKY5V7d77a44o=/1200x675/smart/cloudfront-us-east-1.images.arcpublishing.com/gray/XDDLIRAFIRMF7KDSS56OJSWEIQ.jpg', 'https://miro.medium.com/max/9504/1*1cfuPs8clxIj8f_CDH4sYw.jpeg', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/Idaho-grizzly-bear-hunt-approved-og.jpg', 'https://bloximages.chicago2.vip.townnews.com/ravallirepublic.com/content/tncms/assets/v3/editorial/9/05/90555037-3848-5130-a254-aca92323e46f/5c87f956c8b30.image.jpg?resize=1200%2C755', 'https://media.spokesman.com/photos/2017/11/30/Yellowstone_Grizzlies.JPG.jpg', 'https://www.wpxi.com/resizer/e8abHzJalKpN_CfzbIS07FZBOig=/1200x628/d1hfln2sfez66z.cloudfront.net/09-24-2020/t_82d50577f56b4e2fa00191d8f33c12fe_name_Moose_hunter_mauled__killed_by_grizzly_bear_in_Alaska_Poster.jpg', 'https://bloximages.chicago2.vip.townnews.com/helenair.com/content/tncms/assets/v3/editorial/6/45/645660ec-59b9-58a0-a7d7-213e342af246/5a02181673916.image.jpg', 'https://i2.wp.com/nypost.com/wp-content/uploads/sites/2/2020/09/grizzly-bear-89.jpg?quality=90&strip=all&ssl=1', 'https://www.grandviewoutdoors.com/uploads/images/_facebook/Grizzly.jpg', 'https://i1.wp.com/nypost.com/wp-content/uploads/sites/2/2020/05/grizzly-bear-57.jpg?quality=90&strip=all&ssl=1', 'https://images.fineartamerica.com/images-medium-large-5/grizzly-bear-with-yearling-cub-matthias-breiter.jpg', 'https://davidsuzuki.org/wp-content/uploads/2017/04/grizzly-bears-alaska-wildlife-sanctuary-whittier.jpg', 'https://www.campbellriver.travel/media/2020/02/bear-ears1.jpg', 'https://3.bp.blogspot.com/-ERmaouEbCOw/ThYPiIH9ACI/AAAAAAAAB4E/AzbPHNqSMxE/s1600/Feeling_Grizzly-1600x1200.jpg', 'http://www.famouscutouts.com/images/detailed/0/675-Bear.jpg', 'https://cdn.apartmenttherapy.info/image/upload/f_auto,q_auto:eco,c_fill,g_auto,w_1500/at/news-culture/2019-11/grizzly-bear', 'http://1.bp.blogspot.com/-wzfvrOdkAao/USOjE-6mdDI/AAAAAAAAFAg/GRopRcG-p1k/s1600/Grizzly_Bear_Info_Images+01.jpg', 'http://static.independent.co.uk/s3fs-public/thumbnails/image/2016/06/30/08/yellowstone-grizzly-bear-.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/5004711979_d7bbce11c4_o.jpg', 'https://www.tsln.com/wp-content/uploads/2018/10/bears-tsln-101318-1-1240x823.jpg', 'https://i1.wp.com/www.animalprotectionparty.ca/wp-content/uploads/2017/10/grizzly-bear-report.jpg?fit=1072%2C712&ssl=1', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/grizzly-bear-og_0.jpg', 'http://i.huffpost.com/gen/1390652/images/o-GRIZZLY-BEAR-ROAR-facebook.jpg', 'https://wallsdesk.com/wp-content/uploads/2017/01/Grizzly-Bear-HD.jpg', 'https://thenarwhal.ca/wp-content/uploads/2018/04/BC-grizzly-bear.jpg', 'https://www.vmcdn.ca/f/files/via/import/2018/07/08143100_grizzly-bear.jpg;w=960', 'https://wyo4news.com/wp-content/uploads/2020/07/grizzly-bear.jpg', 'https://cdn.shopify.com/s/files/1/1520/0394/products/Grizzly_bear.jpg?v=1541529305', 'https://bozone.com/site/wp-content/uploads/2019/03/GRIZZLY.jpeg', 'http://www.news1130.com/wp-content/blogs.dir/sites/9/2015/09/17/iStock_000010656016_Double-e1442507974878.jpg', 'http://static-21.sinclairstoryline.com/resources/media/6078265b-815a-4172-a17f-67f94d69e53c-large16x9_ImportedfromLakana.jpg?1514431909725', 'https://4.bp.blogspot.com/-HNxOANqzIwQ/Un_rDbUxCGI/AAAAAAAABUc/CpX9FRsAdLA/s1600/Grizzly_Bear_2.jpg', 'https://cdn4.creativecirclemedia.com/powell/original/20190926-083657-Griz.jpg', 'https://tetonvalleylodge.com/wp-content/uploads/2015/04/grizzly-bear-idaho.jpg', 'https://idahobusinessreview.com/files/2018/08/grizzly-bear-scaled.jpg', 'https://cdn0.wideopenspaces.com/wp-content/uploads/2017/08/grizzly.jpg', 'https://holidayarchitects.co.uk/canada-holidays/wp-content/uploads/sites/7/Grizzly-bear-3-CRWW.jpg', 'https://imgc.allpostersimages.com/img/print/u-g-PZLADY0.jpg?w=338&h=450', 'http://1.bp.blogspot.com/-422rK0Kje1E/UQW3yELFQeI/AAAAAAAAEzs/Rcg4Gniz4rU/s1600/Grizzly-Bear-2.jpg', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/Grizzly-bear-protections-west-OG.jpg', 'https://cdnph.upi.com/sv/ph/og/upi/9261403133345/2014/1/f6484c26a086306575f2eabf184d2d11/v1.5/Conservationists-push-to-triple-Americas-grizzly-bear-population.jpg', 'https://www.the-sun.com/wp-content/uploads/sites/6/2020/09/NINTCHDBPICT000512282491.jpg?strip=all&quality=100&w=1200&h=800&crop=1', 'https://www.wyofile.com/wp-content/uploads/2013/09/blondie-the-griz2_Jackie-Skaggs_June-2011.jpg']\n    Images downloaded.\nblack bear: Got 278 image URLs. Downloading...\n   urls =  ['https://scx2.b-cdn.net/gfx/news/hires/2019/blackbear.jpg', 'https://www.seacoastonline.com/storyimage/SO/20190730/NEWS/190739992/AR/0/AR-190739992.jpg', 'https://www.theintell.com/storyimage/PA/20190610/NEWS/190619908/AR/0/AR-190619908.jpg', 'https://www.enr.gov.nt.ca/sites/enr/files/black_bear_banner.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/02/american-black-bear-2.jpg', 'https://www.gannett-cdn.com/-mm-/870435c5930644f2716846ecf8ba7354bdf4dead/c=0-414-862-901/local/-/media/2018/07/09/Westchester/Westchester/636667658911415885-Armonk-bear-3.jpg?width=3200&height=1680&fit=crop', 'https://wallup.net/wp-content/uploads/2019/09/719345-baribal-black-bear-bear-muzzle-eyes-predator.jpg', 'https://www.visitmt.com/binaries/content/gallery/MTOT/responsive/hero-f/dec-2014/blackbear_subjectmatterlandinghero.jpg', 'https://d3i3l3kraiqpym.cloudfront.net/wp-content/uploads/2016/05/26232920/12079543_847437148710024_685610861999970941_n.jpg', 'http://www.adirondackexplorer.org/wp-content/uploads/2016/12/17-Black-bear-Raquette.jpg', 'https://static1.pointsinfocus.com/2017/12/black-bears-of-revillagigedo-island-ketchikan-alaska/Fat-Bear.jpg', 'https://outsider.com/wp-content/uploads/2020/10/black-bear-encounter-how-to-prevent-survive-attack.jpg', 'https://nypost.com/wp-content/uploads/sites/2/2014/10/black-bear-cub.jpg?quality=90&strip=all&w=1200', 'http://www.stgeorgeutah.com/wp-content/uploads/2015/11/lynn_11-22-2011_black_bear_1.jpg', 'http://bearsmartdurango.org/wp-content/uploads/2013/06/100_0642.jpg', 'https://alaskashoreexcursions.com/media/ecom/prodxl/Baby-Black-Bear.JPG', 'https://www.fascinationwildlife.com/wp-content/uploads/2018/11/TON6447_DxO.jpg', 'http://www.canadianrockies.net/wp-content/uploads/2012/03/blb0389_bigblackbear.jpg', 'https://i.cbc.ca/1.3267259.1444652249!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_620/black-bear.jpg', 'https://binderparkzoo.org/wp-content/uploads/2017/10/black-bear-5-1024x683.jpg', 'http://www.camelcitydispatch.com/wp-content/uploads/2013/05/black-bear-stting-face-on.jpg', 'https://www.huntingnewfoundlandlabrador.com/-/media/marquees/hunting/hunting-species/black-bear/black-bear-main-header.jpg?mh=960&mw=1280&hash=935A53A530E3F07AFB9DC4FF03D08C8B103FD601', 'https://www.gofingerlakes.org/wp-content/uploads/Ursus_americanus_Simon-Pierre-Barrette.CC-BY-SA-004-1.jpg', 'http://elelur.com/data_images/mammals/asian-black-bear/asian-black-bear-04.jpg', 'https://www.cullmantribune.com/wp-content/uploads/2019/08/Black-bear-mother-and-cubs-1024x835.jpg', 'http://3.bp.blogspot.com/_1N1pZVb1_C4/TUCDSYH2XuI/AAAAAAAABU4/N-8QVjyjpiA/s1600/bear_black.jpg', 'http://naturalbridgezoo.com/wp-content/uploads/2017/05/BB1-1.jpg', 'https://live.staticflickr.com/3223/2762479597_07c4b2f908_b.jpg', 'http://grizzlybearwatching.com/wp-content/uploads/2017/03/BlackBear1-1500x998.jpg', 'https://en.reepark.dk/media/1613/_n011753.jpg?width=1024', 'https://render.fineartamerica.com/images/rendered/default/print/8.000/7.000/break/images/artworkimages/medium/2/bear-stare-jerry-lofaro.jpg', 'http://cdn-image.travelandleisure.com/sites/default/files/styles/1600x1000/public/1522685410/black-bear-BEAR418.jpg?itok=qmoXuFRx', 'https://exploreoakridge.com/wp-content/uploads/2019/10/American-Black-Bear-photo-by-Chris-Norcott.jpg', 'https://cottagelife.com/wp-content/uploads/2017/07/shutterstock_503744119.jpg', 'https://i0.wp.com/imagineourflorida.org/wp-content/uploads/2016/09/Florida_Black_Bear_square_Kon_Studio.jpg?fit=800%2C800&ssl=1', 'https://2.bp.blogspot.com/-bMCoR7-d0SU/TmstJPh8sOI/AAAAAAAAAPo/4J5H-Mm2qEM/s1600/black-bear-wallpaper-11-755660.jpg', 'http://infotel.ca/news/medialibrary/image/orig-mediaitemid22167-3862.jpg', 'http://www.sott.net/image/s13/261520/full/Black_bear.jpg', 'https://downeast.com/wp-content/uploads/2014/02/Bear4.jpg', 'https://visitadirondacks.com/sites/default/files/images/adirondack-black-bear.jpg', 'https://www.vpr.org/sites/vpr/files/styles/x_large/public/202007/black-bear-fish-wildlife-dept-Tom-Rogers-2013.png', 'http://www.mountaincountrycabinrentals.com/wp-content/uploads/2014/10/Closeup-of-a-black-bear.jpg', 'https://nypost.com/wp-content/uploads/sites/2/2020/08/black-bear-81.jpg?quality=90&strip=all&w=1200', 'https://bear.org/wp-content/uploads/2008/02/20130607_Bow-copy.jpg', 'https://st.focusedcollection.com/14026668/i/1800/focused_181901682-stock-photo-american-black-bear-ursus-americanus.jpg', 'https://www.gulflive.com/resizer/XxMMGfEvOQQAVOEKfieknpQOIv8=/1280x0/smart/advancelocal-adapter-image-uploads.s3.amazonaws.com/image.gulflive.com/home/gulf-media/width2048/img/news_impact/photo/bears-threatened-fbdfb579fa67c166.jpg', 'https://wildlife.utah.gov/news_photos/11-18-19_black-bear-in-grass.jpg', 'https://www.piecebypiece.ca/wp-content/uploads/2018/05/165-Black-Bear-Cub.jpg', 'https://www.hdwallpapers.in/download/black_bear_hd_animals-HD.jpg', 'https://www.greatswamp.org/wp-content/uploads/2016/01/black-bear.jpg', 'http://getwallpapers.com/wallpaper/full/a/8/4/1452654-wallpaper-black-bear-pictures-2560x1600-for-4k.jpg', 'https://beartrackoutfitters.com/wp-content/uploads/2018/09/bear.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/02/american-black-bear-smell.jpg', 'http://upload.wikimedia.org/wikipedia/commons/1/18/Black_Bear_7.jpg', 'http://snowbrains.com/wp-content/uploads/2015/08/American-Black-Bear-jpg.jpg', 'https://vignette.wikia.nocookie.net/parody/images/5/5a/11775722235_19bb44c6e7_b.jpg/revision/latest?cb=20161008190514', 'https://www.wildlifecenter.org/sites/default/files/patient_images/DSCN2925.JPG', 'https://media-cdn.wehco.com/img/photos/2020/12/11/blackbearnoppers06cmykoriginal1483610356_t1070_h5b279ad6a60bb8d6261090d0367921ea9d3e0cc8.jpg', 'https://www.gannett-cdn.com/-mm-/ba9293e2ff57f90c5ea896bbb7fb36126f38dbc9/c=0-134-3468-2093/local/-/media/2016/06/28/CNYGroup/Elmira/636027114617396897-black-bear.jpg?width=3200&height=1680&fit=crop', 'https://bearexpert.com/wp-content/uploads/2019/10/american-black-bear.jpg', 'https://cdn2.outdoorphotographer.com/gallery/7522/leahed_100307_589944_290c530a74_28f7de1b95-leahed-dsc_9774.jpg', 'https://onncg8dr7k-flywheel.netdna-ssl.com/wp-content/uploads/2020/10/GettyImages-1195788516.jpg', 'https://www.gannett-cdn.com/-mm-/cd26c8713e884a8483a3eb4efd7e9b47f526e46d/c=0-264-4620-2863/local/-/media/Bridgewater/2015/01/05/B9315749966Z.1_20150105153333_000_GUS9JJHMR.1-0.jpg?width=3200&height=1680&fit=crop', 'https://www.mlive.com/resizer/Klh1maBtQ2BXs9MJ79W4mVfcL8k=/1280x0/smart/advancelocal-adapter-image-uploads.s3.amazonaws.com/image.mlive.com/home/mlive-media/width2048/img/outdoors_impact/photo/black-bears-new-exhibit-at-binder-park-zoo-07-a88a34fbe0690fea.jpg', 'https://texnat.tamu.edu/files/2018/07/IMG_1573.jpg', 'https://photos1.blogger.com/blogger/5712/2802/1600/DSC01502.jpg', 'https://bloximages.newyork1.vip.townnews.com/pilotonline.com/content/tncms/assets/v3/editorial/e/d5/ed52c12b-4e68-5024-a886-f0fc147bf94e/563d906095138.image.jpg', 'https://img1-azrcdn.newser.com/image/1255962-12-20190904134100.jpeg', 'https://www.gravel.org/wp-content/uploads/2015/10/Black-Bear.jpg', 'https://3.bp.blogspot.com/-nFt10gfJrzM/UDKIjeW21LI/AAAAAAAADfA/uomt-SjA-Sc/s1600/IMG_5635.jpg', 'https://www.crozetgazette.com/wp-content/uploads/2018/06/iStock-153562011.jpg', 'https://images.thestar.com/-S-RKsYxNaEcCX0LRaqhNeLpMAs=/1200x932/smart/filters:cb(1565190981014)/https://www.thestar.com/content/dam/thestar/vancouver/2019/08/06/child-injured-by-bear-bite-at-greater-vancouver-zoo-officials-say/blackbear.jpg', 'https://socalguidedhunts.com/wp-content/uploads/2013/09/guided-black-bear-hunts.jpg', 'https://seniors101.ca/wp-content/uploads/2015/08/Black-bear-cub.jpg', 'https://www.koamnewsnow.com/content/uploads/2020/05/pixabay-black-bear.jpg', 'https://www.gannett-cdn.com/-mm-/40756a948d5a0edcc7ce6d0e16b133540995c9c8/c=0-138-1997-1266/local/-/media/2017/03/06/JacksonMS/JacksonMS/636243918786987260-0307bear01.jpg?width=3200&height=1680&fit=crop', 'https://www.wallpaperflare.com/static/805/363/779/bear-black-bear-forest-black-wallpaper.jpg', 'https://img.huffingtonpost.com/asset/5d71a56a2500007a1205d0b4.jpeg?cache=i3bilfyydx&ops=1778_1000', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/one-big-black-bear-john-rowe.jpg', 'http://i.huffpost.com/gen/1105509/images/o-MARYLAND-BLACK-BEAR-facebook.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/02/black-bear.jpg', 'https://blog.humanesociety.org/wp-content/uploads/2016/06/BLACK-BEAR-ISTOCK_000046186676_288801-1220x813.jpg', 'https://aldf.org/wp-content/uploads/2018/05/blackbear-121274561-16x9.jpg', 'https://www.timberon.org/wp-content/uploads/Young_black_bear.jpg', 'https://www.outsideonline.com/sites/default/files/styles/img_600x600/public/2019/06/28/black-bear-grazes_s.jpg?itok=ZaqoTZKg', 'https://a-z-animals.com/media/Asiatic-black-bear.jpg', 'https://www.worldlandtrust.org/wp-content/uploads/2019/07/youung-black-bear.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/bear-family-timothy-flanigan.jpg', 'https://nhpbs.org/wild/images/blackbearusfw2.jpg', 'https://a57.foxnews.com/a57.foxnews.com/static.foxnews.com/foxnews.com/content/uploads/2018/09/640/320/1862/1048/Black20Bear20iStock.jpg?ve=1&tl=1?ve=1&tl=1', 'https://www.gannett-cdn.com/presto/2019/05/08/PASH/9be97cd2-c85d-4eb6-a503-68b17b29ce94-NC_Bear_study_NO27_resting_after_work_up.jpg?crop=639,359,x0,y23&width=3200&height=1680&fit=bounds', 'https://abdnews.org/wp-content/uploads/2017/04/Authorities-Rescue-Black-Bear-Cubs-After-Mother-Hit-By-Truck.jpg', 'http://blog.healthywildlife.ca/wp-content/uploads/2016/07/black-bear-hg-1-of-1.jpg', 'https://www.vmcdn.ca/f/files/via/import/2019/10/01132841_black-bear.jpg;w=700;h=347;mode=crop', 'https://advancelocal-adapter-image-uploads.s3.amazonaws.com/image.pennlive.com/home/penn-media/width2048/img/pa-sportsman/photo/23770296-standard.jpg', 'http://wildlife.org/wp-content/uploads/2015/01/Florida-Blackbear-via-Flickr-credit-Rich-Turner-940.jpg', 'http://snowbrains.com/wp-content/uploads/2015/08/connie-lemperle-black-bear-web.jpg', 'https://detroit.cbslocal.com/wp-content/uploads/sites/15909782/2015/05/black-bear-forest.jpg?w=1500', 'https://www.gannett-cdn.com/-mm-/4f6c5cb86bdd5d6da988b998c98d5233eb4312d0/c=0-250-4921-3030/local/-/media/2017/12/30/Tallahassee/Tallahassee/636502285489982021-Black-Bears.jpg?width=3200&height=1680&fit=crop', 'https://www.poconorecord.com/storyimage/PR/20161114/NEWS/161119826/AR/0/AR-161119826.jpg', 'https://floridablackbearscenicbyway.org/wp-content/uploads/2020/02/2411420-1920x1080-DesktopNexus.com_.jpg', 'http://www.wildernesscollege.com/images/blkbr-face.jpg', 'https://www.trbimg.com/img-557b8359/turbine/ct-indiana-black-bear-20150612', 'http://wiseaboutbears.org/wp-content/uploads/2014/04/img034.jpg', 'https://img.thrfun.com/img/214/282/bears_tx3.jpg', 'http://www.sciencebuzz.org/sites/default/files/images/Black_bear.jpg', 'http://i.huffpost.com/gen/1669786/images/o-BLACK-BEAR-CANADA-facebook.jpg', 'https://images.wallpaperscraft.com/image/bear_black_bear_forest_113045_800x1200.jpg', 'https://storage.googleapis.com/afs-prod/media/media:c1a9aa6b7050465d83977305953495aa/3000.jpeg', 'https://www.wildnatureimages.com/images/xl/140729-016-Black-Bear-at-Anan-Wildlife-Observatory.jpg', 'https://brevardzoo.org/wp-content/uploads/2019/05/bearsidebar.jpg', 'https://canadaintherough.com/wp-content/uploads/2020/06/blackbear.jpg', 'https://i1.wp.com/albertawild.com/wp-content/uploads/2017/02/bear-1698.jpg?fit=1698%2C1132', 'http://mediad.publicbroadcasting.net/p/upr/files/styles/x_large/public/201612/black-bear.jpg', 'http://www.encyclopediaofalabama.org/images/m-9033.jpg', 'http://bloximages.newyork1.vip.townnews.com/pilotonline.com/content/tncms/assets/v3/editorial/8/47/84743c51-975b-555f-9f30-1234790e97a8/563d905fe2c27.image.jpg?resize=1200%2C852', 'http://bearlegend.com/wp-content/uploads/2012/04/2641024724_d12e4afcb9_o-361.jpg', 'https://images.dailyhive.com/20190916163711/North-Shore-Black-Bear-Society.jpg', 'https://s29027.pcdn.co/wp-content/uploads/2019/08/Black-Bear-mating-Tekiela-TEK1811.jpg', 'https://someinterestingfacts.net/wp-content/uploads/2016/07/American-black-bear.jpg', 'https://media.gettyimages.com/photos/black-bear-standing-in-rocky-stream-british-columbia-canada-picture-id974154560', 'http://www.toledoblade.com/image/2013/01/21/black-bear.JPG', 'https://www.thehuntingpage.com/wp-content/uploads/sites/2/2014/01/o-BLACK-BEAR-facebook.jpg', 'https://cdn-image.travelandleisure.com/sites/default/files/styles/1600x1000/public/1572373216/black-bear-tennessee-BEARLODGE1019.jpg?itok=jO4eurBD', 'http://4.bp.blogspot.com/-YS1MBBAHGPs/TjkTZIS8WGI/AAAAAAAABi0/alssUhDH3Pk/s1600/Baby+black+bears+3.jpg', 'https://www.transylvaniatimes.com/home/cms_data/dfault/photos/stories/id/9/1/41091/s_top', 'https://www.wsls.com/resizer/fmqqOp8dNrEIIDxQOlsBSRip5KY=/1600x1333/smart/filters:format(jpeg):strip_exif(true):strip_icc(true):no_upscale(true):quality(65)/arc-anglerfish-arc2-prod-gmg.s3.amazonaws.com/public/3VGIKDGQDNAY5I4NO7KP4ZX5N4.jpg', 'https://mediad.publicbroadcasting.net/p/wvtf/files/styles/x_large/public/201607/AP_608334627084.jpg', 'https://gothunts.com/wp-content/uploads/2019/06/Black-Bear-scaled.jpeg', 'http://www.visitmysmokies.com/wp-content/uploads/2013/08/Black-Bear-Cub.jpg', 'https://cdn.audleytravel.com/-/-/79/160033025091139069040022125202060038124167252088.jpg', 'http://2.bp.blogspot.com/-oL5wK6KYI7M/UBji1ikDXXI/AAAAAAAAAMY/jmXUpXVGet4/s1600/Black-Bear-05.jpg', 'https://www.montanaoutdoor.com/wp-content/uploads/2018/05/bear6.jpg', 'http://2.bp.blogspot.com/_1N1pZVb1_C4/TUCDn6EEngI/AAAAAAAABVA/h6BmG9TLADA/s1600/Black+Bear+159087.jpg', 'https://www.oregonlive.com/resizer/nSifR1unuKEMPswzQtebZRI_03g=/600x0/arc-anglerfish-arc2-prod-advancelocal.s3.amazonaws.com/public/6GF72P3X25EXVLEKGSGVXUJRKI.png', 'http://someinterestingfacts.net/wp-content/uploads/2016/07/Black-bear.jpg', 'https://www.remotelands.com/travelogues/app/uploads/2019/07/AsiaWire-BearReturn-2.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/7-maine-black-bear-sharon-fiedler.jpg', 'https://d.ibtimes.co.uk/en/full/1611947/black-bear.jpg', 'https://animalhype.com/wp-content/uploads/2019/12/How-Fast-Do-Black-Bears-Run-1024x682.jpg', 'http://uffmannatureimages.net/wp-content/uploads/2012/07/20120519-_Q2C6428Black-Bears-save-for-web-cs6.jpg', 'https://www.gannett-cdn.com/presto/2019/01/02/PASH/bff6ff36-67d2-44be-bdda-0d5e29cda4e1-Black_bear_Ken_Taylor.jpg?crop=1796,1007,x0,y0&width=3200&height=1680&fit=bounds', 'https://wildlife.org/wp-content/uploads/2015/07/Black-Bear-Reintroduction-Image-1-600.jpg', 'https://imgc.allpostersimages.com/img/print/posters/barrett-hedges-a-large-black-bear-ursus-americanus-moves-quickly-over-the-rocks_a-L-13725186-4990831.jpg', 'http://www.wildernessrealty.com/wp-content/uploads/2015/02/Maine-Black-Bear.jpg', 'http://cdn0.wideopenspaces.com/wp-content/uploads/2014/10/bigstock-Black-bear-cub-62558696.jpg', 'http://archery360.com/wp-content/uploads/2014/06/black-bear-cub.jpg', 'https://www.wideopenspaces.com/wp-content/uploads/2020/09/wisconsinblackbear.jpg', 'https://cdn.extra.ie/wp-content/uploads/2018/05/15124559/Black-bear-1.jpg', 'https://2.bp.blogspot.com/-oMj7lKf-Lb8/UMJTf68A6SI/AAAAAAAABfw/L91dawds-Nk/s1600/black-bear-photos-2012+03.jpg', 'https://i.cbc.ca/1.4130464.1495668985!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/black-bear.jpg', 'http://content.wzzm13.com/photo/2015/10/09/635799990906985114-ThinkstockPhotos-178500326_40932_ver1.0.jpg', 'http://www.my-photo-blog.com/wp-content/uploads/2014/07/Black-Bear-Cub.jpg', 'https://www.massaudubon.org/var/ezdemo_site/storage/images/media/departments/lww/mammals/images/black-bear/black-bear-standing-up-next-to-tree-c-karen-karlberg-500vert/196541-4-eng-US/black-bear-standing-up-next-to-tree-c-karen-karlberg-500vert_blockitemmedium.jpg', 'https://www.grandviewoutdoors.com/uploads/images/BlackBearUSFWS_181004_111434.jpg', 'https://mediad.publicbroadcasting.net/p/wusf/files/styles/x_large/public/201504/black_bear_closeup_gary_langley.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/6-american-black-bear-cub-david-kenny.jpg', 'https://www.coniferousforest.com/wp-content/uploads/2016/08/American-Black-Bear.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/10/asiatic-black-bear-5.jpg', 'https://www.gannett-cdn.com/presto/2019/12/30/PKNS/e7f38dd1-a441-4cc3-a48a-1df9445344de-KNS-Zoo_Knoxville_Bears_BP_3.JPG?width=540&height=&fit=bounds&auto=webp', 'https://miro.medium.com/max/6014/1*sGT7ccEjG6KF0-UjdsJ3nQ.jpeg', 'https://wildlife.org/wp-content/uploads/2017/03/blackbear.png', 'https://bearexpert.com/wp-content/uploads/2020/08/american-black-bear-6.jpg', 'http://cdn0.wideopenspaces.com/wp-content/uploads/2015/12/BlackBearMass.jpg', 'http://dnr.maryland.gov/wildlife/PublishingImages/Maryland-Bear-Stamp-Winning-Artwork-19-20.jpg', 'http://www.thestar.com/content/dam/thestar/news/gta/2015/06/01/black-bear-cornered-in-newmarket-backyard/bear-file.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/2015-08-07_Black_Bear_near_Stewart%2C_BC_1090.jpg/1280px-2015-08-07_Black_Bear_near_Stewart%2C_BC_1090.jpg', 'http://www.adirondackalmanack.com/wp-content/uploads/2017/05/blackbear.jpg', 'https://www.coniferousforest.com/wp-content/uploads/2016/08/American-Black-Bear-Cubs.jpg', 'http://strangesounds.org/wp-content/uploads/2014/08/bear-cubs-photo.jpg', 'https://bear.org/wp-content/uploads/2017/10/Serious-Bear.jpg', 'https://thelaurelofasheville.com/wp-content/uploads/2016/06/Carolina_Nature_Photographers_Association-Black_Bear-6.jpg', 'https://yukon.ca/sites/yukon.ca/files/env/env-imgs/env-black-bear_0.jpg', 'https://www.victoriabuzz.com/wp-content/uploads/2019/08/black-bear-unsplash.jpg', 'https://media.spokesman.com/photos/2017/07/04/BLACK_BEAR.JPG.jpg', 'http://yesofcorsa.com/wp-content/uploads/2018/10/American-Black-Bear-Best-Wallpaper-1024x753.jpg', 'https://wallup.net/wp-content/uploads/2019/09/300546-minnesota-black-bear.jpg', 'https://cdn1.theweek.co.uk/sites/theweek/files/2016/10/161026_black_bear.jpg', 'https://bloximages.newyork1.vip.townnews.com/swnewsmedia.com/content/tncms/assets/v3/editorial/7/1a/71a9b7ca-1825-5fdd-9534-5a7c30ac10c8/5d444ec6ac2d5.image.jpg', 'https://quincy-network.s3.ca-central-1.amazonaws.com/wp-content/uploads/sites/37/2020/06/MGN_1280x960_70623P00-TYOFD-860x645.jpg', 'http://www.karelianbeardog.us/photos/outdoor/blackbears/Black%20bear.jpg', 'https://brevardzoo.org/wp-content/uploads/2019/05/bearmain.gif', 'http://justfunfacts.com/wp-content/uploads/2017/10/asiatic-black-bear-2.jpg', 'http://ovlc.org/wp-content/uploads/2014/07/black_bear.jpg', 'https://cdn0.wideopenspaces.com/wp-content/uploads/2017/02/black-bear-feature.jpg', 'https://nhpbs.org/wild/images/blackbearusfw1.jpg', 'http://uffmannatureimages.net/wp-content/uploads/2012/07/20120518-KQ2C5610Black_Bears-Edit-692x1024.jpg', 'https://www.raincoast.org/wp-content/uploads/2019/12/black-bear-diane-bb-v1.1.jpg', 'https://gohunt-assets-us-west-2.s3.amazonaws.com/media/Large-Montana-black-bear.jpg', 'http://www.hww.ca/kaboom/images/Mammals/Black-Bear/Black-Bear-David-Cracknell.jpg', 'http://2.bp.blogspot.com/-yc7X_uO7iMM/UA7U_q5_NtI/AAAAAAAAAqI/72EaqVglzPM/s1600/black-bear.jpg', 'https://d.newsweek.com/en/full/1092319/black-bear.jpg', 'https://www.grandviewoutdoors.com/uploads/images/_facebook/Black-Bear-Sweepstakes-beauty.jpg', 'http://3.bp.blogspot.com/-vwLa2VwfXg4/Ul6IwR1sIJI/AAAAAAAABYw/INnD5gQH-dM/s1600/Black-Bear.jpg', 'http://wyrz.org/wp-content/uploads/2016/07/black_bear.jpg', 'https://www.gannett-cdn.com/-mm-/98e9eb0055148d8550b3a7d264f19fd6462e6062/c=0-213-4288-2636/local/-/media/2017/03/22/CarolinaGroup/Asheville/636257813777562324-Black-bear-Getty-images.jpg?width=3200&height=1680&fit=crop', 'https://floridawildlifefederation.org/wp-content/uploads/2020/04/black-bear-scaled-e1598566846480.jpg', 'https://www.stgeorgeutah.com/wp-content/uploads/2015/01/lynn_11-22-2011_black_bear_6-1.jpg', 'https://bento.cdn.pbs.org/hostedbento-prod/gallery/20170713_222850_315402black-bear-01-gettyimages-155467721.jpg.1280x720_q85.jpg', 'https://dehayf5mhw1h7.cloudfront.net/wp-content/uploads/sites/301/2020/06/26101300/bear-illinois-dnr.jpg', 'http://2.bp.blogspot.com/-MKgsmEatcvk/UKuf-CiWdFI/AAAAAAAACQU/VPvHckGI7DA/s1600/Black_bear_large.jpg', 'https://kanati.com/wp-content/uploads/black-bear-life-size-mount_poseBB109-feature.png', 'https://compote.slate.com/images/0a308e2c-48e2-476a-ad72-61c20aa92709.jpg', 'https://external-preview.redd.it/ieT9RTLmwvN4hfQBYix4109CLYniR3fSUAeT6jg6kO0.jpg?auto=webp&s=4f2ec1a52f7c1f5b16238503377981645c54ae27', 'http://wallpapersdsc.net/wp-content/uploads/2017/10/Black-Bear-Background.jpg', 'https://www.gannett-cdn.com/-mm-/2b06f05d8e31b4112ec2ecbca6e9ef0db0cf4d0b/c=0-108-2125-1309/local/-/media/2016/05/18/Wilmington/Wilmington/635991906230694281-black-bear.jpg?width=3200&height=1680&fit=crop', 'https://i.ytimg.com/vi/-2cj5pPe778/maxresdefault.jpg', 'https://www.outdoorrevival.com/wp-content/uploads/2017/04/american-black-bear.jpg', 'https://www.pennlive.com/resizer/zKZdn6cmyoUATjOlNY4O_OEWRTI=/1280x0/smart/advancelocal-adapter-image-uploads.s3.amazonaws.com/image.pennlive.com/home/penn-media/width2048/img/pa-sportsman/photo/black-bear-8789aeebd5c67247.jpg', 'http://www.hww.ca/kaboom/images/Mammals/Black-Bear/Black-Bear-Elliott-Fotherby.jpg', 'http://www.hdnicewallpapers.com/Walls/Big/Bear/Animal_Black_Bear_Wallpaper.jpg', 'https://biologydictionary.net/wp-content/uploads/2020/09/shutterstock_1452356795-1.jpg', 'http://theazaleainnbb.com/wp-content/uploads/2018/07/black-bear.jpg', 'https://wallpapercave.com/wp/wp3102474.jpg', 'https://www.simplemost.com/wp-content/uploads/2019/06/AdobeStock_122783246.jpeg', 'https://onekindplanet.org/wp-content/uploads/2017/09/blackbear.jpg', 'https://images.fineartamerica.com/images-medium-large-5/baby-black-bear-jack-nevitt.jpg', 'http://www.wildlifecenter.org/sites/default/files/patient_images/Black_Bear/DSCN6285.JPG', 'https://audubonnatureinstitute.org/images/600x450/zoo/black-bear-600.jpg', 'https://bear.org/wp-content/uploads/2008/01/Bear-by-lake-looking.jpg', 'https://cottagelife.com/wp-content/uploads/2019/02/Cottage-Life-Black-Bear-cub.jpg', 'https://images.axios.com/WACso7e7dbhjup_i7iVfclq3BdI=/0x322:4288x2734/1920x1080/2018/05/22/1527002317479.jpg', 'https://northdakotawildlifefederation.org/wp-content/uploads/2020/02/black-bear.jpg', 'https://a57.foxnews.com/a57.foxnews.com/static.foxnews.com/foxnews.com/content/uploads/2018/11/640/320/1862/1048/louisiana-black-bear.jpg?ve=1&tl=1?ve=1&tl=1', 'https://www.montanaoutdoor.com/wp-content/uploads/2015/05/bears_desktop_2700x1809_hd-wallpaper-832781.jpg', 'https://www.stgeorgeutah.com/wp-content/uploads/2017/01/blackbear.jpg', 'https://fthmb.tqn.com/OUfFPaVmyC8KJnjYhPqr-aq-Sy4=/4896x3264/filters:fill(auto,1)/tired-black-bear-cub-537071213-59bda526845b340011591e7f.jpg', 'https://cdn2.outdoorphotographer.com/gallery/7522/thegagglephotog@gmail.com_102543_647246_d57a3bd609_1c886941c4-thegagglephotoggmail-com-baby-black-bear-standing-fb.jpg', 'http://images.dailyhive.com/20160826144034/black-bear-e1475078273412.jpg', 'http://animalia.bio/uploads/animals/photos/full/1.25x1/YCezTGiM824aJwMLQgqJ.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/10/asiatic-black-bear-eating.jpg', 'https://images.fineartamerica.com/images-medium-large-5/standing-bear-timothy-flanigan.jpg', 'https://www.trbimg.com/img-58eeaeb6/turbine/os-black-bear-dead-sanford-criminal-20170412', 'https://winchestersun.com/wp-content/uploads/sites/44/2020/06/A1-062320-Black-bear-e1593005720644.jpg', 'https://i0.wp.com/taiwan-scene.com/wp-content/uploads/2019/06/taiwan-hualien-formosan-black-bear.jpg?fit=870%2C654&ssl=1', 'https://www.gannett-cdn.com/presto/2018/11/04/PGRF/7c692a2a-f1d1-4f07-8181-6bfc90c42b2c-AP18305732083625.jpg?crop=1999,1144,x0,y211&width=3200&height=1680&fit=bounds', 'http://i.huffpost.com/gen/1190583/thumbs/o-BLACK-BEAR-facebook.jpg', 'http://1.bp.blogspot.com/-Jc2_QqcDTz4/UBjix7v9rjI/AAAAAAAAAMA/Hr67qTiw_I4/s1600/Black-Bear-02.jpg', 'http://mediad.publicbroadcasting.net/p/nhpr/files/styles/x_large/public/201303/April26girlapril30.jpg', 'http://mediad.publicbroadcasting.net/p/nhpr/files/201303/April25onlog.jpg', 'http://www.exploreclarion.com/wp-content/uploads/2018/11/black-bear.png', 'https://external-preview.redd.it/ny2anVEkjB6eTawYfAN6KcZ7TUyH69buX9Vhm56cds8.jpg?auto=webp&s=500e3a52f7e797eff002fcd91f0f3cf62ff1f401', 'https://img.huffingtonpost.com/asset/594a9e981700001f001021dc.jpeg?ops=1910_1000', 'https://site-547756.mozfiles.com/files/547756/medium/Black_Bear3.jpg', 'https://images.fineartamerica.com/images-medium-large/pa-black-bear-portrait-bill-maile-.jpg', 'https://www.oceanlight.com/stock-photo/ursus-americanus-black-bear-image-18790-351363.jpg', 'https://www.gannett-cdn.com/-mm-/a5076e7a43a0cec6129489319d0fb728e2cd1814/c=0-264-5184-3193/local/-/media/2017/02/15/DetroitFreePress/DetroitFreePress/636227474186999380-GettyImages-497031868.jpg?width=3200&height=1680&fit=crop', 'https://3.bp.blogspot.com/-T62hFwmwBSg/TmstHyDCjWI/AAAAAAAAAAg/hZSRZIqaEpE/s1600/black-bear-backgrounds-4-751651.jpg', 'https://media.news4jax.com/photo/2015/11/06/Black-bear-blurb-jpg_308110_ver1.0_1280_720.jpg', 'https://downeast.com/wp-content/uploads/2014/02/Bear9.jpg', 'http://www.minnesotaseasons.com/Mammals/Large/American_black_bear_03.jpg', 'https://animals.net/wp-content/uploads/2018/12/Black-Bear-2.jpg', 'https://i.cbc.ca/1.4656566.1525955305!/cpImage/httpImage/image.jpg_gen/derivatives/16x9_780/alberta-black-bear.jpg', 'http://www.hdnicewallpapers.com/Walls/Big/Bear/Animal_Black_Bear_in_Forest.jpg', 'http://2.bp.blogspot.com/-NU71JkUr_MU/UNois4w5TnI/AAAAAAAABK4/ul6piYtCZUU/s1600/christmas+bear.JPG', 'http://www.trbimg.com/img-52bde9f3/turbine/os-blackbearhabitat9910-jpg-20131227/2000/2000x1182', 'http://elelur.com/data_images/mammals/asian-black-bear/asian-black-bear-05.jpg', 'https://globaljusticeecology.org/wp-content/uploads/black-bear-937037_1920-980x460.png', 'http://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/embed/public/2017/05/16/black-bears-19723071920.jpg', 'http://www.nwtrek.org/wp-content/uploads/2017/12/KBYG-black-bear.jpg', 'https://wildlifefriendly.org/wp-content/uploads/2015/09/Black-bear-Full.jpg', 'https://www.sott.net/image/s13/270500/full/Black_bear_looks_angry_Shutter.jpg', 'http://wiseaboutbears.org/wp-content/uploads/2014/06/bearmaleJune-222012D80_2149-copy.jpg', 'https://www.columbiatribune.com/storyimage/MO/20200519/NEWS/200519240/AR/0/AR-200519240.jpg', 'https://cdn2.bigcommerce.com/server100/6ca92/product_images/uploaded_images/florida-black-bear.jpg?t=1430242234', 'http://explorenorth.com/library/nature/images/brown_black_bear-tutshi_lake-0709.jpg', 'https://cdn.shopify.com/s/files/1/1603/1175/products/American_Black_Bear_Cub_3265_2048x.jpg?v=1523665570', 'http://getwallpapers.com/wallpaper/full/a/0/7/1303653-black-bear-desktop-wallpaper-1920x1080-for-meizu.jpg', 'https://farm6.staticflickr.com/5588/15080045718_78c2561e6e_b.jpg', 'https://www.nps.gov/shen/learn/nature/images/BlackBear_6.jpg?maxwidth=1200&autorotate=false', 'https://www.gannett-cdn.com/-mm-/a5076e7a43a0cec6129489319d0fb728e2cd1814/c=0-264-5184-3193/local/-/media/2017/06/23/PAGroup/YorkDailyRecord/636338199651309082-GettyImages-482466374.jpg?width=3200&height=1680&fit=crop', 'http://www.reportingday.com/wp-content/uploads/2018/07/Black-Bear-Cute-Pictures-1.png', 'https://upload.wikimedia.org/wikipedia/commons/9/9c/%22Cinnamon%22_Black_Bear.jpg', 'https://media.timesfreepress.com/img/photos/2014/11/09/111014b04_black_bears_t800_h8b150168072b2d9c018ca481200aa3e621674f4c.JPG', 'https://www.realtree.com/sites/default/files/styles/site_large/public/content/inserts/642px-blackbearatwoburnsafaripark.jpg', 'http://adwimages.co.uk/Blog/wp-content/uploads/2016/11/adult-black-bear.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/black-bear-cub-in-a-tree-ursula-salzmann.jpg', 'https://cdn.vox-cdn.com/thumbor/SNyMBO6T0qwheM_sxxaWIKDDPAc=/0x0:5000x3313/1200x800/filters:focal(1435x1342:2235x2142)/cdn.vox-cdn.com/uploads/chorus_image/image/57942213/GettyImages_187133529.0.jpg']\n    Images downloaded.\nteddy bear: Got 283 image URLs. Downloading...\n   urls =  ['https://i2-prod.birminghammail.co.uk/incoming/article17583074.ece/ALTERNATES/s1227b/0_IFP_BEM_160120bears011JPG.jpg', 'http://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-14271861817446_1200x.jpg?v=1584804420', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/970/6782/Life_size_72in_black_teddy_bear_Juju_Cuddles_is_Tall_dark_and_adorable__95688.1463592717.jpg?c=2', 'https://www.sugardelites.com/assets/images/teddybearlarge.jpg', 'http://www.photos-public-domain.com/wp-content/uploads/2010/12/white_teddy_bear.jpg', 'http://wallsdesk.com/wp-content/uploads/2016/11/Lonely-Teddy-Bear.jpg', 'http://www.fitzulas.com/Merchant4c/graphics/00000001/2014/gund-4040161-xl.jpg', 'https://backdropscanada.ca/wp-content/uploads/2015/01/p-8282-Teddy_Bear_Blue_TL_800302__02186.jpg', 'https://cdn0.rubylane.com/shops/chippewalakeantiques/863.1L.jpg', 'https://cdn.shopify.com/s/files/1/2975/6858/products/tan-big-bear-9_1200x1200.jpg?v=1524446749', 'https://cdn.notonthehighstreet.com/system/product_images/images/001/708/113/original_standing-teddy-bears.jpg', 'https://n2.sdlcdn.com/imgs/i/6/4/X-Atom-Teddy-Bears-Couple-SDL372766290-2-658f9.jpeg', 'https://www.sippycupmom.com/wp-content/uploads/2017/09/NTBD-Exclusive-Bear.jpg', 'http://theawesomedaily.com/wp-content/uploads/2017/10/teddy-bear-images-10-2.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Photos.png', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/883/8957/48in_green_teddy_bear_Big_smile_and_soft_cuddly_bear__93790.1493921224.jpg?c=2', 'https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1543338622-costco-giant-teddy-bear-1543338603.jpg?crop=1xw:1xh;center,top', 'http://icecreamoffpaperplates.com/wp-content/uploads/2016/08/teddy-bears.jpg', 'http://cdn.shopify.com/s/files/1/2610/9472/products/BL1985_BIRKIN_TEDDY_BEAR_01_1024x1024.jpg?v=1554885535', 'https://d3n8a8pro7vhmx.cloudfront.net/accessalliance/pages/454/meta_images/original/Cute_but_Lonely_Teddy_Bear_Sitting_on_Grass.jpg?1591840883', 'https://www.super99.in/media/catalog/product/cache/1/image/1800x/040ec09b1e35df139433887a97daa66f/0/1/01_22.jpg', 'https://static1.momsimage.com/wordpress/wp-content/uploads/2020/03/2020-03-30_110127.jpg', 'https://www.stuffedwithplushtoys.com/assets/full/27-1704436.jpg?20200129144114', 'https://www.celebrationgiftware.com.au/media/catalog/product/cache/cb58f11d039ee9c021f50f3360077761/t/s/tshirtbear-yellow-birthdetails.jpg', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-8100-14243358867558_1024x1024.jpg?v=1588363171', 'https://cdn.notonthehighstreet.com/fs/34/8c/3995-961a-4318-86e4-a5ee78006bf4/original_personalised-teddy-bear-gift.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG114.png', 'http://edicollector.com/wp-content/uploads/2017/10/Krampus-Teddy-Bear-Edicollector-02.jpg', 'https://img.pngio.com/teddy-bear-png-baby-teddy-bear-png-1456_1456.png', 'https://cdn1.tedsby.com/tb/large/storage/1/3/0/13062/cute-handmade-teddy-bear-miniature-ooak-artist.jpg', 'https://cdn0.rubylane.com/_pod/item/1068141/0001459/1910s-Antique-Steiff-Tiny-Teddy-Bear-pic-1-2048-50-f.jpg', 'https://cdn0.rubylane.com/_pod/item/1494839/RL446/Large-Ideal-Antique-Teddy-Bear-full-2o-2048-14-r-ffffff-9f9b7e.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG104.png', 'https://startknitting.org/wp-content/uploads/2018/03/Teddy-Bear-Free-Knitting-Pattern-f.jpg', 'https://cdn.notonthehighstreet.com/fs/7a/fe/5bda-9f46-403c-9d21-c798fa5de5ed/original_personalised-siblings-teddy-bears.jpg', 'https://cdn1.tedsby.com/tb/large/storage/1/3/9/139462/cute-handmade-teddy-bear-oliver.jpg', 'http://www.listofdownload.com/wp-content/uploads/2017/02/red-roses-cute-teddy-bear-picture-background-wallpaper.jpg', 'https://i5.walmartimages.com/asr/5d5541cd-d9cc-4b47-b7de-ebc107b74d82_1.9f14f307142cfd730852e875760b90bc.jpeg', 'https://townsquare.media/site/43/files/2020/08/Teddy-Bear.jpg?w=1200&h=0&zc=1&s=0&a=t&q=89', 'https://alqurumresort.com/img/vermont-teddy-bear-valentines-day.jpg', 'https://api.time.com/wp-content/uploads/2016/12/teddy-bear.jpg', 'https://www.kentishembroidery.co.uk/133/mm01-cuddly-mumbles-bracken-teddy-bear-989.jpg', 'http://www.fitzulas.com/Merchant4c/graphics/00000001/2012/gund-015418-xl.jpg', 'http://oglasi.ovh/images/2020/02/15/84/pomeranian-boo-white-teddy-bear_4.jpg', 'https://perpetualpreschool.com/wp-content/uploads/2017/12/32221695_ml.jpg', 'https://cdn.shopify.com/s/files/1/0270/2981/products/Bear_Plush_Pink_Color_with_Bow_25_cm_1024x1024.jpg?v=1527556065', 'https://www.myflowergift.com/media/catalog/product/cache/1/image/1200x1200/9df78eab33525d08d6e5fb8d27136e95/s/n/snow_white..3_feet_height_white_teddy_bear_4000.jpg', 'http://www.photos-public-domain.com/wp-content/uploads/2011/01/teddy-bear-wearing-mardi-gras-beads.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Wallpaper.jpg', 'https://3.bp.blogspot.com/-MF361EGT_e4/TrwxX4LsSMI/AAAAAAAAEP8/xKbu4DUVjEI/s1600/little+bear.JPG', 'https://www.petlandracine.com/wp-content/uploads/2019/05/1386238_800.jpg', 'https://2.bp.blogspot.com/-v-DbNHEhI0c/UkK6mc0ImdI/AAAAAAAAAB8/4rdaS87Bxqo/s1600/Giant+Teddy+Halloween+2013+Photo+Blog+4.jpg', 'https://cdn.shopify.com/s/files/1/0798/6397/products/eddie_cover_no_logo.jpg?v=1535766533', 'https://www.kidswholesaleclothing.co.uk/4599-thickbox_default/soft-toys-for-personalisation-teddy-bear-grey.jpg', 'https://cdn.thisiswhyimbroke.com/images/ebonics-romantic-bear.jpg', 'https://www.worldofbears.com/acatalog/Oxford%2010in_WEB.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG5.png', 'https://cdn.notonthehighstreet.com/system/product_images/images/001/941/993/original_baby-teddy-bear-dress-up-costume.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG14.png', 'https://www.zookini.com.au/media/catalog/product/cache/8/image/5beb446e055b53f6b95ea24807ed418d/w/a/waffle-cute-teddy-bear-beige-01.jpg', 'https://teddy.ph/wp-content/uploads/2019/08/feature-30.jpg', 'http://www.sippycupmom.com/wp-content/uploads/2017/09/Teddy-Bear.jpg', 'https://cdn.jewishboston.com/uploads/2019/01/iStock-932242472.jpg', 'https://newcastlebeach.org/images/teddy-bear-with-heart-4.jpg', 'https://1.bp.blogspot.com/_GSA-YfDFMXc/TDhlS3NvdcI/AAAAAAAAAb8/m0-7WTaPif8/s1600/DSC_1528.JPG', 'https://teddybears.co.uk/wp-content/uploads/2019/02/uk-2019-2736x3648.jpg', 'https://cdn.thewhoot.com/wp-content/uploads/2018/06/Knitted-Teddy-Bear-Patterns-1.jpg', 'https://cdn0.rubylane.com/shops/1147637/bk727.1L.jpg', 'https://cdn0.rubylane.com/_pod/item/737200/266/Vintage-Steiff-Original-Teddy-Bear-1950x7827s-full-3o-2048-eea9d521-f.jpg', 'https://cdn-r.fishpond.com/0037/184/777/237402416/6.jpeg', 'https://cdn.notonthehighstreet.com/fs/0e/9b/9f39-5053-4edd-a7b8-33a6780fb100/original_personalised-1st-christmas-teddy-bear.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG24.png', 'https://www.teddybearaustralia.com.au/wp-content/uploads/2018/09/FullSizeRender-6.jpg', 'https://worldofbears.com/acatalog/Steiff-022098.jpg', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/883/8961/4ft_green_teddy_bear_Big_smile_and_soft_cuddly_bear__85725.1493921227.jpg?c=2', 'https://thegadgetflow.com/wp-content/uploads/2013/01/Personalised-Teddy-Bear-In-A-Gift-Tin-4.jpg', 'https://www.steiffteddybears.co.uk/img/products/027710.png', 'https://image.made-in-china.com/2f0j00wqhfEUTWOVbo/Factary-Price-Giant-Teddy-Bear-Big-Plush-Teddy-Bear-Soft-Toys-for-Girls.jpg', 'https://cdn0.rubylane.com/_pod/item/1491561/Peacockx20Bear/Peacock-Stores-Teddy-Bear-foot-label-pic-1A-2048:10.10-27-f.jpg', 'https://n3.sdlcdn.com/imgs/g/u/e/Ultra-Blue-Teddy-Bear-12-SDL607416536-2-76098.jpeg', 'https://1.bp.blogspot.com/-ChPUEPH4ssg/T-nXRUqH6zI/AAAAAAAAAWY/7HEyWdQvV_E/s1600/teddy+bear+picnic+(7).JPG', 'https://s2.r29static.com/bin/entry/a85/0,0,2000,1050/x,80/1655673/image.jpg', 'https://app.skufetch.com/images.tmp/Gemmy_8_5_ft_Animate_294606_109_0_res.jpg', 'https://www.stuffedwithplushtoys.com/assets/full/u4040131.jpg', 'https://www.bloomsonly.com/media/catalog/product/cache/2/image/1200x1200/9df78eab33525d08d6e5fb8d27136e95/m/e/meng-meng-cute-brown-teddy-bear-head-tilt-twisting-holiday-gifts-velvet-ribbon-light-brown-bear.jpg', 'https://www.desicomments.com/dc3/02/215657/215657.jpg', 'https://www.teddybearacademy.net/wp-content/uploads/2014/08/Marco-full.jpg', 'https://st4.depositphotos.com/4431055/22853/i/950/depositphotos_228530988-stock-photo-cute-teddy-bears-hugging-back.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG45.png', 'http://images.antiquesatlas.com/dealer-stock-images/sladesantiques/Vintage_Small_9_Steiff_Mohair__as527a528z.jpg', 'https://www.worldofbears.com/acatalog/LG01.jpg', 'https://teddybears.co.uk/wp-content/uploads/2017/08/075797-2736x3648.jpg', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/882/8984/6ft_Green_lucky_teddy_Life_Size_bear_cuddly_Giant_and_biggest_cuddles_bear__64679.1493929106.jpg?c=2', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG11.png', 'http://www.pavlovspuppets.com/p/teddy3.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Pictures.jpg', 'https://www.taylorherring.com/wp-content/uploads/2015/04/GiantTeddyBears-6.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Wallpapers.jpg', 'https://primopromo.com.au/assets/Uploads/Teddy-Bear+Dark-Green-v3.jpg', 'https://cdn0.rubylane.com/shops/700632/I-1277.1L.jpg', 'https://cdn1.bigcommerce.com/server1900/dee9d/images/stencil/700x900/products/970/6782/Life_size_72in_black_teddy_bear_Juju_Cuddles_is_Tall_dark_and_adorable__95688.1463592717.jpg?c=2', 'https://n1.sdlcdn.com/imgs/i/7/9/Teddy-bear-SDL324941391-1-f3e93.png', 'https://1.bp.blogspot.com/-eTFJzsbT2Pg/T3qHL_rPBMI/AAAAAAAABsA/5J8eXBe2d9A/s1600/Teddy+Bear+Wallpapers.jpg', 'http://geekologie.com/2014/11/10/teddy-bear-dog-2.jpg', 'https://plazarealestate.com.au/wp-content/uploads/sites/131/2016/07/Teddy-Bears.jpg', 'https://cdn0.rubylane.com/_pod/item/1491561/1920sx20Germanx20bear/lovely-German-1920s-teddy-bear--full-2-2048-93.jpg', 'https://cdn.geekwire.com/wp-content/uploads/2017/11/Pasted-image-at-2017_11_15-02_22-PM.png', 'https://twinsmom.com/wp-content/uploads/2020/04/GUND-Slumbers-Teddy-Bear-Stuffed-Animal-Plush-Brown-1722.jpg', 'https://teddybears.co.uk/wp-content/uploads/2019/02/1903-2736x3648.jpg', 'https://dayz.gamepedia.com/media/dayz.gamepedia.com/e/e5/TeddyBearBrown.png', 'https://www.melijoe.com/images/142519/open_graph.jpg', 'http://3.bp.blogspot.com/-oU6bGNvicT0/UjH80jmsEfI/AAAAAAAAFTA/H6gDxpq_rE4/s1600/teddy_bear_wallpapers+%2814%29.jpg', 'https://i.ebayimg.com/images/i/221655718748-0-1/s-l1000.jpg', 'https://www.worldofbears.com/acatalog/steiff-ean-001765.jpg', 'https://winkgo.com/wp-content/uploads/2016/09/Proud-Grandpa-Buys-Giant-Teddy-Bear-His-Granddaughter-05.jpg', 'https://s-i.huffpost.com/gen/1995262/images/o-TEDDY-BEAR-COOKIES-facebook.jpg', 'https://n3.sdlcdn.com/imgs/i/h/h/Mable-Cuddle-Big-Teddy-Bear-SDL312942984-1-7984b.jpeg', 'https://i5.walmartimages.com/asr/dfe8f0fc-3ada-4f4d-b641-4e86af319c30_1.2c587767c675128916b101e1563dc5b2.jpeg', 'https://n4.sdlcdn.com/imgs/a/0/r/Toysaa-Brown-Jumbo-Teddy-SDL441635333-1-f7f59.jpg', 'https://www.worldofbears.com/acatalog/steiff-ean-012037.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Background.jpg', 'https://www.tteengift.com/wp-content/uploads/2014/12/Small-Classic-Teddy-Bear-G01.jpg', 'https://www.dhresource.com/0x0s/f2-albu-g3-M01-AF-81-rBVaHFoewiyATnxuAAPEbt5vzFw787.jpg/63-160-cm-white-giant-teddy-bear-big-huge.jpg', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-105-14241810382950_1200x.jpg?v=1584732813', 'https://www.jarrold.co.uk/userdata/root/images/products/steiff/teddy-bear-replica-1909.jpg', 'https://usercontent2.hubstatic.com/14656219_f1024.jpg', 'https://www.worldofbears.com/acatalog/steiff-113673.jpg', 'https://4.bp.blogspot.com/_OTdJvruSaiI/Swtbm1GvbWI/AAAAAAAAFdo/WXyAoA_1Qco/s1600/IMG_3609.JPG', 'http://cdn.shopify.com/s/files/1/0748/9103/products/steiff_10_1920_brass_000713-37_grande.jpeg?v=1426675827', 'https://giftteens.com/wp-content/uploads/2018/01/PPST0063.jpg', 'https://theawesomedaily.com/wp-content/uploads/2017/10/teddy-bear-images-17-1.jpg', 'http://www.worldofbears.com/acatalog/gund-4056332.jpg', 'http://4.bp.blogspot.com/_ED0zYuiiwBg/TBke2UCnvHI/AAAAAAAACJU/AfQyXaz-NJI/s1600/aetna+bear.jpg', 'https://www.buildabear.co.uk/on/demandware.static/-/Sites-buildabear-master/default/dw82e7e236/25239x.jpg', 'https://www.lifejackets.com/wp-content/uploads/2020/03/229-EBbearblueribboneverych-1.png', 'http://www.funandcash.com/ebay/teddy-white-small-X-1-A.jpg', 'https://3.bp.blogspot.com/_ED0zYuiiwBg/TLNvSWhZTCI/AAAAAAAACrM/1zXtB07mYlY/s1600/teddy+bears+halloween+closeup+2.JPG', 'https://jooinn.com/images/teddy-bear-48.jpg', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-106-14242280636518_1200x.jpg?v=1597087632', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-8101-14241651163238_1200x.jpg?v=1584559861', 'https://www.carouselshop.co.uk/media/catalog/product/cache/9/image/2000x/040ec09b1e35df139433887a97daa66f/B/F/BFFC8662B837D2AA8409CA13393604F6.jpg', 'https://www.worldofbears.com/acatalog/Steiff-237010.jpg', 'https://cdn3.volusion.com/9nxdj.fchy5/v/vspfiles/photos/BB-1486-2.jpg?v-cache=1520605016', 'https://cdn.shopify.com/s/files/1/3045/2256/products/GRID-1_76173971-e87f-42ab-bebd-e5f0db3fe386_600x900.png?v=1594136153', 'https://www.grabhub.co.uk/wp-content/uploads/2020/06/Big-Cuddly-Teddy-Bear-4.jpg', 'https://cdn.shopify.com/s/files/1/1417/0972/products/4056786-high-res_1024x1024.jpg?v=1548786811', 'https://lovemydoodles.com/wp-content/uploads/2020/04/IMG_0006-18.jpg', 'https://i5.walmartimages.com/asr/ab097d42-d89e-4db0-a7df-6d87b08de9ab_1.0e68fae386f2fafcb61bfbd7fc12828e.jpeg', 'https://www.worldofbears.com/acatalog/steiff-000867.jpg', 'http://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-110-14235158642790_1200x.jpg?v=1584033067', 'https://www.braillehouse.org.au/wp-content/uploads/2018/12/Hand-knitted-teddy-bear-3.jpg', 'https://3.bp.blogspot.com/-vmp7XBf08eM/Tca-_DqLdeI/AAAAAAAACGQ/cJa3P2VUpT4/s1600/%252362+Teddy+bears+picnic.JPG', 'https://image.made-in-china.com/2f0j00NdUtRvuYOLbp/2017-New-Design-Scary-Teddy-Bear-Zombie-Bear-Undead-Bear-Bloody-Bear-Horrible-Teddy-Bear.jpg', 'https://i5.walmartimages.com/asr/aa216a37-5af0-447c-a7eb-515b94be2bc7.c9ca3e8b8902b7cac68d5a93587a5598.jpeg', 'https://teddybears.co.uk/wp-content/uploads/2015/09/petsy-28cm.jpg', 'http://www.iheartteacups.com/wp-content/uploads/2017/05/IMG_3852.jpg', 'https://cdn.notonthehighstreet.com/fs/93/ed/2016-9d7d-4c2d-bc12-e84f66c2e4d5/original_valentine-s-day-bertie-bear.jpg', 'https://4.bp.blogspot.com/-003EDE5jTOw/UDSwjEWC7oI/AAAAAAAADuQ/lA_x_5ECQAo/s1600/teddy-bear-14-hqwallpapers4u.co.cc.jpg', 'https://3.bp.blogspot.com/_ED0zYuiiwBg/TTeCTVngmNI/AAAAAAAADF0/b6ySiPXTVlA/s1600/sleep+eyed+bear.JPG', 'http://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-108-14241933426790_1200x.jpg?v=1588417411', 'http://www.fitzulas.com/Merchant4c/graphics/00000001/2016/4056250.jpg', 'https://www.sayitwithbears.co.uk/wp-content/uploads/2016/07/large-personalised-bonnie-honey-scaled.jpg', 'https://webimg.secondhandapp.com/1.1/5e30189ab33a15626007900b', 'https://www.buildabear.com/on/demandware.static/-/Sites-buildabear-master/default/dwa68008ec/26614x.jpg', 'http://www.worldofbears.com/acatalog/mxmas-2017-1.jpg', 'https://hdwallpaperfun.com/wp-content/uploads/2015/10/Cute-Teddy-Bear-Wallpaper.jpg', 'https://teddybears.co.uk/wp-content/uploads/2015/05/MT-new-2017-cropped.jpg', 'https://cdn0.rubylane.com/shops/1539575/0025.1L.jpg', 'http://cdn.shopify.com/s/files/1/2429/7549/products/Edgar-_4_634bce55-3448-4579-adaf-349cc2dfe069_1200x1200.jpg?v=1557254763', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-14271863685222_1024x1024.jpg?v=1586794398', 'https://2.bp.blogspot.com/_ED0zYuiiwBg/S_FJV5v3f4I/AAAAAAAAB5o/7GGX-bleb1Y/s1600/bears.JPG', 'https://cdn11.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/59/9448/Huge_Blue_Teddy_Bear_Happy_Cuddles_38in__38354.1495128960.jpg?c=2', 'http://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-103-14241683964006_1200x.jpg?v=1585177055', 'https://n1.sdlcdn.com/imgs/g/x/j/ToyHub-3-Feet-Super-Soft-SDL448496021-1-7dfe5.jpeg', 'https://cdn-image.realsimple.com/sites/default/files/styles/portrait_435x518/public/1510689111/giant-teddy-bear.jpg?itok=JRUEDZtP', 'https://www.buildabear.com/dw/image/v2/BBNG_PRD/on/demandware.static/-/Sites-buildabear-master/default/dw78d308a2/28167x.jpg?sw=600&sh=600&sm=fit&q=70', 'https://www.pcsoutdoors.com/images/products/detail/BeaverBearLgFront.jpg', 'https://www.buildabear.co.uk/on/demandware.static/-/Sites-buildabear-master/default/dwdac83d3c/24663x.jpg', 'http://allpicts.in/wp-content/uploads/2015/10/Cute-Teddy-bear-with-Pink-roses.jpg', 'https://www.thetoyshoppe.com/images/productslarge/57457d.jpg', 'https://www.stuffedwithplushtoys.com/assets/full/17223-RD.jpg?20201103131623', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG97.png', 'https://teddybears.co.uk/wp-content/uploads/2017/08/113413_113420_113437_web-2736x3648.jpg', 'https://4.bp.blogspot.com/-fGtPzuETOho/T3qG6e61h9I/AAAAAAAABro/z8CWfoVG3bQ/s1600/Teddy+Bear+Wallpapers+2.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG94.png', 'http://cdn.shopify.com/s/files/1/0704/5013/products/teddy-bears-gund-maxie-bear-beige-24-1_1024x1024.jpg?v=1544388291', 'http://c.shld.net/rpx/i/s/i/spin/image/spin_prod_1099814412??hei=64&wid=64&qlt=50', 'https://cdn0.rubylane.com/shops/1754162/118.1L.jpg', 'http://www.photos-public-domain.com/wp-content/uploads/2010/12/cream_colored_teddy_bear.jpg', 'https://cdn0.rubylane.com/_pod/item/737200/266/Vintage-Steiff-Original-Teddy-Bear-1950x7827s-full-1o-2048-fe3a394d-f.png', 'https://cdn0.rubylane.com/_pod/item/1491561/Peacockx20Bear/Peacock-Stores-Teddy-Bear-foot-label-full-3o-2048-94-r-ffffff-d8cbbb.jpg', 'https://cdn0.rubylane.com/shops/your-favorite-doll/156.1L.jpg', 'https://954puppies.com/wp-content/uploads/2019/07/DSC03401-1.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG7.png', 'https://www.pets4homes.co.uk/images/classifieds/2018/08/16/2028360/large/stunning-toy-cavapoo-teddy-bear-puppies-pra-clear-5b9d3f067fc36.jpg', 'https://n1.sdlcdn.com/imgs/f/g/5/Lovely-Multicolour-Small-Teddy-Bear-SDL963778079-1-36406.jpg', 'https://www.buildabear.com/on/demandware.static/-/Sites-buildabear-master/default/dwbebeadb4/24925x.jpg', 'http://cdn.shopify.com/s/files/1/2679/2566/articles/19424368_161403274403867_6843080688077087305_n_1200x1200.jpg?v=1563408993', 'https://www.luxurypuppies2u.com/wp-content/uploads/2020/01/bibi-teddy-bear-dog-2.jpg', 'https://cdn.notonthehighstreet.com/fs/d7/0c/8e2c-5db7-4ecf-8fec-9dd401de329a/original_personalised-new-baby-teddy-bear.jpg', 'https://www.worldofbears.com/acatalog/steiff-403286.jpg', 'https://www.worldofbears.com/acatalog/steiff-026812a.jpg', 'https://www.pioupiou-et-merveilles.fr/3237-large_default/giant-teddy-bear-gaston-beige-100cm.jpg', 'https://d3h6k4kfl8m9p0.cloudfront.net/stories/qjGrIHo8t0ha-j8tgE3tdQ.jpeg', 'https://rare.us/wp-content/uploads/sites/3/2017/07/teddy-bears.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG90.png', 'http://1.bp.blogspot.com/_ED0zYuiiwBg/TCoGLyKhVNI/AAAAAAAACMU/Sk2m6kJtwQ0/s1600/odd+bear+seated.jpg', 'https://www.henrymayo.com/images/content-images/Teddy-Bear.jpg', 'https://www.onlinedelivery.in/images/thumbnails/1202/1165/detailed/29/8522.jpg', 'https://usercontent2.hubstatic.com/14720461_f1024.jpg', 'https://www.buildabear.com/on/demandware.static/-/Sites-buildabear-master/default/dw2b81d8a7/28502x.jpg', 'https://cdn.shopify.com/s/files/1/2622/2548/products/wholesale-plush-in-a-rush-112-14235212972134_1200x.jpg?v=1584034090', 'https://www.hicodenver.com/wp-content/uploads/2020/05/Teddy-Bear-with-Heart.png', 'http://1.bp.blogspot.com/-z6yWAofGr14/T3FujYmUuTI/AAAAAAAABZ8/LY-pQFofo0o/s1600/Christmas+Teddy+Bear+Wallpapers+4.jpg', 'https://www.worldofbears.com/acatalog/steiff-113666.jpg', 'http://traditionsjewishgifts.com/media/brown-hanukkah-plush-teddy-bear-RLTYTEDDY2.jpg', 'https://www.school-bears.co.uk/wp-content/uploads/2018/01/personalised-teddy-bear-anne.jpg', 'https://www.worldofbears.com/acatalog/ean-034978.jpg', 'https://sc01.alicdn.com/kf/HTB17fixJ1uSBuNjy1Xcq6AYjFXaJ.jpg', 'https://api.time.com/wp-content/uploads/2019/12/Teddy-Bear.jpg?quality=85&w=1200&h=628&crop=1', 'http://weneedfun.com/wp-content/uploads/2015/10/Cute-Teddy-Bear-Wallpaper-4.jpg', 'https://1.bp.blogspot.com/-gdpjJWS07gg/UkK6oVmYZXI/AAAAAAAAACY/RZu_k0a2hl4/s1600/Giant+Teddy+Halloween+2013+photo+blog+1.jpg', 'https://pixfeeds.com/images/baby-care/baby-gifts/1280-622057492-white-teddy-bear.jpg', 'https://purepng.com/public/uploads/large/two-teddy-bear-in-basket-qyo.png', 'https://3.bp.blogspot.com/_ED0zYuiiwBg/S_FJynlFrhI/AAAAAAAAB5w/ioeinkP5Ri0/s1600/bears+profile.JPG', 'https://cdn11.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/143/1173/Sweetie-Pie-Big-Love-hazelnut-brown-teddy-bear-30in__97755.1542992358.jpg?c=2', 'https://d3h6k4kfl8m9p0.cloudfront.net/stories/B4LWqbQh48c0ultSGdJraA.jpeg', 'https://cdn.notonthehighstreet.com/system/product_images/images/001/942/001/original_baby-teddy-bear-dress-up-costume.jpg', 'http://cdn.shopify.com/s/files/1/0748/9103/products/jellycat_18_bumbly_bears_grande.jpg?v=1530708337', 'https://www.taylorherring.com/wp-content/uploads/2015/04/GiantTeddyBears-5-781x1020.jpg', 'http://1.bp.blogspot.com/-4Z91ISiPs94/UYzX1S-xB9I/AAAAAAAAE7c/MGsL0bQ2EpY/s1600/Coco-L-Cuddles-white-teddy-bear-with-necklace-26in__23715.1327379838.1280.1280.jpg', 'https://theeducationhub.org.nz/wp-content/uploads/2020/03/Teddy-bear-hunt-scaled.jpg', 'https://cdn.suwalls.com/wallpapers/photography/happy-teddy-bear-family-under-the-snowy-pine-tree-51847-1920x1080.jpg', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/875/6201/Adorable_Brown_Bears_Set_Of_three_Gift_Huge_Family_Giant_Teddy__55363.1436899340.jpg?c=2', 'https://teddybearsandfriends.com/wp-content/uploads/2013/12/Teddy-Bear-Miniature-US-Special-1984-97-98-1_.jpg', 'https://usercontent2.hubstatic.com/11954973_f260.jpg', 'https://wallsdesk.com/wp-content/uploads/2016/11/Teddy-Bear-Wallpapers-HD.jpg', 'http://www.fitzulas.com/Merchant4c/graphics/00000001/2017/173902.jpg', 'https://4.bp.blogspot.com/-m23FOuiaNfM/Vlcb1HUm1-I/AAAAAAAABi4/NAAxnfHZ7kY/s1600/IMG_2519.JPG', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/1280x1280/products/354/2910/Sweetie-Tubs-mocha-brown-teddy-bear-65in_3__58929.1323818431.jpg?c=2', 'https://oneofakindantiques.com/r.php?itemNumber=5261&imageNumber=1&w=1500&type=regular', 'https://teddybears.co.uk/wp-content/uploads/2017/09/113321.jpg', 'https://i.ebayimg.com/00/s/MTAyNFg3Njg=/z/pnMAAOSw7eld87sL/$_86.JPG', 'https://www.megelles.com/assets/full/MEG-frankie.jpg?20191212083854', 'https://www.fromme.co.zw/uploads/2020/08/Teddy-FM_.jpg', 'http://2.bp.blogspot.com/-uBj8qN2NvdM/T3FurKV2-WI/AAAAAAAABaE/04CNHLuEczw/s1600/Christmas+Teddy+Bear+Wallpapers.jpg', 'https://bestdolltoys.com/wp-content/uploads/2020/03/51YOoVe9H1L.jpg', 'https://cdn.thewhoot.com/wp-content/uploads/2018/06/Knitted-Teddy-Bear-Patterns--550x896.jpg', 'https://cdn0.rubylane.com/_pod/item/1130386/Bear1/Adorable-antique-Jointed-Mohair-Teddy-Bear-full-4o-2048-a9115339-r-ffffff-f68787.jpg', 'https://i.ebayimg.com/00/s/MTAyNFg4OTk=/z/pwYAAOSw4VRc6XRa/$_86.JPG', 'http://2.bp.blogspot.com/_ED0zYuiiwBg/TTd9rrMJswI/AAAAAAAADFc/i3w3l43MeGg/s1600/aetna.JPG', 'https://s32071.pcdn.co/wp-content/uploads/ep0195_1.jpg', 'https://craftsmumship.com/wp-content/uploads/2013/04/Kina-dolls-size-knitted-cardigan.jpg', 'https://www.professorshouse.com/wp-content/uploads/2019/08/teddy-bear-names.jpg', 'http://mylitter.com/wp-content/uploads/2019/02/bear.jpg', 'https://petvblog.com/wp-content/uploads/2020/06/teddy-bear-puppies-1024x683.jpg', 'https://www.worldofbears.com/acatalog/steiff-ean-000997.jpg', 'https://3.bp.blogspot.com/_ED0zYuiiwBg/TLNuVSd7t_I/AAAAAAAACrE/AXkCJv9QF9Y/s1600/teddy+bears+halloween.JPG', 'http://cdn.shopify.com/s/files/1/2975/6858/products/large-tan-bear_1200x1200.jpg?v=1524446719', 'http://www.familytree.com/wp-content/uploads/2017/09/National-Teddy-Bear-Day-Find-more-genealogy-blogs-at-FamilyTree.com_.jpg', 'https://s3.amazonaws.com/hoover.church.of.christ/wp-content/uploads/2020/08/teddy-bear-sick-in-the-hospital.jpg', 'https://www.stuffedwithplushtoys.com/assets/full/9750-30.jpg?20180212090824', 'https://www.worldofbears.com/acatalog/steiff-ean-111556.jpg', 'https://cdn8.bigcommerce.com/s-dee9d/images/stencil/700x900/products/883/8961/4ft_green_teddy_bear_Big_smile_and_soft_cuddly_bear__85725.1493921227.jpg?c=2', 'https://www.carouselshop.co.uk/media/catalog/product/cache/9/image/2000x/040ec09b1e35df139433887a97daa66f/E/E/EE49104AE5D9AFA3C9C246D2F4F81F73.jpg', 'https://www.elizabeths.co.uk/upload/mt/eliz126/products/lg_null-teddy-bears---grey.jpg', 'https://www.sayitwithbears.co.uk/wp-content/uploads/2017/05/SN0778-925x1024.jpg', 'https://www.grabhub.co.uk/wp-content/uploads/2020/07/Peek-a-Boo-Teddy-Bear-1.jpg', 'https://images.wallpaperscraft.com/image/soft_toy_teddy_bear_bear_117110_938x1668.jpg', 'https://www.snappysnaps.co.uk/media/catalog/product/cache/62/image/650x650/9df78eab33525d08d6e5fb8d27136e95/s/n/snappy_teddy_bear.png', 'https://petclassifieds.com/wp-content/uploads/2020/10/20201003_153234.jpg', 'https://images.wallpaperscraft.com/image/toy_teddy_bear_teddy_31381_800x1420.jpg', 'https://www.curiousmondo.com/images/2017/11/02/teddy-bear-making-04.jpg', 'https://www.worldofbears.com/acatalog/LCG01.jpg', 'https://image.made-in-china.com/2f0j00iOetbZrDhvzU/Mini-Baby-Teddy-Bear-Keychain-with-Shirt.jpg', 'https://pngimg.com/uploads/teddy_bear/teddy_bear_PNG106.png', 'https://www.thunderbay.ca/en/recreation/resources/Thunder-Bay_recreation_interior_0032_Events---teddy-bears-picnic.jpg', 'http://1.bp.blogspot.com/-NosZBJngTnA/Tp_rt11Z4PI/AAAAAAAAAJg/GNVaaoWmesg/s1600/Teddy+bear+detail1.JPG', 'https://www.carouselshop.co.uk/media/catalog/product/cache/9/image/2000x/040ec09b1e35df139433887a97daa66f/D/1/D10F255704F7CE7FBEB25963EFF90A74.jpg', 'https://www.scarymommy.com/wp-content/uploads/2017/11/screen-shot-2017-11-13-at-8-39-00-am.png', 'https://3.bp.blogspot.com/-JB9Xu_dEIxA/UkLLT_b-TXI/AAAAAAAAADM/2UnCl1TvU8g/s1600/Giant+Teddy+Fall+2013+photo+blog+3.jpg', 'https://s34468.pcdn.co/wp-content/uploads/2019/09/DSC07821-scaled.jpg', 'https://cdn0.rubylane.com/shops/738907/T722.1L.jpg?63']\n    Images downloaded.\nAfter checking images for issues, 714 (total) images remain.\nfiles has 714 elements.\nfiles[0] = scraped_images/grizzly/00000143.jpg, files[-1] = scraped_images/teddy/00000109.png2.png\n\n\nAnd a routine that will parse filename to get the class label, and another that will list all the files in a certain class\n\ndef label_func(file_path):\n    return file_path.parents[0].name  # just get the name of the parent directory for the file\n\nprint(f\"label_func yields {set([label_func(fp) for fp in files])}\")  # sanity check: do we get all the labels we expect?\n\nlbl2files = {l: [f for f in files if label_func(f) == l] for l in labels}\n\nlabel_func yields {'teddy', 'grizzly', 'black'}\n\n\n\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        if len(self) &gt; 2:\n            img1,img2,similarity = self\n        else:\n            img1,img2 = self\n            similarity = 'Undetermined'\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), title=similarity, ctx=ctx, **kwargs)\n\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, splits):\n        self.splbl2files = [{l: [f for f in files[splits[i]] if label_func(f) == l] for l in labels}\n                          for i in range(2)]\n        self.valid = {f: self._draw(f,1) for f in files[splits[1]]}\n    def encodes(self, f):\n        f2,same = self.valid.get(f, self._draw(f,0))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, int(same))\n    \n    def _draw(self, f, split=0):\n        same = random.random() &lt; 0.4\n        cls = label_func(f)\n        if not same: cls = random.choice(L(l for l in labels if l != cls)) \n        return random.choice(self.splbl2files[split][cls]),same\n\nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, splits)\ntls = TfmdLists(files, tfm, splits=splits)\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n                      after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nvalids = [v[0] for k,v in tfm.valid.items()]      \nassert not [v for v in valids if v in files[splits[0]]]\n\n\n\nDefining the Contrastive Loss Model\nWe’re going to, again, copy the fastai Siamese Networks tutorial but change up the model a bit: we’re going to output two “embedding vectors”, one for each input image.\nI found it way easier to interface with the rest of fastai if I just “packaged” these two vectors into one big long vector, and then later just ‘cut it in half’ whenever I needed one vector or the other.\nInitially we’re going to use an embedding dimension of 3 so we can visualize it. This isn’t actually going to be ideal for achieving high accuracy, so afterward we’ll re-do it with the dimension increased.\n\nclass SiameseNet(nn.Module):\n    def __init__(self, \n        arch=resnet34, \n        lin_ftrs= [256, 128], \n        emb_sz=128,  # dimensionality of embeddings: 128 works pretty well. Sadly 3 is too small in my experience\n        ps=0.5, \n        bn_final=False):\n        super(SiameseNet, self).__init__()\n        store_attr()\n        self.body = create_body(self.arch, cut=-2) \n        self.head = create_head(num_features_model(self.body), self.emb_sz, self.lin_ftrs, self.ps, self.bn_final)\n        self.cnn = nn.Sequential(self.body, self.head)\n                                  \n    def forward(self, x1, x2):  # note that this expects two inputs not a single pair-like \"SiameseImage\" \n        output1 = self.cnn(x1)\n        output2 = self.cnn(x2)\n        return torch.cat((output1,output2),dim=-1)  \n        \n    def get_embedding(self, x): # operates on just one image instead of 2\n        return self.cnn(x)\n\n\ndef split_out(out):  # this will split the model output down the middle, into the set of two embeddings\n    return torch.split(out, int(out.shape[-1]/2), dim=-1)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nmodel = SiameseNet().to(device)\napply_init(model.head, nn.init.kaiming_normal_)\n\n\nCL_DISTANCE_FUNC = F.pairwise_distance  # btw, cosine similarity did not work well for me -SH\nCL_MARGIN = 4.   # distance beyond which we stop pushing dissimilar items apart. 4 seems to work as well as any other value I tried.\n\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\"\"\"\n    def __init__(self, margin=CL_MARGIN):  # big margin for training means \"always keep pushing dissimilar points away\"\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n    def forward(self, embs, target):\n        emb1, emb2 = split_out(embs)\n        dist = CL_DISTANCE_FUNC(emb1, emb2)\n        pdist, ndist = dist*target, dist*(1-target)\n        loss = 0.5* ((pdist**2) + (F.relu(self.margin-ndist)**2))  # relu serves as the \"hinge\"\n        return loss.mean()\n    \n    \ndef siamese_splitter(m):\n    return L(m.body, m.head).map(params)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n\n\n\n\n\n\n\n\nWe’ll define the following custom way of measuring accuracy, called sim_acc\n\n# custom accuracy metric \ndef sim(p, delta=None): \n    \"where outputs are within delta of each other, they are similar (1), otherwise not (0)\" \n    if delta is None: delta = CL_MARGIN * 0.45  # a bit less than half the margin seems to work ok\n    p0, p1 = split_out(p)\n    return (CL_DISTANCE_FUNC(p0,p1) &lt;= delta)*1\n\ndef sim_acc(pred,targ, delta=None):\n    \"similarity accuracy\"\n    return ( sim(pred, delta=delta) == targ ).float().mean()\n\nDefine the model, freeze it, and run the LRFinder:\n\nlearn = Learner(dls, model, loss_func=ContrastiveLoss(), splitter=siamese_splitter, metrics=sim_acc)\nlearn.freeze() \n\n\nlearn.lr_find()   # for some reason lr_find takes FOREVER.\n\n\n\n\nSuggestedLRs(valley=tensor(0.0132))\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 0.02)   # below, note that sim_acc=0.5 is roughly the same as \"random guessing\"\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nsim_acc\ntime\n\n\n\n\n0\n182.274185\n35.945770\n0.654930\n00:55\n\n\n1\n127.189186\n14.088330\n0.654930\n00:55\n\n\n2\n91.577187\n6.649684\n0.654930\n00:57\n\n\n3\n69.468987\n4.976199\n0.669014\n00:55\n\n\n4\n55.596302\n4.554039\n0.683099\n00:56\n\n\n\n\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-5, 1e-3))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nsim_acc\ntime\n\n\n\n\n0\n15.782226\n4.337954\n0.690141\n00:56\n\n\n1\n15.417539\n4.133821\n0.725352\n00:57\n\n\n2\n14.990416\n4.219871\n0.753521\n01:00\n\n\n3\n14.579235\n4.243847\n0.767606\n00:59\n\n\n\n\n\nSo, the accuracy is not as good as our regular “traditional”/“vanilla” classifier, but it’s still…well…actually…. I’ve run this many times in the course of preparing the post, and I’ve seen it score as high as 95% and as low as 75%. …Hmm. Well, moving on for now.\nNow we’ll add a show_results method:\n\nmyacc_val = 0  # for diagnostically messing with delta\n@typedispatch\ndef show_results(x:SiameseImage, y, samples, outs, ctxs=None, max_n=6, nrows=None, ncols=3, figsize=None, **kwargs):\n    global myacc_val\n    sims = sim(y[2], delta=None if 'delta' not in kwargs else kwargs['delta'])\n    acc = (sims == x[2]).float().mean().item()   # compute accuracy for diagnostic purposes\n    if 'show_acc' in kwargs and kwargs['show_acc']: print(\"accuracy = \",acc)\n    myacc_val = acc\n    if 'just_acc' in kwargs: return\n\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs):\n        pred, targ = sims[i].item(), x[2][i].item()\n        title = f'Actual: {[\"Not similar\",\"Similar\"][targ]} \\n Prediction: {[\"Not similar\",\"Similar\"][pred]}'\n        if pred != targ: title = '***'+title.upper()+'***' # uppercase when we get it wrong\n        SiameseImage(x[0][i], x[1][i], title).show(ctx=ctx)\n\n\nlearn.show_results(max_n=12, delta=2, show_acc=True) #  make max_n large & vary delta to tweak performance\n\n\n\n\naccuracy =  0.78125\n\n\n\n\n\n\n\nNow Let’s Visualize the Embeddings\n\n@patch\ndef siamembed(self:Learner, item, rm_type_tfms=None, with_input=False):\n    return self.predict(item, rm_type_tfms=None, with_input=False)[0]\n\n\nimport plotly.graph_objects as go\n\nnclasses = len(labels)\ndef plot_clusters(labels,dim=3):\n    fig = go.Figure()\n    colors = ['red','green','blue','orange']+['black']*max(nclasses-4,0)\n    for i,l in enumerate(labels):      \n        cluster = np.zeros((len(lbl2files[l]),dim))\n        for j,f in enumerate(lbl2files[l]):         # TODO: this is insanely slow; should use batches instead\n            img = PILImage.create(f)\n            cluster[j] = learn.siamembed(SiameseImage(img, img))[0:3].detach().numpy() # TODO: for now just grab the first 3 coorrdinates, not even PCA\n        fig.add_trace( go.Scatter3d(x=cluster[:,0], y=cluster[:,1], z=cluster[:,2], hovertext=labels[i], name=labels[i], \\\n            mode='markers', marker=dict(size=5, opacity=0.6, color=colors[i])))\n    fig.show()\n    \nplot_clusters(labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\n^TODO: that is the first 3 of 128 dimensions. So it’ll look more “overlappy” than it really is.\n\n\nAnd Now: Do the Zero-Shot!\nGive it the spectacled bears, which this model has never seen before!\n\nlabels = ['grizzly','black','teddy','spectacled']\nscrape_for_me(dl_path, [labels[-1]], search_suffix, erase_dir=False, max_n=300)\n\nsubdirs, path, files = labels, Path(dl_path), []\nfor s in subdirs: files += get_image_files(path/s)\n\nprint(f'{len(files)} files ready to load.')\nprint(f\"files[0] = {files[0]}, files[-1] = {files[-1]}\")\n\nprint(f\"label_func yields {set([label_func(fp) for fp in files])}\")  # sanity check: do we get all the labels we expect?\n\nlbl2files = {l: [f for f in files if label_func(f) == l] for l in labels}\n\nplot_clusters(labels)\n\nspectacled bear: Got 262 image URLs. Downloading...\n   urls =  ['https://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/embed/public/2017/06/28/andean-bear-14218511920.jpg', 'http://4.bp.blogspot.com/-6xZLUXHh8Uc/UUxmEy9QZBI/AAAAAAAAXcY/TMXg3WwS3GA/s1600/Spectacled+Bear+2.jpg', 'https://www.coolearth.org/wp-content/uploads/2016/06/Spectacled-Bear-2.jpg', 'https://s.hdnux.com/photos/01/07/31/01/18714719/5/920x920.jpg', 'https://www.bioexpedition.com/wp-content/uploads/2014/10/Spectacled_Bear_Dave-Pape.jpg', 'http://www.bearsoftheworld.net/images/bears/spectacled_bear_02.jpg', 'https://live.staticflickr.com/7431/9349596216_623a25b335_b.jpg', 'https://wallpapercave.com/wp/wp2676931.jpg', 'http://3.bp.blogspot.com/-Sg8TmR5qh88/UUxmNY4d0HI/AAAAAAAAXcw/dpL2qFHZvp0/s1600/Spectacled+Bear+5.jpg', 'https://wanderlord.com/wp-content/uploads/2017/03/Spectacled-Bear.jpg', 'https://i.natgeofe.com/n/804d9cdb-f8da-449a-93b0-aac7a448b7c0/spectacled-bear_thumb.JPG?w=1200', 'https://img00.deviantart.net/46a0/i/2011/237/1/8/spectacled_bear_by_skarkdahn-d47sr16.jpg', 'https://www.animalphotos.me/mamm1/bear-spec_files/spectacle2.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-5.jpg', 'https://www.activewild.com/wp-content/uploads/2018/03/spectacled-bears-1024x683.jpg', 'https://a-z-animals.com/media/animals/images/original/spectacled_bear4.jpg', 'http://www.rainforest-alliance.org/sites/default/files/2016-09/spectacled-bear-header.jpg', 'https://lh6.googleusercontent.com/-Sptgd7fYIqY/TYk4YVRbndI/AAAAAAAAB7E/MYqqcMu-isw/s1600/Spectacled_Bear_Tennoji_2.jpg', 'https://pre00.deviantart.net/e486/th/pre/i/2012/023/6/5/spectacled_bear_by_shimmichan-d4nd6wh.jpg', 'https://www.vanwageningen.net/i/upload/2014/10/29/20141029132015-bc5e40e5-me.jpg', 'https://live.staticflickr.com/6124/6014296334_89fcc4133a_b.jpg', 'https://farm4.staticflickr.com/3606/3292437597_527cab83ef_z.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-1002804-spectacled-bear-face.jpg', 'https://focusingonwildlife.com/news/wp-content/uploads/003_MTF_0221_spectacled.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear.jpg', 'https://res.cloudinary.com/dk-find-out/image/upload/q_70,c_pad,w_1200,h_630,f_auto/MA_00155387_i4leyu.jpg', 'https://colombiareports.com/wp-content/uploads/2017/07/spectacled_bear.jpg', 'https://i.ytimg.com/vi/wWgdrMOH8d4/maxresdefault.jpg', 'http://1.bp.blogspot.com/-qAm-_Q_k5I0/TfA4AvZt_1I/AAAAAAAABNM/Ov-m-olxu_I/s640/Spectacled-Bear.jpg', 'https://1.bp.blogspot.com/-hq_blqxeAVk/XsFZuPiS06I/AAAAAAAADps/lVr1KiOLbLEfjVO34nufzWmMdo8DJXsOQCPcBGAYYCw/w1200-h630-p-k-no-nu/Spectacled_Bear.jpg', 'http://tenrandomfacts.com/wp-content/uploads/2016/07/Spectacled-Bear.jpg', 'https://live.staticflickr.com/8393/8696456854_49f079714b_b.jpg', 'https://happygringo.com/wp-content/uploads/2020/02/portada-1.jpg', 'https://static1.squarespace.com/static/56a1a14b05caa7ee9f26f47d/t/57390f5520c6471cf068c6cc/1463357302812/', 'https://www.zoochat.com/community/media/spectacled-bears.112852/full?d=1282939786', 'http://3.bp.blogspot.com/-EGI8NqbPcvo/UUxmUJ6t1qI/AAAAAAAAXdA/fDUqD60Tv00/w1200-h630-p-k-no-nu/Spectacled+Bear.jpg', 'https://www.zoochat.com/community/media/spectacled-bear.109154/full?d=1281223819', 'https://nationalzoo.si.edu/sites/default/files/styles/1400x700_scale_and_crop/public/animals/andean-bear-007.jpg?itok=MJoAKHeV', 'https://colombiareports.com/wp-content/uploads/2017/07/andean-bear-2-2017-01-31.jpg', 'https://img.izismile.com/img/img3/20100513/640/cute_spectacled_bear_640_11.jpg', 'https://mma.prnewswire.com/media/1030258/The_Wild_Animal_Sanctuary_Rescued_Spectacled_Bear.jpg?p=facebook', 'https://www.zoochat.com/community/media/spectacled-bear.108090/full?d=1280694438', 'https://natureandculture.org/wp-content/uploads/2019/03/Cloud-Forest-Andean-Bear-768x432.jpg', 'http://featuredcreature.com/wp-content/uploads/2012/10/6a010535647bf3970b0134830b6483970c-800wi2.jpg', 'https://lh6.googleusercontent.com/-Sptgd7fYIqY/TYk4YVRbndI/AAAAAAAAB7E/MYqqcMu-isw/w1200-h630-p-k-no-nu/Spectacled_Bear_Tennoji_2.jpg', 'https://live.staticflickr.com/2900/14021204319_c4d111529b_b.jpg', 'http://theamazonianassignment.weebly.com/uploads/1/2/1/3/12136910/944137409_orig.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-ecuadorian-andes-mountains-328x562.jpg?c14113', 'http://norwichsciencefestival.co.uk/wp-content/uploads/sites/4/SavingSpectacledBear1200x500_1200x500_acf_cropped.jpg', 'https://www.parkgrandhydepark.co.uk/blog/wp-content/uploads/2017/05/shutterstock_492598162.jpg', 'https://www.apex-expeditions.com/wp-content/uploads/2015/04/04-Spectacled-Bear-Papallacta-%C2%A9-Jonathan-Rossouw.jpg', 'https://66.media.tumblr.com/9decce76250961499fe7054e55a907b2/tumblr_omltex2dq01qg5f3ho1_1280.jpg', 'http://i1.treknature.com/photos/2959/bear_0111111.jpg', 'https://nationalzoo.si.edu/sites/default/files/animals/andean-bear-001.jpg', 'https://live.staticflickr.com/4103/4831613245_311f43d099_b.jpg', 'https://imgs.mongabay.com/wp-content/uploads/sites/20/2018/06/15124317/MTF_0409_golden_spectacled_bear_%C2%A9michael_tweddle.jpg', 'https://img.izismile.com/img/img3/20100513/640/cute_spectacled_bear_640_13.jpg', 'http://farm4.staticflickr.com/3419/3774261161_577e09be2c_z.jpg?zz=1', 'http://2.bp.blogspot.com/-vdZv8wGWMdo/UUxmNCvsvhI/AAAAAAAAXco/H5Pzgk5dtOk/s1600/Spectacled+Bear+6.jpg', 'https://www.zoochat.com/community/media/spectacled-bear.342492/full?d=1479072515', 'https://www.si.edu/sites/default/files/newsdesk/photos/20100422-040mm.jpg', 'http://1.bp.blogspot.com/-G-M1STXREJI/UUxmFpUBSoI/AAAAAAAAXcg/qvb2WX5SW0s/s1600/Spectacled+Bear+3.jpg', 'https://www.wisebirding.co.uk/wp-content/uploads/Spectacled-Bear2.jpg', 'https://www.sustainability-times.com/wp-content/uploads/thumbs/AndeanBear_ZN-38o7iv74jqj9grl16xtssg.jpg', 'https://media.sciencephoto.com/image/c0175188/800wm/C0175188-Spectacled_Bear.jpg', 'https://wallpapercave.com/wp/wp2676880.jpg', 'http://www.stlzoo.org/files/1014/0017/1158/ANDEANBEAR9410WINKELMAN2.jpg', 'https://2.bp.blogspot.com/-KBaTMMdVHoY/T_A-HQlM8mI/AAAAAAAAMHE/cBonrzV6sfY/s1600/Spectacled+Bear2.jpg', 'https://www.theanimalfiles.com/images/spectacled_bear_1.jpg', 'https://www.mindenpictures.com/cache/pcache2/00542362.jpg', 'https://www.tourradar.com/days-to-come/wp-content/uploads/2016/01/Spectacled-bear.jpg', 'https://www.zoochat.com/community/media/spectacled-bear.250657/full?d=1393011952', 'https://images.fineartamerica.com/images-medium-large-5/2-spectacled-bear-in-andean-foothills-peru-tui-de-roy.jpg', 'https://images.robertharding.com/preview/RM/RH/HORIZONTAL/1127-16827.jpg', 'https://newyork.cbslocal.com/wp-content/uploads/sites/14578484/2017/05/julie-larsen-maher_1638_andean-bear-and-cub_qz_05-01-17.jpg?w=1024&h=0&crop=1', 'http://2.bp.blogspot.com/-1jh4WfVEyx4/T_A-OoNHGFI/AAAAAAAAMHM/Wm41l9WFfgs/s1600/Spectacled+Bear3.jpg', 'http://blog.nationalgeographic.org/wp-content/uploads/2009/02/Spectacled-Bear-picture-2.jpg', 'http://2.bp.blogspot.com/-und_0cRk_O8/URGtY1vnHGI/AAAAAAAAARQ/9aAIlFO437s/s1600/spectacled-bear3.jpg', 'http://www.rainforest-alliance.org/sites/default/files/styles/750w_585h/public/2016-09/spectacled-bear-header.jpg?itok=0V7_KGpe', 'https://wtop.com/wp-content/uploads/2015/03/16619693699_5bd5ac78d4_b.jpg', 'https://i.pinimg.com/originals/90/88/80/908880871027303cfe43c19ea4655e75.jpg', 'https://www.sustainability-times.com/wp-content/uploads/2019/06/AndeanBear_ZN.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Habitat.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-sitting-820x377.jpg?d8bc0c', 'http://seethewild.org/wp-content/uploads/2016/05/features.jpeg', 'https://i.pinimg.com/originals/24/eb/4e/24eb4e758b8bf55ef1c8a0d10e3d6b4a.jpg', 'https://www.discoveranimals.co.uk/wp-content/uploads/2014/09/spectacled_bear.jpg', 'https://www.shropshirestar.com/resizer/GqPX0-puTDFAn5Ff_V9fBEpM2Jk=/1000x0/filters:quality(100)/arc-anglerfish-arc2-prod-shropshirestar-mna.s3.amazonaws.com/public/4PL5ZCNSKVBPVK6UPFUAPHLMUI.jpg', 'http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Spectacled_Bear.jpg/436px-Spectacled_Bear.jpg', 'https://d.ibtimes.co.uk/en/full/1613674/spectacled-bear.jpg?w=400', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/1/spectacled-bear-portrait-rose-santuci-sofranko.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-cubs.jpg', 'http://1.bp.blogspot.com/-Srm8EQe_xjU/TwK6I75A_jI/AAAAAAAAEt8/jQqGqztNuls/s1600/spectacled-bear-spectacled-bear.jpg', 'https://st2.depositphotos.com/4380587/11651/i/950/depositphotos_116510914-stock-photo-picture-of-a-spectacled-bear.jpg', 'http://kids.sandiegozoo.org/sites/default/files/2017-07/andean-bear-tree.jpg', 'https://southafricatoday.net/wp-content/uploads/2018/07/1532665810_1-1024x1000.jpg', 'https://wallpapercave.com/wp/wp2676970.jpg', 'https://upload.wikimedia.org/wikipedia/en/c/ce/Spectacled-bear.jpg', 'https://i.pinimg.com/originals/0b/24/7b/0b247b1295ab238a6a9873bfc9465d45.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/6093558068_e2c0675823_o.jpg', 'http://4.bp.blogspot.com/-zHQPWNws7VQ/TwK7vuoleYI/AAAAAAAAEuI/iZJGb0R9CEQ/s1600/spectacled_bear-spectacled_bear.jpg', 'http://www.ecns.cn/hd/2020/05/18/e41ef93a186c40a38726189b8901f6f3.jpg', 'https://lh3.googleusercontent.com/-TijTJ1kPu7U/TYk4Z-b-flI/AAAAAAAAB7M/X8njDXrfBAk/s1600/spectacled-beard.jpg', 'https://vignette.wikia.nocookie.net/vsbattles/images/0/0c/Spectacled_bear_vector_by_phoenixtdm_dcek9fs-fullview.png/revision/latest?cb=20190617023302', 'https://4.bp.blogspot.com/-xKDgjZh9q_Y/Ux2oRtCuyXI/AAAAAAAAMsM/PEgp-j44bS8/s1600/Spectacled-Bear.jpg', 'https://www.naturalhistoryonthenet.com/wp-content/uploads/2017/01/Spectacled-Bear.jpg', 'https://www.saczoo.org/wp-content/uploads/2017/01/Spectacled-Bears.jpg', 'https://images6.alphacoders.com/406/406817.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Baby-Spectacled-Bear.jpg', 'http://www.mammalwatching.com/wp-content/uploads/2018/01/EC-square-fox.jpg', 'https://www.natureplprints.com/p/729/lucas-bustamante/spectacled-andean-bear-tremarctos-ornatus-maquipucuna-15316013_73045.jpg', 'http://4.bp.blogspot.com/-VxZ-1v7UTjY/UEAGEWmRNTI/AAAAAAAAk5w/iLr_Y0Pnrtg/s1600/120813+Spectacled+Bear+small.jpg', 'https://www.bearsinmind.org/Uploaded_files/Content/brilbeer.a3cb36.JPG', 'http://newyorktrendnyc.com/wp-content/uploads/2013/11/andean-bear.jpg', 'https://i.pinimg.com/originals/5b/88/ab/5b88ab3ff6b9d74aea2e1d94e521bc72.jpg', 'http://www.zooborns.com/.a/6a010535647bf3970b0192aac4e2da970d-500wi', 'https://home.bt.com/images/spectacled-bears-and-snow-leopards-among-creatures-getting-funding-boost-136427641293802601-180606124211.jpg', 'http://2qn4774cz98e17p2h549htbt.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Male-Andean-Bear-face.jpg', 'https://nationalzoo.si.edu/sites/default/files/animals/andeanbear-006.jpg', 'https://live.staticflickr.com/250/515823267_281f36649e_b.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/5409184198_ccc4b2c5a4_o.jpg', 'https://www.worldatlas.com/r/w728/upload/73/02/b4/shutterstock-118635484-min.jpg', 'https://4.bp.blogspot.com/-8Na0W3PKG-U/UpRE_rP_gXI/AAAAAAAABZI/rU-WV9XxIIE/s1600/Spectacled-Bear-gaert.JPG', 'https://www.pitara.com/wordpress/wp-content/uploads/2001/02/spectacled-bear.jpg', 'http://3.bp.blogspot.com/-EGI8NqbPcvo/UUxmUJ6t1qI/AAAAAAAAXdA/fDUqD60Tv00/s1600/Spectacled+Bear.jpg', 'https://www.si.edu/sites/default/files/newsdesk/photos/bernardo-021mm.jpg', 'https://lh6.googleusercontent.com/-cc05A4QkDxc/TYk4S3w1qFI/AAAAAAAAB7A/3vb0t85A7GU/s1600/spectacled_bear_03.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-rocks-820x545.jpg?c14113', 'https://philadelphiazoo.160over90.io/wp-content/uploads/2019/10/Andean.SpectacledBear_Joaquin_0665.jpg', 'https://i.ytimg.com/vi/VXFIYIQgEkg/maxresdefault.jpg', 'https://a4.pbase.com/o6/34/564334/1/70568852.6wuZLzi7.20061119spectacledbear03ccomp.jpg', 'http://www.animalphotos.me/mamm1/bear-spec_files/spectacle1.jpg', 'https://ca-times.brightspotcdn.com/dims4/default/0a24096/2147483647/strip/true/crop/2000x3000+0+0/resize/840x1260!/quality/90/?url=https:%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Faa%2F74%2F245459fb41edb020b50a502e2a3a%2Ft16-0554-017-lg.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-71192249-spectacled-bear.jpg', 'https://farm3.staticflickr.com/2913/14137750328_d101997a4c_z.jpg', 'http://www.realworldholidays.co.uk/blog/wp-content/uploads/2015/12/spectacled-bear1.jpg', 'http://www.facts-about.info/wp-content/uploads/2014/12/Andean-Bear-at-Reid-Park-Zoo.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-2.jpg', 'https://1.bp.blogspot.com/-Tou2SRVCRqw/UpRFIXCbzQI/AAAAAAAABZ8/CTtSSU6GhZI/s1600/Spectacled-Bear.jpg', 'https://img.izismile.com/img/img3/20100513/640/cute_spectacled_bear_640_01.jpg', 'http://www.facts-about.info/wp-content/uploads/2014/12/Andean-Bear1.jpg', 'http://upload.wikimedia.org/wikipedia/commons/6/62/Spectacled_Bear_059.jpg', 'https://media.nbcwashington.com/2019/09/4621594915_9022bfd814_b.jpg?fit=1024%2C702', 'https://philadelphiazoo.org/wp-content/uploads/2019/10/Andean.SpectacledBear_Rosie_3395.jpg', 'https://mma.prnewswire.com/media/1030258/The_Wild_Animal_Sanctuary_Rescued_Spectacled_Bear.jpg?p=publish&w=950', 'https://c1.staticflickr.com/5/4149/4956021357_d3aa0407c1_z.jpg', 'http://images.fineartamerica.com/images-medium-large/spectacled-bear-tremarctos-ornatus-cub-pete-oxford.jpg', 'https://i.pinimg.com/736x/b9/ab/6f/b9ab6f27f50527d8ddd610f0f02e88cb--spectacled-bear-spirit-animal.jpg', 'http://2.bp.blogspot.com/-4bGjmY9K4pk/T_A-Yq5aNGI/AAAAAAAAMHc/pv9gEJ8O6Uo/s1600/Spectacled+Bear5.jpg', 'https://vg5b2ejdwb-flywheel.netdna-ssl.com/wp-content/uploads/2015/06/7oboa0_tu_u.jpg', 'https://resize.hswstatic.com/w_907/gif/types-of-spectacled-bear0.jpg', 'http://1.bp.blogspot.com/-xGs4-W-3iSY/TjaZG6_XnOI/AAAAAAAABUE/AkEJ97L1HQc/s1600/156521.jpg', 'https://www.robertharding.com/watermark.php?type=preview&im=RM/RH/HORIZONTAL/1127-12838', 'https://www.zooportraits.com/wp-content/uploads/2018/05/Spectacled-Bear-Tremarctos-Ornatus.jpg', 'https://i.pinimg.com/736x/fa/40/33/fa403340878f175322c07b860b9d6426--spectacled-bear-bolivia.jpg', 'http://img.izismile.com/img/img3/20100513/640/cute_spectacled_bear_640_03.jpg', 'https://ourplnt.com/wp-content/uploads/2016/03/Spectacled-bear.jpg', 'https://i.pinimg.com/originals/ec/0c/96/ec0c969cd38a1a701a2c1e2a87a96561.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/3956278926_328d1c6a54_o.jpg', 'https://www.rainbowtours.co.uk/upload-files/product-excursions-gallery/EXC-PER-024_1_spectacled-bear.jpg', 'https://2.bp.blogspot.com/-6-AcWMkgynE/VEMIW1x0vtI/AAAAAAAAAEU/xUsNNkFj6rg/s1600/oso.jpg', 'http://cincinnatizoo.org/system/assets/uploads/2014/02/spectaculed-bear.jpg', 'http://3.bp.blogspot.com/-brSuqjGOIG0/URGtpFtyo_I/AAAAAAAAARg/vLE13RVoCsk/s1600/spectacled-bear.jpg', 'https://c2.staticflickr.com/2/1501/24144684635_73b9e79d88_b.jpg', 'https://critter.science/wp-content/uploads/2019/05/sb1.jpg', 'https://reidparkzoo.org/wp-content/uploads/2016/05/andeanbear9768z.jpg', 'https://srcnaut.com/cdn-cgi/image/f=auto,fit=crop,g=0.5x0.5,w=2000,h=1125,q=90,d=1/upload/32/02/df/shutterstock-1188627676.jpg', 'http://i1.chesterchronicle.co.uk/incoming/article8089938.ece/ALTERNATES/s615b/JS50546090.jpg', 'https://conservationnation.org/wp-content/uploads/2020/03/andean-bear-hero.jpg', 'http://seethewild.org/wp-content/uploads/2016/05/dreamstime_15373008-1.jpg', 'https://i1.wp.com/www.bearz.org/en/wp-content/uploads/2015/06/604014_10200989864724976_224585035_n1.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-30167654-spectacled-bear-with-bamboo.jpg', 'https://www.textbooktravel.com/wp-content/uploads/2020/08/Spectacled-Bear-Cub.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Spectacled_Bear_-_Houston_Zoo.jpg/1200px-Spectacled_Bear_-_Houston_Zoo.jpg', 'https://1.bp.blogspot.com/-cloWtyc4dnw/T-5pMXHLd7I/AAAAAAAACog/ebO0-a0x89M/s1600/SpectacledBear.jpg', 'https://www.zoochat.com/community/media/spectacled-bear-at-chester-15-07-12.192796/full?d=1342391273', 'https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1440,f_auto/MA_00155387_i4leyu.jpg', 'http://2.bp.blogspot.com/-PQGmVUDZAhs/TwK5urRX1LI/AAAAAAAAEtw/s569QHCcx8Y/s1600/spectacled+bear.jpg', 'https://bw-1651cf0d2f737d7adeab84d339dbabd3-gallery.s3.amazonaws.com/images/image_2865567/db2b44df10ba2a567a8e5cc34ddfeed0_original.JPG', 'http://etc.usf.edu/clippix/pix/spectacled-bear-with-piece-of-food-in-its-mouth_medium.jpg', 'https://www.activewild.com/wp-content/uploads/2018/03/spectacled-bear-standing-768x1024.jpg', 'https://www.biolib.cz/IMG/GAL/BIG/28527.jpg', 'http://2.bp.blogspot.com/-1mdQ66udYmw/T_A-UjaJk5I/AAAAAAAAMHU/uJw0kPcz1vc/s1600/Spectacled+Bear4.jpg', 'https://www.apex-expeditions.com/wp-content/uploads/2015/04/02-Spectacled-Bear-Chaparri-JR-5.jpg', 'https://www.factzoo.com/sites/all/img/mammals/bears/spectacled-bear-face.jpg', 'http://images.fineartamerica.com/images-medium-large/andean-or-spectacled-bear-tremarctos-philippe-henry.jpg', 'https://i.pinimg.com/736x/ad/b9/c5/adb9c533cfd5c8f292ce21cabae6f2f9--spectacled-bear-baby-animals.jpg', 'https://4.bp.blogspot.com/-6TV6R9HEc-o/T_A98vwyTEI/AAAAAAAAMG8/ffIXmwgDoXw/s1600/Spectacled+Bear.jpg', 'http://cuencahighlife.com/wp-content/uploads/2015/03/chl-bear2.jpg', 'https://cms.prod.nypr.digital/images/27490/fill-1200x650/', 'https://www.worldatlas.com/upload/43/04/5c/shutterstock-92656222-min.jpg', 'https://c1.staticflickr.com/4/3444/3293260140_4257d5b231.jpg', 'https://www.animalfactsencyclopedia.com/images/bear-spectacledcloseinset.jpg', 'https://i.pinimg.com/originals/9b/34/7e/9b347e3e5fb5b58d954ce84aeccdcc42.jpg', 'https://wallpapercave.com/wp/wp2676943.jpg', 'https://www.si.edu/sites/default/files/newsdesk/photos/andean-bear.jpg', 'https://ecosphere.plus/wp-content/uploads/2018/02/Spectacled-bear_LAM0695-Heinz-Plenge-2-1080x540.jpg', 'https://i2-prod.chesterchronicle.co.uk/incoming/article8089934.ece/ALTERNATES/s1200/JS50546081.jpg', 'https://dancantravel.azurewebsites.net/wp-content/uploads/2017/09/DSC09006-2.jpg', 'https://pre00.deviantart.net/fc19/th/pre/i/2015/204/a/b/an_11dv___spectacled_bear_by_szallonntaidalma-d92d0pi.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-4.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-3.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-ogimage.jpg', 'https://images.freeimages.com/images/large-previews/e0f/spectacled-bear-1561333.jpg', 'https://kentattractions.co.uk/wp-content/uploads/2016/09/Spectacled-Bear.jpg', 'https://i.pinimg.com/originals/7c/48/b1/7c48b17f45853a48890e0a32f02aa875.jpg', 'http://gringosabroad.com/wp-content/uploads/2013/11/Male-Andean-Bear-face.jpg', 'https://rangerplanet.com/wp-content/uploads/2019/10/spectacled-bear.jpg', 'https://2.bp.blogspot.com/-1jh4WfVEyx4/T_A-OoNHGFI/AAAAAAAAMHM/Wm41l9WFfgs/s400/Spectacled+Bear3.jpg', 'http://1.bp.blogspot.com/-qM3UoT_SQRk/TiAwjcnmqiI/AAAAAAAAAKE/mQAIjK3XPRg/s1600/spectacled_bear_2.jpg', 'https://www.wisebirding.co.uk/wp-content/uploads/Spectacled-Bear.jpg', 'http://www.factzoo.com/sites/all/img/mammals/bears/spectacled-bear-south-american.jpg', 'https://s3-eu-west-1.amazonaws.com/images.noahsarkzoofarm.co.uk/uploads/tinymce/rasu.jpg', 'https://i.ytimg.com/vi/2TmnhBSyQoo/maxresdefault.jpg', 'https://reidparkzoo.org/wp-content/uploads/2016/05/andeanbear9646z.jpg', 'https://vistapointe.net/images/spectacled-bear-5.jpg', 'https://www.aboutanimals.com/images/spectacled-bear-tree-820x513.jpg?d8bc0c', 'https://standfirst-whitleyaward-production.imgix.net/content/uploads/1996/12/18134457/Tremarctos_ornatus_portrait.jpg?auto=compress,enhance,format&crop=faces,entropy,edges&fit=crop&fm=pjpg&w=1866&h=1400', 'https://c1.staticflickr.com/7/6037/6304684676_8fe4be5717_b.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/5566818433_9c994ec063_o.jpg', 'https://www.bwallpaperhd.com/wp-content/uploads/2018/12/SpectacledBear.jpg', 'https://cms.prod.nypr.digital/images/27460/fill-661x496/', 'https://i.ytimg.com/vi/lLcCBkNDQxs/maxresdefault.jpg', 'http://justfunfacts.com/wp-content/uploads/2017/11/spectacled-bear-eating-fruit.jpg', 'http://img07.deviantart.net/9fc0/i/2010/201/7/6/spectacled_bear_by_citronvert79.jpg', 'https://critterfacts.com/wp-content/uploads/2019/02/s_158118350.jpg', 'https://images.fineartamerica.com/images-medium-large-5/andean-bear-spectacled-bear-houston-zoo-tessa-fairey.jpg', 'https://images.fineartamerica.com/images/artworkimages/mediumlarge/2/spectacled-bear-david-stasiak.jpg', 'https://media.buzzle.com/media/images-en/photos/mammals/bears/1200-598140-spectacled-bear.jpg', 'http://magarticles.magzter.com/articles/8803/216564/58e38ecbbeeb4/Conservation-Insight-Andean-Bear.jpg', 'https://1.bp.blogspot.com/-DAyv4HJRE4E/XjRg-JFM_SI/AAAAAAAAAPU/NUHh82dMHZYSbMEQR3WEJOUXhO08GI4pACLcBGAsYHQ/s1600/baby-animal-photo-05032016-image-056.jpg', 'http://www.rainforest-alliance.org/sites/default/files/styles/large/public/2016-09/spectacled-bear.jpg?itok=BXWidYtz', 'http://www.parkgrandhydepark.co.uk/blog/wp-content/uploads/2017/05/shutterstock_550667422.jpg', 'https://nationalzoo.si.edu/sites/default/files/animals/andeanbear-005.jpg', 'https://bw-1651cf0d2f737d7adeab84d339dbabd3-gallery.s3.amazonaws.com/images/image_2865571/a3195154ccdac7c9efcbed5727462346_original.JPG', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-in-the-Wild.jpg', 'http://bearlegend.com/wp-content/uploads/2012/04/6862037661_30664cf53f_o.jpg', 'https://www.bwallpaperhd.com/wp-content/uploads/2018/12/SpectacledBear-1024x576.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Eating.jpg', 'http://2.bp.blogspot.com/-3UzekS1270M/TiAwko0VrQI/AAAAAAAAAKI/rOXYR8LXLHM/s1600/spectacled_bear_3.jpg', 'http://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bears-300x225.jpg', 'https://animalia.us-east-1.linodeobjects.com/animals/photos/full/1x1/determined.jpg', 'http://cdn4.awwnews.com/images/2015/03/044_3-Spectacled-Bear-Cubs.jpg', 'http://www.peru-breathtaking-trips.com/imgs/Spectacled%20bear.jpg', 'http://lh4.ggpht.com/-2YrERpe5ZeU/UOUBP6Zr8ZI/AAAAAAAAASQ/7oSIuID4iSs/%25255BUNSET%25255D.jpg', 'https://i.pinimg.com/736x/d2/99/fa/d299fa76c3d1a5c0e017514b086ddd9c--spectacled-bear-teddy-bears.jpg', 'https://c2.staticflickr.com/4/3685/8852797884_8f3a84d84c_b_d.jpg', 'https://www.si.edu/sites/default/files/newsdesk/photos/andean_bear_cubs2.jpg', 'https://critterfacts.com/wp-content/uploads/2019/02/s_1083570302_.jpg', 'https://www.worldlandtrust.org/wp-content/uploads/2017/08/RS26943_Ramiro-Mendoza-scr.jpg', 'http://www.kidzone.ws/lw/bears/images/spectacled-bear.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Oso_andino_Porcon.jpg/1200px-Oso_andino_Porcon.jpg', 'https://www.bioexpedition.com/wp-content/uploads/2012/05/Spectacled-Bear.jpg', 'https://www.earlham.ac.uk/sites/default/files/images/articles/Saving-Spectacled-Bear-Deepest-Darkest-Colombia/Saving-Spectacled-Bear-Deepest-Darkest-Colombia-Andean-770.jpg', 'https://featuredcreature.com/wp-content/uploads/2012/10/6a010535647bf3970b0134830b6313970c-800wi2.jpg', 'https://www.apex-expeditions.com/wp-content/uploads/2015/04/01-Spectacled-Bear-Chaparri-JR-2.jpg', 'http://2.bp.blogspot.com/-uKXzB18yA0M/UDBPCtHRm6I/AAAAAAAABrg/cceW2WF1lwQ/w1200-h630-p-k-no-nu/Spectacled-Bear-1.jpg', 'https://www.animalspot.net/wp-content/uploads/2015/12/Spectacled-Bear-Images.jpg', 'https://ptes.org/wp-content/uploads/2018/12/Andean-bear_credit-Tashkin-Meza-Andean-Bear-1000.jpg', 'http://www.telesurtv.net/export/sites/telesur/img/news/2015/11/27/spectacled_bear_2.jpg_1167511010.jpg', 'https://www.worldlandtrust.org/wp-content/uploads/2017/07/RS4817_Spectacled-Bear-2007-Fundacion-Ecominga-Candelaria-Reserve-c-Lou-Jost-1.jpg', 'http://etc.usf.edu/clippix/pix/spectacled-bear_medium.jpg']\n    Images downloaded.\nAfter checking images for issues, 958 (total) images remain.\n958 files ready to load.\nfiles[0] = scraped_images/grizzly/00000143.jpg, files[-1] = scraped_images/spectacled/00000003.jpg\nlabel_func yields {'teddy', 'grizzly', 'black', 'spectacled'}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n            \n                \n            \n            \n            \n        \n\n\n\n\nTODO: Woops! Forgot to score how the model did on the spectacled bears! Will add that soon. ;-) Watch this space."
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#discussion",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#discussion",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "3. Discussion",
    "text": "3. Discussion\nFrom this example, it looks like the contrastive loss method is worse than traditional classification in a couple ways: it requires more data (or else the model tends to overfit), it runs slower, and it’s less accurate. (And we completely left out the challenge of training CL models which typically need to have their positive and negative examples “mined” to find “hard” cases so that the model “learns” more per iteration.) Why would anyone ever want to use it?\nWell, we already mentioned the use case of “produces semantically meaningful embeddings” that can then be used for other downstream tasks. For example, word embeddings like Word2Vec or GloVe can be used for lots of NLP tasks. Jeremy Howard in the FastAI course shows how to embeddings of tabular data and how that can work better and be more meaningful than one-hot encoded tabular data.\nFurthermore, regarding accuracy: OpenAI’s CLIP became SOTA on ImageNet in early 2020! They used a TON of pre-training data, and ended up classifying images via contrastive losses better than any other method!\nAre contrastive loss methods right for you and your use case, and your dataset? I don’t know. Hopefully after this, you at least have some clue of how to implement such a thing, and how it compares and constrasts with the sort of vanilla classification most people who work with neural networks are used to."
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#fastai-details-how-i-wrote-my-first-callback",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#fastai-details-how-i-wrote-my-first-callback",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "4. FastAI Details: How I Wrote My First Callback",
    "text": "4. FastAI Details: How I Wrote My First Callback\nI wrote a Callback to use my visualization routines. It’s generic enough that you could easily pass in some other visualization routine – in fact, mine is generic enough that I must assume someone else (LOL, as in Jeremy) has probably already written one at least as good, but this one is mine!\nThe basic structure of the callback is very simple. In the following I’ve removed all the URL-bookkeeping so you can see the structure:\n\nclass VizPreds_Structure(Callback):\n    \"\"\"Structure of my Progress-like callback: call the bokeh triangle plot (or some other routine) \n    with each batch of training. Scroll down to next code block for 'full' VizPreds routine.\"\"\"\n    order = ProgressCallback.order+1    # run it after the main fastai ProgressCallback\n    def __init__(self,\n        method='auto',   # method is a plotting function, must have a .do_plot(preds,targs) and not mind unexpected kwargs. Or 'auto'\n        activation=F.softmax, # PyTorch activation for preds. For trad. classification, 'preds' are logits & need softmax. or None\n    ):\n        store_attr()    # one of my favorite fastai routines ever, sets all the \"self.\" vars\n\n    def before_fit(self):       # set up stuff \n        if self.method=='auto': # added the 'auto' thing so it choose which default method to use based on num. of dimensions\n            self.method = TrianglePlot2D_Bokeh if len(self.learn.dls.vocab) &lt;= 3 else TrianglePlot3D_Plotly\n\n    def before_epoch(self):           # remove our accumulated info over batches\n        if not self.learn.training:   # only operated on validation datasets\n            self.learn.viz_preds, self.learn.viz_targs = None, None # stuff that's gonna get plotted\n\n    def after_batch(self, **kwargs):  # make predictions after each val batch and accumulate them to plot at the end\n        if not self.learn.training:   # only operated on validation datasets\n            with torch.no_grad():\n                preds = self.activation(self.learn.pred, dim=1) if (self.activation is not None) else preds \n                preds, targs = [x.detach().cpu().numpy().copy() for x in [preds, y]] # Uh...I probly don't need \"copy\" do I?\n                self.learn.viz_preds = preds if self.learn.viz_preds is None else np.vstack((self.learn.viz_preds, preds))\n                self.learn.viz_targs = targs if self.learn.viz_targs is None else np.hstack((self.learn.viz_targs, targs))\n\n    def after_epoch(self):          \n        if not self.learn.training:\n            print(f\"Epoch {self.learn.epoch}: Plotting {len(self.learn.viz_targs)} (= {len(self.learn.dls.valid.items)}?) points:\")\n            # call whatever the plotting method is and make the plot\n            self.method(self.learn.viz_preds, self.learn.viz_targs, labels=self.dls.vocab,\n                comment=f'Epoch {self.learn.epoch}', thumb_height=self.thumb_height).do_plot()\n\nThat’s the idea. If you only cared about plotting the very last batch of predictions, you could put all that code currently in after batch into the start of after_epoch – which is how I had it initially until I started wondering, “Hey where’s the rest of my validation data?!”\nIt’s important that this viz routine gets registered as a callback for the fit, not the Learner. If you make a callback for the Learner, it will be executing lots of times when you don’t want it to, such if/when you call the LRFinder. Don’t do that.\nIn order to implement the mouseover-image-thumbnail preview feature that my mrspuff routine TrianglePlot2D_Bokeh provides, there’s a bunch of extra bookkeeping needed to generate and then hang on to the thumbnail URLs. If we include all that, then the full Callback is below.\n\nclass VizPreds(Callback):\n    \"\"\"Progress-like callback: call the bokeh triangle plot (or some other routine) \n    with each batch of training\"\"\"\n    order = ProgressCallback.order+1    # run it after the main fastai ProgressCallback\n    def __init__(self,\n        method='auto',   # method is a plotting function, must have a .do_plot(preds,targs) and not mind unexpected kwargs. Or 'auto'\n        activation=F.softmax, # PyTorch activation for preds. For trad. classification, 'preds' are logits & need softmax. or None\n        gen_urls=True,   # generate thumbnail urls (if they don't exist) (2D_Bokeh only)\n        thumb_height=80, # lets you vary the height of image thumbnails (2D_Bokeh only)\n        force_new_urls=False, # if False, keeps whatever thumbnail urls are in learn.dls (2D_Bokeh only)\n    ):\n        store_attr()\n        self.we_made_the_thumbs = False  # so that we don't force-gen new urls a 2nd time after un-freezing\n\n    def before_fit(self):   # set up the plotting method and generate thumbnail URLs if applicable\n        if self.method=='auto':\n            self.method = TrianglePlot2D_Bokeh if len(self.learn.dls.vocab) &lt;= 3 else TrianglePlot3D_Plotly\n\n        dv = self.learn.dls.valid       # dv is just a shorthand for \"dls.valid\"\n        if self.gen_urls and self.method==TrianglePlot2D_Bokeh and (('url_dict' not in dir(dv)) or self.force_new_urls) and not self.we_made_the_thumbs:\n            self.we_made_the_thumbs = True  # as in \"I made the donuts\", https://www.youtube.com/watch?v=IYRurPB4WA0\n            #url_dict maps filenames to urls.  if you're on regular-Jupyter instead of Colab, you can use filenames as urls \n            self.learn.dls.valid.url_dict = dict(zip(dv.items, get_thumb_urls(dv.items, size=(self.thumb_height,self.thumb_height))))\n\n    def before_epoch(self):\n        if not self.learn.training:\n            self.learn.viz_preds, self.learn.viz_targs = None, None\n\n    def after_batch(self, **kwargs): # make predictions after each val batch and accumulate them to plot at the end\n        if not self.learn.training:\n            with torch.no_grad():\n                preds = self.activation(self.learn.pred, dim=1) if (self.activation is not None) else preds \n                preds, targs = [x.detach().cpu().numpy().copy() for x in [preds, y]] # remove copy()?\n                self.learn.viz_preds = preds if self.learn.viz_preds is None else np.vstack((self.learn.viz_preds, preds))\n                self.learn.viz_targs = targs if self.learn.viz_targs is None else np.hstack((self.learn.viz_targs, targs))\n\n    def after_epoch(self):  # now that we have all our pred/targ batches and urls ready (if applicable), plot!\n        if not self.learn.training:\n            print(f\"Epoch {self.learn.epoch}: Plotting {len(self.learn.viz_targs)} (= {len(self.learn.dls.valid.items)}?) points:\")\n            urls = None\n            if self.method==TrianglePlot2D_Bokeh:\n                dv = self.learn.dls.valid       # dv is just a shorthand for \"dls.valid\"\n                urls = [dv.url_dict[f] for f in dv.items] if 'url_dict' in dir(dv) else dv.items # image urls or filenames\n            self.method(self.learn.viz_preds, self.learn.viz_targs, urls=urls, labels=self.dls.vocab,\n                comment=f'Epoch {self.learn.epoch}', thumb_height=self.thumb_height).do_plot()\n\nTrianglePlot2D_Bokeh is my mrspuff routine that allows for image-thumbnail previews, and these require that we generate and hang on to URLs that point to the images. (If you were to cut out all the URL-related stuff, this callback is really trivial.)\nOn regular-Jupyter (on your local machine, or on Paperspace Gradient, or Binder, etc.) the Jupyter server lets us just use filenames and it converts them all to URLs automatically for us! But on Colab, we need to generate the “sharing URLs”, and then remember those. So that’s what a lot of the extra verbiage in this callback is for: it’s about managing the image thumbnail URLs (if we’re plotting in 2D, i.e. 3 classes where we’re in the plane of the “triangle” that forms).\nIf there are in 4 or more dimensions, then we automatically trigger the Plotly-based plotter, which will automatically PCA-down to a 3D projection – so for 4 classes, we get a 3D tetrahedron. And there are no thumnail URLs because sadly Plotly doesn’t support those."
  },
  {
    "objectID": "posts/2021-06-17-contrasting-contrastive-loss.html#acknowledgements",
    "href": "posts/2021-06-17-contrasting-contrastive-loss.html#acknowledgements",
    "title": "Typical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI",
    "section": "Acknowledgements:",
    "text": "Acknowledgements:\nSpecial thanks to Zach Mueller for assistance in interfacing with fastai!"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html",
    "href": "posts/2021-05-04-the-joy-of-3d.html",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "",
    "text": "(This blog post is an extended treatment of a talk I recently gave. To see the slides for the talk, click here.)"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#intro",
    "href": "posts/2021-05-04-the-joy-of-3d.html#intro",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Intro",
    "text": "Intro\nWe’re going to explore the difference between what I term “traditional” neural network (NN)-based classification and so-called “zero-shot” (or “few shot”) classifiers that rely on embedding semantically meaningful features as clusters in space by means of contrastive losses. These “zero-shot” (or “few shot”) or “contrastive loss” methods are increasingly prevalent in the literature, and have the nice property that, unlike traditional ML classifiers, they don’t need to be re-trained whenever new classes are introduced. If we want to understand these embedding-based / zero-shot / contrastive loss methods, it will be helpful to first consider traditional classification as an embedding method of it own.\nThere is also a strong pedagogical point that I wish to make in this post. Often in teaching ML, many authors will spend some time on binary classification via logistic regression (see my post “Naughty by Numbers: Classifications at Christmas”) and then jump immediately into multi-class classification where the number of classes is 10, or 1000, or 1000 and up. There is an opportunity that is being passed over. The opportunity is visualization, and what is being passed over is the special case of three classes. (Or, as we’ll see, we can squeeze an extra 4th class.)"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#viz-matters",
    "href": "posts/2021-05-04-the-joy-of-3d.html#viz-matters",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Viz Matters",
    "text": "Viz Matters\nVisualization is an important part of the teaching process as well as for researchers wanting to understand their data. Much of my own teaching work has involved building data visualization apps for students to use in learning acoustics, and seeing Yang Hann Kim’s speech when he received on Rossing Prize in Acoustics Education for his visualization efforts only further inspired me to continue developing such tools for students and instructors. (cf. The Physics Teacher featured my “Polar Pattern Plotter” app on the cover of its February 2018 issue.)\nHumans cannot visualize beyond 3 dimensions, so problems involving more than 3 semantic features invariably rely on projection methods such a Principal Component Analysis (see my blog post, “PCA from Scratch”) or nonlinear embedding methods like t-SNE or UMAP. The problem with PCA is that projected data points tend to overlap, and with the latter methods twist and distort the space so much that the global structure is completely obfuscated. But in 3D the representations are exact!\n\nMath point, for those who are interested: When we move from two classes to more than two, we typically switch from the logistic sigmoid activation and Binary Cross Entropy loss to softmax activation and Cross Entropy (~ K-L divergence) loss. Softmax contains more complexity than sigmoid (even if the latter is just two-class softmax with a coordinate transformation, and along each axis softmax acts like a sigmoid), yet for the case of 3 classes softmax contains all the complexity of higher numbers of classes, while still being visualizable. \n\nWe’re going to make use of a little code library I’m in the process of putting together called mrspuff! It’s geared toward teaching via visualization and running on Google Colab, and (increasingly, as I learn) built to work on & with fast.ai."
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#dimensions-and-embeddings",
    "href": "posts/2021-05-04-the-joy-of-3d.html#dimensions-and-embeddings",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Dimensions and Embeddings",
    "text": "Dimensions and Embeddings\nPeople who are not mathematicians, physicists, data scientists, etc. may be unaccustomed to this talk of “dimensions” when dealing with data. Let’s dive in to the specific case of three-class classification. Say we’re developing a computer program to guess (“predict”) whether a given image contains a cat, a dog, or a horse. Traditionally the “ground truth” or “target” values are expressed as “one hot encoded” vectors, such as…\n   cat: (1,0,0)           dog: (0,1,0)         horse: (0,0,1)\nThen given an image of an animal, our neural network model will produce a set of 3 probabilities for each class, say…\n\n\nCode\nimport numpy as np \nfrom mrspuff.viz import *\nfrom mrspuff.utils import *\nfrom mrspuff.scrape import *\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nlabels = ['cat','dog','horse']\ndata = np.array([[0.7,0.2,0.1],[0.15,0.6,0.25],[0.05,0.15,0.8]])\nfor i in range(3):\n    image_and_bars(data[i], labels, CDH_SAMPLE_URLS[i]).show(config = {'displayModeBar': False})\n    print(\"\")\n\n\n\n                                                \n\n\n\n\n\n\n\n\n                                                \n\n\n\n                                                \n\n\n…the goal (of training the neural network model) is to get the predicted values to match up with the target values.\n\n\n\n\n\n\nNote\n\n\n\nwe’ll be using image-based examples, but for everything you’ll see in these posts, similar methods have been applied to audio, text, and tabular data.\n\n\nThese numbers can be viewed as the strength of an attribute in an image, e.g. measures of cat-ness, dog-ness, and horse-ness (or measure of the likelihood of being a cat, dog, or horse, respectively), where a value of 1 means 100% of that property. Notice in each case, the three “class” probabilities add up to 1. This is always the case: probabilities always have to sum up to 1, i.e. 1 is “100% certainty” that gets split among the 3 classes. (This summing to 1 is an important property that we’ll come back to in a bit.)\nOne thing that scientists like to do is take different variables and view them as coordinates of a single point in a multi-dimensional space. So for 3 classes we have 3 coordinates for 3 dimensions. We could make the “cat-ness” prediction probability be the “x” coordinate, and “dog-ness” be the “y” values, and “horse-ness” could be along the “z” axis. Then instead of drawing bar graphs, we could plot points in 3D space, where the coordinates of each point tell us the predictions:\nAll the 3D plots in this post can be rotated & zoomed with the mouse or your finger. Try it!\n\n\n\n\n\n\n                                                \n\n\n(Here we also used the 3 class probabilities to set the R,G,B color values of the points. There’s no new information contained in this; it just looks cool.)\nWhat scientists tend to do is, even in cases where there are more then 3 variables (say, 10), we regard these as dimensions in some fancy abstract mathematical space where the laws may or may not conform to those of our universe – for example, the idea of “distance” may be totally up for grabs. In cases where the number of values is infinite (say, as coefficients in a infinite series, or as a function of a continuous variable) we might even work in infinite dimensions! Often when we talk like this, it doesn’t mean that we’re actually picturing geometrical spaces in our heads – we can’t, for anything beyond 3 dimensions – but it’s a handy way of encapsulating a particular way of viewing the data or functions involved. And sometimes we do try to see what kinds of geometrical insights we can glean – which is what we’re going to do here!\nRemember when we said that the individual class probabilities have to add up to 1? Look what happens when we plot a lot of such points…\n\n\nCode\nprob, targ = calc_prob(n=400)\nTrianglePlot3D_Plotly(prob, targ=None, labels=labels, show_bounds=False).do_plot()\n\n\n\n                                                \n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though these are points in 3D space, they make up a triangle which lies along a plane – a 2D :subspace” of 3D. This is a consequence of having the “constraint” that all class probabilities add up to 1.\n\n\nWe can color the points by their expected class values by choosing the triangle point (or “pole”) that they’re nearest to – i.e. by which “bar” is largest among the class probabilities. And we can include the boundaries between classes:\n\n\nCode\nTrianglePlot3D_Plotly(prob, targ=targ, labels=labels, show_bounds=True).do_plot()"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#showing-3d-3-class-classification-in-2d",
    "href": "posts/2021-05-04-the-joy-of-3d.html#showing-3d-3-class-classification-in-2d",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Showing (3D) 3-Class Classification in 2D",
    "text": "Showing (3D) 3-Class Classification in 2D\nSince these points lie along a plane, we can change coordinates and just use a 2D plot instead of a 3D plot.\n\n(Optional) Math Trivia: Typically this would involve calculating a coordinate transformation either by hand or using something like PCA to do it for us, but in this case there’s a simple “hack” transformation that will get us from \\(x\\), \\(y\\), and \\(z\\) in 3D to our 2D coordinates \\(x'\\) and \\(y'\\): \\[ x' = y - x,\\ \\ \\ \\ \\ \\ \\ y' = z \\]\n\nIn a 2D version of our triangle plot, we can even enable “image tooltips” so that when the mouse hovers over a datapoint, you can see the image it represents:\n\n\nCode\nfrom duckduckgo_search import DDGS\n\ndef grab_urls(targ, labels=['cat','dog','horse']):\n    \"\"\"grabs a set of urls, in order of images that match the labels corresponding to targets\"\"\"\n    dim = targ.max()+1\n    url_store = [[] for t in range(dim)]\n    for t in range(dim): # for each set of targets, scrape that many urls for the label\n        with DDGS() as ddgs:\n            label, n = labels[t], np.sum(targ == t )# count how many of each target there are\n            search_results = ddgs.images(f'{label}', max_results=n)\n            urls = [next(search_results).get(\"image\") for _ in range(n)]\n            #print(\"'hey, urls = \",urls)\n            url_store[t] = urls\n    return [ url_store[targ[t]].pop(0) for t in range(len(targ)) ] # supply a url matching each target\n\n\n\nurls = grab_urls(targ, labels)\nTrianglePlot2D_Bokeh(prob, targ=targ, labels=labels, show_bounds=True, urls=urls).do_plot()"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#how-traditional-nn-classification-training-proceeds",
    "href": "posts/2021-05-04-the-joy-of-3d.html#how-traditional-nn-classification-training-proceeds",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "How Traditional NN Classification Training Proceeds",
    "text": "How Traditional NN Classification Training Proceeds\nWhen we start training our classifier, the data (points) get mapped all over the place; it’s a big jumble. The classifier will ultimately be scored by how many points lie on the “correct side of the line” for the class boundaries, but that’s a discontinuous (either-or) criterion that’s no good for training neural networks. So instead we use a loss function and a gradient descent on this loss function to try to minimize the distance from the mapped point to the “pole” of the target class point. In other words, training proceeds by trying to collapse all the data points onto the 3 (or 4) points corresponding to 100% certainty about each class prediction: The following a cartoon example time-lapse of ten training steps (we’ll show real NN training in Part 2 of this blog series):\n\n\nCode\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nprobs, tmp = calc_prob(n=400, s=2.2)\ntargs = np.random.randint(3,size=probs.shape[0])\ntargs_3 = one_hot(targs)   # not used for plotting but for compiting gradients\nmaxsteps = 10\n\ndef sequence(step):\n    lr, grad = 1/maxsteps, targs_3-probs\n    TrianglePlot2D_MPL(probs+step*lr*grad, targ=targs, labels=labels, show_bounds=True, comment=f'Step {step+1}:').do_plot()\n\n# Could be interactive fun in Jupyter/Colab, but not easy to do in the blog:\ndo_interact = False \nif do_interact:\n    print(\"Move the slider below to advance the training step.\")\n    print(\"(Note, this is just a 'cartoon' for now; to see actual NN training steps, wait until post Part 2.\")\n    interact(sequence, step=widgets.IntSlider(min=0,max=19,step=1,value=0));\nelse:\n    for step in range(maxsteps):\n        sequence(step) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the training proceeds, it tries to get the groups of data points to collapse to single locations at each target “pole”."
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#loss-vs.-accuracy",
    "href": "posts/2021-05-04-the-joy-of-3d.html#loss-vs.-accuracy",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Loss vs. Accuracy",
    "text": "Loss vs. Accuracy\nThis method of visualization also allows us to visually “see” the concepts of loss and accuracy. For 3-classes with a softmax activation and categorical cross-entropy, the loss is nearly linear in the difference between the prediction and the target, i.e.:\n\nLoss: ~distance from target\n\nThis is a continuous variable, which makes it suitable for training models via gradient descent.\nIn contrast to loss, accuracy is how many classifications the model gets correct, expressed as a percentage of the total number of data points. This is determined by what side of the decision boundary each prediction is on.\n\nAccuracy: % of points on the correct side of decision boundary (discontinuous)\n\nIt is instructive to view two configurations of data points with nearly identical loss values but wildly different accuracies:\nNearly identical losses:\n\n\nCode\n# generate and save images that we'll load in the next cell \n!mkdir images\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# generate data along boundaries\ndef gen_bound(x, y, z, n=20, ind0=1): # ind0=1 skips the \"first point\"\n    return np.linspace(np.array([x[0],y[0],z[0]]), np.array([x[1],y[1],z[1]]), num=n+ind0)[ind0:]\n  \ndef gen_bound_data(n_per=20, ind0=0):\n    bdata = np.zeros((n_per*3,3))\n    bdata[:n_per] = gen_bound(x=[0.333,0.5], y=[0.333,0.5], z=[0.333,0], n=n_per, ind0=ind0) \n    bdata[n_per:2*n_per] = gen_bound(x=[0.333,0], y=[0.333,0.5], z=[0.333,0.5], n=n_per, ind0=ind0)\n    bdata[-n_per:] = gen_bound(x=[0.333,0.5], y=[0.333,0], z=[0.333,0.5], n=n_per, ind0=ind0)\n    return bdata\n\ndef gen_near_bound_data(n_per=50, scale=7, eps=0.01):\n    bdata = gen_bound_data(n_per=n_per)\n    lower, right, left = bdata[0:n_per,:], bdata[n_per:2*n_per,:], bdata[-n_per:,:]\n\n    # shift data a bit\n    lower_catty = softmax( scale*(lower+np.array([eps,0,0])) )\n    lower_doggy = softmax( scale*(lower+np.array([0.0,eps,0])) )\n\n    left_catty = softmax( scale*(left+np.array([eps,0,0])) )\n    left_horsey = softmax( scale*(left+np.array([0,0,eps])) )\n\n    right_horsey = softmax( scale*(right+np.array([0,0,eps])) )\n    right_doggy = softmax( scale*(right+np.array([0,eps,0])) )\n\n    return np.vstack((lower_catty, lower_doggy, left_catty, left_horsey, right_horsey, right_doggy))\n\n# move boundary a bit toward the \"correct\" side\neps = 0.007\nacc_data = gen_near_bound_data(eps=eps)\nbtarg = np.argmax(acc_data, axis=-1)\nTrianglePlot2D_MPL(acc_data, targ=btarg, show_bounds=True, labels=labels, comment='100% Accuracy:').do_plot()\nplt.savefig(\"images/acc_100.png\")\n\n# move boundary a bit toward the \"wrong\" side (keeping labels the same as before)\ninacc_data = gen_near_bound_data(eps=-eps)\nibtarg = btarg.copy()\nTrianglePlot2D_MPL(inacc_data, targ=ibtarg, show_bounds=True, labels=labels, comment='0% Accuracy:').do_plot()\nplt.savefig(\"images/acc_0.png\")\n\n\nmkdir: images: File exists\n\n\n\n\n\n\n\n\nThus visualization can serve as a valuable teaching tool.\nFurthermore, by letting us track which points are “not where they should be” on a plot, we can “see” the “confusion matrix” typically provided as a classification metric. More on that in Part 2.\n\nAside: Even 4 Classes?\nJust as 3 class probabilities form a triangular 2D subspace (in 3D) that we can then plot in 2D, so too 4 classes form a tetrahedron (a pyramid made up of triangles), which is a 3D shape embedded in 4D space! So if we restrict our attention to this 3D subspace and use a 3D plotting program then we can actually represent 4 classes. Say we add another animal class, say “bird” symbolized by dark-colored points. Then our diagram could look like this:\n\n\nCode\nprob4, targ4 = calc_prob(n=500, s=2.7, dim=4)       # 4d probabilities\nTrianglePlot3D_Plotly(prob4, targ=targ4, labels=labels+['bird'], show_labels=True, show_axes=False, add_poles=True).do_plot()"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#metric-based-embedding-methods",
    "href": "posts/2021-05-04-the-joy-of-3d.html#metric-based-embedding-methods",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Metric-Based Embedding Methods",
    "text": "Metric-Based Embedding Methods\nIn contrast to all this, metric-based embedding methods don’t try to collapse all the data to a predefined set of 3 (or 4, or more) “poles.” Rather, they try to get similar data to end up as points that are near each other, and dissimilar data points far away from each other. This tends to produce “clusters” but they are not (typically) along the “axes” of the space, they’re just “somewhere out there.”\nDeep Learning experts Raia Hadsell, Sumit Chopra, and Yann LeCun described it this way (I’m paraphrasing): Imagine all the data points are connected to each other via special kinds of springs. Similar kinds of points are connected by attractive springs that pull them together. Dissimilar kinds of points are connected by repulsive springs that push them further away from each other — except these repulsive springs are special in that they only apply a force when they’re close together; beyond a certain distance (called the “margin”), no repulsion occurs. (Why this special property is stipulated is a fine point we can get to later).\n\n\n\nThis picture of springs is the essence of a “contrastive loss” function. Unlike traditional NN classification where the loss is based on the “distance” to a “target” (or “ground truth”) value, with these metric-based methods we send in two (or even 3) data points together, and then either let them attract or repel each other, and we do this over and over and over until we reach some stopping criterion. Eventually, what we’ll have is a space that contains clusters of similar points, separated by a “margin” distance that we specify.\n\nContrastive Loss Cartoon Demo\nBelow is a JavaScript animation of the “springs” cartoon above:\n\n\n\n        \n        \n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can think of the margin as the amount of “push.”\n\n\n\n\n\n\n\n\nNote\n\n\n\n(A computational aside) In this scenario, we don’t try to connect every point to every other point at the same time, as in some kind of N-body simulation. That could be computationally expensive and require something like a Barnes-Hut/octtree algorithm to make it efficient. Rather, we’re just randomly pairing two points at a time. Also, other metric learning schemes using “triplet loss” will group three points, setting one to be the “anchor” and then grabbing a positive and a negative example. For more on that, see a recent paper or some code from a couple years ago. Update: You’ll see that I just added a “Triplets” checkbox above! Note that it will internally drop the number of steps by a factor of 2 for “honest” comparison with the non-triplet case.\n\n\nThe cool thing about these methods is that the embedding that gets learned tends to work for classes the method has never seen before. So, for example, the embedding learned for grouping images of cats, dogs, and horses together would map images of birds to nearby points in the space. Then “all we have to do” if we want to predict a class is see whether a new instance is “nearby” (according to some distance measure we decide) to other similar points. We could even look at the “center points” of various clusters and regard these as the “class prototype” and use that in the future.\nThis fits (somewhat) with the notions of “prototypes” in human classification advanced by Eleanor Rosch in her revolutionary psychology work in the early 1970s. We can say more about this later. ;-)\nThis same method of contrastive losses and metrics is used not for classification per se but for things like photographic identity verification (an example that is given in Andrew Ng’s Machine Learning course on Coursera): Say you want to have a facial recognition system (highly problematic for ethical reasons but it’s a good example of the method so bear with me) for a company where there can be turnover in employees: You probably don’t want to train a traditional classifier with separate a class for each employee because then you’d have to re-train it every time someone joins or leaves the company. Instead, you can store an image of each employee, and then when they appear in front of a camera for identity verification, you could compare the “distance” between the embedded data point for the new photo from the data point for the stored photo(s). If the distance is small enough, then you can have confidence it’s the same person.\nThis is the idea behind a “Siamese Network”, so called because like Siamese twins, it uses two identical branches consisting of the same network (or just the same network run twice):\n\n\n\nExample of a Siamese Network (source: Sundin et al)\n\n\nSo in using metric-based learning for classification, we’re essentially adopting this identity-verification app and applying it to entire classes instead of individuals.\nWhat’s nice about this is that, after you’ve trained your embedding system, it can typically still be used to measure similarity between pairs of things it’s never seen before, because in the process of training it was forced to learn “semantically meaningful” ways of grouping points together. This use of the linguistic work “semantic” is not accidental: the language model systems that rely on “word embeddings” can learn to group similar words together, and even have mathematical-like relationships in analogies (e.g., gender: “king - man + woman = queen”, or countries-and-capitals: “Russia - Moscow + France = Paris”) by treating the embedded data points as vectors that point from the origin of the coordinate system to the data point. We can say more about this and the distance metric they use (“cosine similarity”) another time."
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#how-well-do-they-work",
    "href": "posts/2021-05-04-the-joy-of-3d.html#how-well-do-they-work",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "How well do they work?",
    "text": "How well do they work?\nSo, how do traditional ML classification and metric-based zero-shot methods stack up? Which one is more accurate?\nWell, depends on what you want it for, but for example in 2016, a group of researchers scored pretty high on a Kaggle competition for classification using “entity embeddings.” They said:\n\n“Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features.”\n\nAs far as our own demonstrations, we’ll explore that in Part 2. As a teaser, here’s an image from my own Contrastive Loss Modification of the FastAI tutorial on Siamese Networks:\n\n\n\n…the model achieved an accuracy of 88 to 90% in identifying pet breeds – here’s the kicker – that it had never seen before. The model was trained on a completely different set of breeds, which would have stumped a traditional NN classifier, but the similarity-based embedding method was able to apply the “semantically meaningful” representations learned during training to group new pet breeds by similarity!\nThere’s an important point / “confession” that needs to be made here: These “pet breeds” results were obtained using way more than 3 dimensions – 128 dimensions to be exact. In the real world, such high dimensions are typically necessary. In Part 2, we’ll explore more carefully how the dimensionality of the embedding can affect our accuracy."
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#summary",
    "href": "posts/2021-05-04-the-joy-of-3d.html#summary",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Summary",
    "text": "Summary\nThe “Joy of 3D” referred to in this blog post is about 3D as a teaching tool to motivate our understanding of both traditional NN classification and contrastive-loss based metric learning as both being types of embeddings.\nThe types of “triangle plots” introduced here give students a visual interpretation of “where the data points are” in terms of…\n\nprediction/probability values (via locations of dots)\nlosses (~ distance from target)\naccuracies (which side of “the line” they’re on).\n\nThus it also gives you a visual representation of the “confusion matrix”.\n\nYou can inspect the data points by mousing over the dots to see the images.\n\nThus it allows you to track “top losses” visually, i.e. points that are “not where they’re supposed to be”.\n\n\nNext time, in Part 2, I’ll walk you through the specifics and show you some real training examples (that you can run on your own) with these sorts of “triangle plots”, via my new FastAI callback VizPreds (that I learned how to write)!"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#acknowledgements",
    "href": "posts/2021-05-04-the-joy-of-3d.html#acknowledgements",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "Acknowledgements:",
    "text": "Acknowledgements:\nSpecial thanks to Zach Mueller, Tanishq Abraham and Isaac Flath for assistance in interfacing with fastai!"
  },
  {
    "objectID": "posts/2021-05-04-the-joy-of-3d.html#references",
    "href": "posts/2021-05-04-the-joy-of-3d.html#references",
    "title": "Typical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D",
    "section": "References:",
    "text": "References:\n\n“Siamese and triplet learning with online pair/triplet mining” by Adam Bielski\nExample of new Pipeline class to create data for a Siamese model,“ by Jeremy Howard\n“Siamese Network & Triplet Loss” by Rohith Gandhi\npytorch-metric-learning by Kevin Musgrave\n“Contrastive Loss Explained” by Brian Williams\n“Bootstrap Your Own Latent (BYOL)” by Grill et al\n“Siamese Neural Networks for One-shot Image Recognition” by Koch et al\n“Few Shot Learning” by msiam\n“Learning to learn, Low shot learning” by Katerina Fragkiadaki\n“Embarassingly Simple”:  “We describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets”"
  },
  {
    "objectID": "posts/Transformers1-Attention.html",
    "href": "posts/Transformers1-Attention.html",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "",
    "text": "(GIF animation, zooming in on Attention part of Transformer model diagram)"
  },
  {
    "objectID": "posts/Transformers1-Attention.html#weighted-averages",
    "href": "posts/Transformers1-Attention.html#weighted-averages",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "2.1 Weighted Averages",
    "text": "2.1 Weighted Averages\nOn computers, the way we often emphasize certain parts of an array more than others is to multiply them by another array that has the “weights” to be assigned. In some other contexts, the “weights” array may be also called a “mask”, such as a “hard mask” of 1’s and 0’s to turn on or off parts of the array, or a “soft mask” with floating-point values to emphasize certain elements.\n\n\n\n\n\n\nKey Idea\n\n\n\nAttention is just a soft mask.\n\n\nLet’s use the sample sentence above, and make up some mask weights to stress the relative importance of different words. We’ll display the weight values with a colored background to visualize their magnitude (darker = more weight):\n\n\nShow the code\nimport numpy as np \nimport pandas as pd \nfrom IPython.display import HTML, display\n\ninput = \"Please go to the store and get some milk\"\nwords = input.split()\nmask = np.array([0.5,.9,0.1, 0.01, .7, 0.1,.98,0.1,.99])\ndf = pd.DataFrame({\"word\":words,\"weight\":mask})\ndisplay(df.set_index('word').T.style.background_gradient(vmin=0, vmax=1).format(precision=3).hide())\nprint(f\"Sum of mask weights = {mask.sum():.3g}\")\n\n\n\n\n\n\n\nPlease\ngo\nto\nthe\nstore\nand\nget\nsome\nmilk\n\n\n\n\n0.500\n0.900\n0.100\n0.010\n0.700\n0.100\n0.980\n0.100\n0.990\n\n\n\n\n\nSum of mask weights = 4.38\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the above example, I made up a weight of nearly zero for “the” because it really doesn’t matter. The meaning of the sentence would still be unambiguous without “the”. We’ll use this trick of setting some weights to zero later on when we talk about “Masked Attention” for the Decoder part of the Transformer.\n\n\nWe’ll make it so that the weights all add up to one. If all \\(N\\) words were weighted equally, the weight values would all be \\(1/N\\). For other weighting schemes, we divide the mask weights by their sum to get our attention weights. For the example sentence above, dividing by the sum gives us:\n\n\nShow the code\nattention = mask/mask.sum()\ndf = pd.DataFrame({\"word\":words,\"weight\":attention})\ndisplay(df.set_index('word').T.style.background_gradient(vmin=0, vmax=.22).format(precision=3).hide())\nprint(f\"Sum of attention weights = {attention.sum():.3g}\")\n\n\n\n\n\n\n\nPlease\ngo\nto\nthe\nstore\nand\nget\nsome\nmilk\n\n\n\n\n0.114\n0.205\n0.023\n0.002\n0.160\n0.023\n0.224\n0.023\n0.226\n\n\n\n\n\nSum of attention weights = 1\n\n\nNow, the words themselves will each be represented by an array of numbers we call a “word vector”. Usually, word vectors are viewed as living in some high-dimensional space given by 256, 512, or even 1024 numbers. When we combine the word vectors we get a “context vector”, and the contributions of the word vectors will be weighted appropriately. These weights are literally the attention. To put it differently:\n\n\n\n\n\n\nKey Idea\n\n\n\nAttention is just a weighted average of inputs.\n\n\nSo how should we actually get those weights – how should they be determined? That’s what we’ll cover for the rest of this lesson. Firstly, it helps if we understand the context of something called “context vectors.”"
  },
  {
    "objectID": "posts/Transformers1-Attention.html#what-are-q-k-v",
    "href": "posts/Transformers1-Attention.html#what-are-q-k-v",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "4.1 What are Q, K, & V?",
    "text": "4.1 What are Q, K, & V?\nLiterally, these stand for “Query,” “Key,” and “Value,” respectively. We already mentioned the value \\(V\\) as the “inputs” that are being attended. Note: these “inputs” could be actual data points, or just the outputs from a previous layer in the (stacked) Transformer architecture.\n“Query” and “Key” are typically described in terms of search algorithms, with \\(Q\\) being “what am I looking for?” and \\(K\\) being, say, the keywords or metadata associated with the “actual” value \\(V\\). For example, if you’re searching on YouTube for videos, you’re not searching the videos themselves, you’re searching over metadata/“keys” about those videos, and then the values of the actual, corresponding videos are served up.\nBuuuuut…in the context of Transformer models, I tend to find the “search” analogy to be lacking…or at least it can be an impediment to my understanding. So I’ll say it this way: \\(Q\\) is usually the “current state of the machine,” such as the context vector describing the preceding words so that we can predict the next word.\nIn the simplest implementations \\(K\\) is exactly the same as \\(V\\). Like, literally \\(K\\)=\\(V\\) can work in many cases. We’re not going to do that, but just to say, it can be done. Instead we’ll say that \\(K\\) is an “alternate reduced-dimensional embedding of the embedding of \\(V\\),” meaning that typically \\(Q, K,\\) and \\(V\\) are usually not the vectors themselves but rather the result of a remapping by a (typically) single-layer neural network which adds a little extra expressiveness to the model and which could also be used to project these vectors into some new space with a different (reduced) number of dimensions which is called \\(d_k\\).\nOne more thing: we’ve been saying “vectors” but technically \\(Q, K,\\) and \\(V\\) are matrices consisting of multiple vectors stacked up in rows to form a “batch” that allows us to take advantage of the parallel-processing capabilities of GPUs. In our discussion, the matrix nature doesn’t really affect anything. (If you like, you might think of the vector version as a matrix with a batch size of 1.)"
  },
  {
    "objectID": "posts/Transformers1-Attention.html#similarity-and-its-measures",
    "href": "posts/Transformers1-Attention.html#similarity-and-its-measures",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "4.2 Similarity and its Measures",
    "text": "4.2 Similarity and its Measures\nThe thing that goes inside the parentheses of the softmax function is something that measures the similarity between the query \\(Q\\) and the key \\(K\\). Treating these as vectors, we have a few choices that I’ll show on a diagram:\n\n\n\nFigure 3. Diagram illustrating a few similarity measures for vectors \\(Q\\) and \\(K\\).\n\n\nOur options for measuring similarity include the following 3, the last of which is what the Transformer’s Attention uses:\n\n4.2.1 Euclidean Distance: \\(\\Delta\\)\nThis is just what you think: it’s just the magnitude of the difference \\(\\Delta = Q-K\\), i.e. square the differences of all the coordinates and add up the squares. (The true Euclidean distance would be the square root of that sum, but usually we’d skip that.) Another way of saying is that we take the “dot product” of \\(\\Delta\\) with itself, \\(\\vec{\\Delta} \\cdot \\vec{\\Delta}\\) which in matrix form would be \\(\\Delta\\) times its transpose, ie. \\(\\Delta\\,\\Delta^T\\).\nThe Euclidean distance is “okay,” and there’s nothing inherently wrong with this measure, but for very high-dimensional spaces it can be tricky to make work well (e.g., it can require extra tweaking to get the scaling right) since most points end up both super-far away from each other and yet (paradoxically?) also distributed around near the surface of a hypersphere. 🤷‍♂️ If what I just wrote didn’t resonate with you, just shrug like that emoji-man and read on.\n\n\n4.2.2 Cosine Similarity: \\(\\cos\\theta\\)\nThis measure is extremely common in the field of NLP: We just look at the angle between \\(Q\\) and \\(K\\), and this maps really nicely if you use the cosine of the angle. So, similar vectors will point in nearly the same direction and have a cosine close to 1, whereas vectors that are pointing in opposite directions will have a cosine of -1. Notice that this only considers the directions of \\(Q\\) and \\(K\\) and ignores their magnitudes \\(|Q|\\) and \\(|K|\\). It’s as if we’re treating \\(Q\\) and \\(K\\) like they were unit vectors (which is often fine or at least “good enough”). In fact, the cosine is just the dot product between the unit-vector versions of \\(Q\\) and \\(K\\):\n\\[\\cos\\theta = {Q\\over |Q|}\\cdot{K \\over |K|} = {Q K^T \\over |Q||K|}\\]\nAs we said, this is really handy, and works fine in many situations. But…what if we didn’t throw away the magnitude information? Might that work even better? Also, consider the fact it takes extra effort to compute the denominator \\(|Q||K|\\) every time. What if we just computed \\(Q K^T\\) and went with that?\n\n\n4.2.3  🌟 Scaled Dot Product: \\(QK^T / \\sqrt{d_k}\\)\nThe quantity \\(Q K^T\\) is a “dot product” that yields a distance corresponding to the length of \\(Q\\) projected along \\(K\\) (or vice versa), as shown in Figure 3 above. This works like the cosine similarity but includes the magnitude, and allows us to consider the relative length of the two vectors. It also can map to any real number instead of just the interval \\([-1, 1]\\) like the cosine. But if we just use \\(Q K^T\\), we run into a problem in high dimensions similar to what we see for the Euclidean distance. We’ll look at the fix for that in the next sub-section, which is where the \\(1\\sqrt{d_k}\\) comes in…\n\n4.2.3.1 Why the \\(1/\\sqrt{d_k}\\,\\) ?\nSimply put, it keeps the variance of our resulting data from exploding as the number of dimensions increases. Take a look at Figure 4 (a) and (b).\n\n\n\n\n\n\n\n\na) Without \\(1/\\sqrt{d_k}\\)\n\n\n\n\n\n\n\nb) With \\(1/\\sqrt{d_k}\\)\n\n\n\n\nFigure 4. (Mouse-over or press to expand.) Histograms of dot-products between random vectors in increasingly high-dimensional spaces, both without (a) and with (b) the \\(1/\\sqrt{d_k}\\,\\) factor. Note how, in (a), the horizontal scale and standard deviation \\(\\sigma\\) both grow with the number of dimensions (written as \\(D\\) instead of \\(d_k\\)), whereas in (b) these remain constant. Source: Another blog post of mine [5].\n\n\nThe first figure (a) shows histograms of \\(QK^T\\) for a bunch of random vectors as we increase the number of dimensions: what happens is that the “spread” of these values grows arbitrarily big. And generally in machine learning this can be a bad thing – we like to keep our values “normalized” within a certain range. It turns out that if you just divide by the square root of the number of dimensions…well, check out what happens in (b)! The data stays nicely distributed in the same range regardless of how many dimensions you want to use. 👍\n\n\n\n\n\n\nKey Idea\n\n\n\nInputs will get high attention-weight whenever their key \\(K\\) has high similarity with the query \\(Q\\)."
  },
  {
    "objectID": "posts/Transformers1-Attention.html#multi-headed-attention",
    "href": "posts/Transformers1-Attention.html#multi-headed-attention",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "5.1 Multi-Headed Attention",
    "text": "5.1 Multi-Headed Attention\nIf we extend the dimensionality of our attention appararatus, we can look at different “senses” of the sentence. These extra dimensions are referred to as “heads”. One head might focus on the sheer “facts” or instructions, such as “go get milk”. Another head might sense the tone of the text, such as the use of “Please” making it polite. By allowing for different “attention heads”, this can be handled automatically, using the broadcasting capability of Python."
  },
  {
    "objectID": "posts/Transformers1-Attention.html#masked-attention",
    "href": "posts/Transformers1-Attention.html#masked-attention",
    "title": "To Understand Transformers, Focus on Attention",
    "section": "5.2 Masked Attention",
    "text": "5.2 Masked Attention\nWe’ve already likened Attention to a mask function. When saying “Masked Attention,” the authors mean that some values will be given zero weight, effectively removing them from the sequence. This kind of masked attention is found in the Decoder part of the Transformer when we are trying to (“auto-regressively”) generate new words without “looking into the future” of the training data. Karpathy’s video [2] does a great job of explaining this.\nSay we were training a model to generate the phrase “Please go to the store and get some milk” one word at a time starting with the word “Please.” We’d start by giving weights of zero to all the later words.\n\n\n\n\n\n\nNote\n\n\n\nTransformers do not handle variable-length sequences; all sequences have the same length in Transformer models – this is what makes them parallelizable and therefore fast. So to vary the length of a sequence, we just set all the unused parts to some token values such as zeros.\n\n\nTo get a value of 0 in the output of a softmax function, we supply -inf (i.e. \\(-\\infty\\)) to the logits (i.e. the arguments of softmax), since \\(\\lim_{x\\rightarrow-\\infty}e^x = 0\\). So to mask out future words in Masked Attention, we supply the value -inf for those words.\nThe net result of this will be a sequence of logits with progressively fewer -inf’s in them, or accordingly a set of attention wights with progressively fewer 0’s. As shown below (white=very little weight, dark=most weight):\n\n\nShow the code\ndef softmax(x):\n    sums = np.sum(np.exp(x), axis=-1)[:,np.newaxis]\n    return np.exp(x)/sums\n    \ninput = \"Please go to the store and get some milk\"\nwords = input.split()\nn = len(words)\nattn_weights = np.array([0.114, 0.205, 0.023, 0.002, 0.160, 0.023, 0.224, 0.023, 0.226])\nlogits0 = np.log(attn_weights+1.0e-8)\nlogits = np.full((n,n), -np.inf)\nfor i in range(n): \n    logits[i,0:i+1] = logits0[0:i+1]\n\nattention = softmax(logits)\n\nprint(\"logits:\")\ndf = pd.DataFrame(logits, columns=words)\ndisplay(df.style.background_gradient(vmin=-10, vmax=-1.4).format(precision=3).hide())\n\n\nlogits:\n\n\n\n\n\n\n\nPlease\ngo\nto\nthe\nstore\nand\nget\nsome\nmilk\n\n\n\n\n-2.172\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-inf\n-inf\n-inf\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-inf\n-inf\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-1.833\n-inf\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-1.833\n-3.772\n-inf\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-1.833\n-3.772\n-1.496\n-inf\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-1.833\n-3.772\n-1.496\n-3.772\n-inf\n\n\n-2.172\n-1.585\n-3.772\n-6.215\n-1.833\n-3.772\n-1.496\n-3.772\n-1.487\n\n\n\n\n\n\n\nShow the code\nprint(\"\\nAttention weights = softmax(logits), row-wise:  \")\ndf = pd.DataFrame(attention, columns=words)\ndisplay(df.style.background_gradient(vmin=0, vmax=0.5).format(precision=3).hide())\n\n\n\nAttention weights = softmax(logits), row-wise:  \n\n\n\n\n\n\n\nPlease\ngo\nto\nthe\nstore\nand\nget\nsome\nmilk\n\n\n\n\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n0.357\n0.643\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n0.333\n0.599\n0.067\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n0.331\n0.596\n0.067\n0.006\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\n0.226\n0.407\n0.046\n0.004\n0.317\n0.000\n0.000\n0.000\n0.000\n\n\n0.216\n0.389\n0.044\n0.004\n0.304\n0.044\n0.000\n0.000\n0.000\n\n\n0.152\n0.273\n0.031\n0.003\n0.213\n0.031\n0.298\n0.000\n0.000\n\n\n0.147\n0.265\n0.030\n0.003\n0.207\n0.030\n0.289\n0.030\n0.000\n\n\n0.114\n0.205\n0.023\n0.002\n0.160\n0.023\n0.224\n0.023\n0.226\n\n\n\n\n\nIf you want to see this in action, I recommend Karpathy’s video [2] at around 56 minutes in.\n\n\n\n\n\n\nNote\n\n\n\nThe Transformer can operate on the the above array all at once, in parallel — BLAM! So, even though we say the model is “autoregressive” that doesn’t mean it has to “wait” until it’s generated one word first before it generates the next word. This model can actually generate entire sentences or even entire paragraphs (or more) all in one step. This (parallelism-based) speed is another key reason why large companies like these models. ;-)"
  },
  {
    "objectID": "posts/2018-02-21-Resolving-OSX-Aliases.html",
    "href": "posts/2018-02-21-Resolving-OSX-Aliases.html",
    "title": "Resolving Mac OS X Aliases in Python",
    "section": "",
    "text": "Mac OSX aliases are not symbolic links. Trying to read one may crash your code.\nIn an app I’m developing, I want users to be able to easily create a “library” of symbolic links to other places on their machine, and this is most easily achieved for many of them by Cmd-Option-Dragging and dropping the files. This creates an “alias”, which is a special file that Apple dreamed up. UNIX users are accustomed to symbolic links, and codes written in UNIX will not follow or “resolve” Mac aliases. Instead, they will cause an exception to be thrown.\nThere used to be some libraries to handle this, but they relied on Apple’s old Carbon framework which is no longer supported. There is a mac_alias package but the documentation is lacking. So, I found an old post on MacWorld where one solution is given, and I ported that for what I need.\nHappy to share with you, so that you won’t have to worry about this. As an added bonus, you can tell it to convert aliases to symbolic links, so that “next time” you won’t have to deal with this. Enjoy.\n#!/usr/bin/env python3\n#\n# Resolve Mac OS X 'aliases' by finding where they point to\n# Author: Scott H. Hawley\n#\n# Description:\n# Mac OSX aliases are not symbolic links. Trying to read one will probably crash your code.\n# Here a few routines to help. Run these to change the filename before trying to read a file.\n# Intended to be called from within other python code\n#\n# Python port modified from https://hints.macworld.com/article.php?story=20021024064107356\n#\n# Requirements: osascript (AppleScript), platform, subprocess, shlex\n#\n# TODO: - could make it work in parallel when mutliple filenames are given\n#\n# NOTE: By default, this only returns the names of the original source files,\n#       but if you set convert=True, it will also convert aliases to symbolic links.\n#\nimport subprocess\nimport platform\nimport os\n\n# returns true if a file is an OSX alias, false otherwise\ndef isAlias(path, already_checked_os=False):\n    if (not already_checked_os) and ('Darwin' != platform.system()):  # already_checked just saves a few microseconds ;-)\n        return False\n    checkpath = os.path.abspath(path)       # osascript needs absolute paths\n    # Next several lines are AppleScript\n    line_1='tell application \"Finder\"'\n    line_2='set theItem to (POSIX file \"'+checkpath+'\") as alias'\n    line_3='if the kind of theItem is \"alias\" then'\n    line_4='   return true'\n    line_5='else'\n    line_6='   return false'\n    line_7='end if'\n    line_8='end tell'\n    cmd = \"osascript -e '\"+line_1+\"' -e '\"+line_2+\"' -e '\"+line_3+\"' -e '\"+line_4+\"' -e '\"+line_5+\"' -e '\"+line_6+\"' -e '\"+line_7+\"' -e '\"+line_8+\"'\"\n    args = shlex.split(cmd)      # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security)\n    p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    retval = p.wait()\n    if (0 == retval):\n        line = p.stdout.readlines()[0]\n        line2 = line.decode('UTF-8').replace('\\n','')\n        if ('true' == line2):\n            return True\n        else:\n            return False\n    else:\n        print('resolve_osx_alias: Error: subprocess returned non-zero exit code '+str(retval))\n    return None\n\n\n# returns the full path of the file \"pointed to\" by the alias\ndef resolve_osx_alias(path, already_checked_os=False, convert=False):        # single file/path name\n    if (not already_checked_os) and ('Darwin' != platform.system()):  # already_checked just saves a few microseconds ;-)\n        return path\n    checkpath = os.path.abspath(path)       # osascript needs absolute paths\n    # Next several lines are AppleScript\n    line_1='tell application \"Finder\"'\n    line_2='set theItem to (POSIX file \"'+checkpath+'\") as alias'\n    line_3='if the kind of theItem is \"alias\" then'\n    line_4='   get the posix path of (original item of theItem as text)'\n    line_5='else'\n    line_6='return \"'+checkpath+'\"'\n    line_7 ='end if'\n    line_8 ='end tell'\n    cmd = \"osascript -e '\"+line_1+\"' -e '\"+line_2+\"' -e '\"+line_3+\"' -e '\"+line_4+\"' -e '\"+line_5+\"' -e '\"+line_6+\"' -e '\"+line_7+\"' -e '\"+line_8+\"'\"\n    args = shlex.split(cmd)              # shlex splits cmd up appropriately so we can call subprocess.Popen with shell=False (better security)\n    p = subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    retval = p.wait()\n    if (0 == retval):\n        line = p.stdout.readlines()[0]        \n        source = line.decode('UTF-8').replace('\\n','')\n        if (convert):\n            os.remove(checkpath)\n            os.symlink(source, checkpath)\n    else:\n        print('resolve_osx_aliases: Error: subprocess returned non-zero exit code '+str(retval))\n        source = ''\n    return source\n\n\n# used for multiple files at a time, just a looped call to resolve_osx_alias\ndef resolve_osx_aliases(filelist, convert=False):  # multiple files\n    #print(\"filelist = \",filelist)\n    if ('Darwin' != platform.system()):\n        return filelist\n    outlist = []\n    for infile in filelist:\n        source = resolve_osx_alias(infile, already_checked_os=True, convert=convert)\n        if ('' != source):\n            outlist.append(source)\n    #print(\"outlist = \",outlist)\n    return outlist\n\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description='Resolve OSX aliases')\n    parser.add_argument('file', help=\"alias files to resolve\", nargs='+')\n    args = parser.parse_args()\n    outlist = resolve_osx_aliases(args.file)\n    print(\"outlist = \",outlist)\nThe above code is part of the utils/ directory in my Panotti package. The way it’s called is in the context of trying to read an audio file, called in the file panotti/datautils.py:\ndef load_audio(audio_path, mono=None, sr=None, convertOSXaliases=True):  # wrapper for librosa.load\n    try:\n        signal, sr = librosa.load(audio_path, mono=mono, sr=sr)       # try to read the file 'normally'\n    except NoBackendError as e:                                      \n        if ('Darwin' == platform.system()):                           # if an exception is thrown, check: Am I on a Mac? If so try to resolve an alias\n            source = resolve_osx_alias(audio_path, convert=convertOSXaliases, already_checked_os=True) # ...and convert to symlinks for next time\n            try:\n                signal, sr = librosa.load(source, mono=mono, sr=sr)   # Now try to read again\n            except NoBackendError as e:                               # Ok, even that didn't work, giving up (for now).\n                print(\"\\n*** ERROR: Could not open audio file {}\".format(audio_path),\"\\n\",flush=True)\n                raise e\n        else:                                                         # Failure for some other reason.\n            print(\"\\n*** ERROR: Could not open audio file {}\".format(audio_path),\"\\n\",flush=True)\n            raise e\n    return signal, sr\nHappy coding!\nNOTE: Currently this code only follows one alias. If there’s an alias pointing to an alias to a file, it won’t resolve to that file. Full generality would involve adding an iterative or recursive way of traversing multiple aliases which…I may do later. ;-)"
  },
  {
    "objectID": "posts/2023-06-12-RVQ.html",
    "href": "posts/2023-06-12-RVQ.html",
    "title": "Residual Vector Quantization",
    "section": "",
    "text": "“Vector Quantization.” Sounds intimidating. “Residual Vector Quantization” sounds even more intimidating, even near-meaningless to most people. Turns out, these are easy to understand given a few pictures, even to the point a child could understand them – uh… if the child wanted to. Certainly, there can be sophisticated ways to implement these algorithms, and we’ll cover a bit of that later on, but the basics are very simple.\nResidual Vector Quantization (RVQ) is a data compression technique found in state-of-the-art neural audio codecs such as Google’s SoundStream, and Facebook/Meta AI’s Encodec, which in turn form the backbone of generative audio models such as AudioLM (Google) and MusicGen (Facebook). It’s also the subject of Lucidrain’s library vector-quantize-pytorch, which we’ll use toward the end because it’s so fast and so good.\nWhat is RVQ and how does it work?\nFirst we should consider regular vector quantization (VQ). VQ has been around for decades, and it shows up in many areas of signal processing when compression is involved.\n\nNote: Whenever I say something that applies to both VQ and RVQ, I’ll use the abbreviation “(R)VQ.”"
  },
  {
    "objectID": "posts/2023-06-12-RVQ.html#introduction",
    "href": "posts/2023-06-12-RVQ.html#introduction",
    "title": "Residual Vector Quantization",
    "section": "",
    "text": "“Vector Quantization.” Sounds intimidating. “Residual Vector Quantization” sounds even more intimidating, even near-meaningless to most people. Turns out, these are easy to understand given a few pictures, even to the point a child could understand them – uh… if the child wanted to. Certainly, there can be sophisticated ways to implement these algorithms, and we’ll cover a bit of that later on, but the basics are very simple.\nResidual Vector Quantization (RVQ) is a data compression technique found in state-of-the-art neural audio codecs such as Google’s SoundStream, and Facebook/Meta AI’s Encodec, which in turn form the backbone of generative audio models such as AudioLM (Google) and MusicGen (Facebook). It’s also the subject of Lucidrain’s library vector-quantize-pytorch, which we’ll use toward the end because it’s so fast and so good.\nWhat is RVQ and how does it work?\nFirst we should consider regular vector quantization (VQ). VQ has been around for decades, and it shows up in many areas of signal processing when compression is involved.\n\nNote: Whenever I say something that applies to both VQ and RVQ, I’ll use the abbreviation “(R)VQ.”"
  },
  {
    "objectID": "posts/2023-06-12-RVQ.html#vector-quantization-partitioning-space",
    "href": "posts/2023-06-12-RVQ.html#vector-quantization-partitioning-space",
    "title": "Residual Vector Quantization",
    "section": "Vector Quantization = Partitioning Space",
    "text": "Vector Quantization = Partitioning Space\n“Vector Quantization” is really about dividing up the space of your data points into a discrete set of regions. Put differently, we “partition” the space.\nLet’s say we have a bunch of points in space:\n\n\nShow the code\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# make some data\nn_points = 25\nDATA_MIN, DATA_MAX = -0.5, 0.5 # we'll let these be globals\nnp.random.seed(9)  # for reproducibility\ndata = DATA_MIN + (DATA_MAX-DATA_MIN)*np.random.rand(n_points, 2)\n\n# plot it\nfig, ax = plt.subplots(figsize=(3,3))\nax.set_xlim(DATA_MIN, DATA_MAX)\nax.set_ylim(DATA_MIN, DATA_MAX)\n#ax.set_xticks([])  # hide axis ticks\n#ax.set_yticks([])\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.scatter(data[:, 0], data[:, 1], s=16)\nplt.show()\n\n\n\n\n\nFigure 1. A set of data points, aka ‘vectors’.\n\n\n\n\nTo computer scientists, the \\((x,y)\\) coordinates of each point define a “vector”. (To mathematicians and physicists, the “vector” points from the origin to each point, but that distinction isn’t going to matter to us.)\n\n\n\n\n\n\nClarification\n\n\n\nFor neural audio codecs, the vectors could be vertical slices (columns) from an audio spectrogram, i.e., a list of amplitudes for a set of frequencies that were found over a short time interval called a “frame.” More often, however, the vectors are themselves the outputs of some other audio encoder (e.g. a Convolutional Neural Network), and the (R)VQ is done to compress those encoded vectors even more in order to do things like Latent Diffusion.\n\n\nNow divide up the space into a bunch of regions. How we do that can take many forms. For now, just consider the two examples of my hand drawing with colors, and a set of tiled squares. There are “fancier” algorithms to partition the space in a way that best “fits” the data (e.g., see “k-Means”, below). We can cover schemes like the third image later.\n\n\n\n\n\n\n\n\n(a) By hand\n\n\n\n\n\n\n\n(b) Squares\n\n\n\n\n\n\n\n(c) Fancy (credit: Aalto University, Finland)\n\n\n\n\nFigure 2. Examples of ways to divide up or “quantize” space, aka “Partitioning schemes.”\n\n\nMoving forward, I can do a bit more code-wise with the squares, so let’s start there. ;-)\nLet me just formalize that a bit: We’ll let the number of squares be controlled by the variable “n_grid”. So for our two-dimensional examples, there will be n_grid\\(^2\\) square regions.\nWith vector quantization, we give an index value to every region (e.g. 0 to 24 for a 5x5 square grid), and then we replace each vector’s value with the index of the region.\n\n\nShow the code\n#| code-fold: true\n#| code-summary: \"Show the code\"\nimport pandas as pd \n\ndef plot_data_grid(data, n_grid=5, hide_tick_labels=True, show_indices=False, show_centroids=False, \n                   show_next_level_grid=False):\n    \"big ol' workhorse plotting routine that we'll progressively make use of as the lesson proceeds\"\n    fig, ax = plt.subplots(figsize=(3,3))\n    h = 1.0 / n_grid\n    ax.set_xlim(DATA_MIN, DATA_MAX)\n    ax.set_ylim(DATA_MIN, DATA_MAX)\n\n    for i in range(n_grid + 1):\n        ax.axhline(DATA_MIN + i*h, color='black')\n        ax.axvline(DATA_MIN + i*h, color='black')\n        \n    if show_indices:\n        index = 0\n        for j in range(n_grid):\n            for i in range(n_grid):\n                x = DATA_MIN + (i + 0.5) / n_grid\n                y = DATA_MIN + 1 - (j + 0.5) / n_grid\n                ax.text(x, y, str(index), ha='center', va='center',fontsize=14)\n                index += 1\n\n    if show_centroids: \n        for j in range(n_grid):\n            for i in range(n_grid):\n                x = DATA_MIN + (i + 0.5)*h\n                y = DATA_MIN + (j + 0.5)*h\n                ax.plot(x, y, 'ro', markersize=6)\n                \n    if hide_tick_labels:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    ax.set_aspect('equal')\n    if data is not None:\n        plt.scatter(data[:, 0], data[:, 1], s=16)\n    plt.show()\n    \n    \nn_grid=5\n#plot_data_grid(data, n_grid=n_grid)\nplot_data_grid(data, n_grid=n_grid, show_indices=True)\n\n\n\n\n\nFigure 3. (Integer) Indices for each region.\n\n\n\n\nFor each of the “vectors” represented by the data points, we no longer use the (\\(x\\),\\(y\\)) coordinate pairs, but rather the (integer) index of the region it’s in.\n\n\n\n\n\n\nNote\n\n\n\nWe’ve gone from needing two floating point numbers per point to just one integer value. In two dimensions, the “savings” or data-compression amount that we get from that may not be readily apparent, but stick around: As we go to large numbers of dimensions later, this scheme will save us a ton of data.\n\n\nIf we want the coordinates that go with the indices, we’ll use the centroid of each region. In this sense, the vectors are “quantized” so that they can only take on values given by the centroids of the regions. In the following example, the centroids are shown in red:\n\n\nShow the code\nplot_data_grid(data, n_grid=n_grid, show_indices=False, \n               show_centroids=True)\n\n\n\n\n\nFigure 4. Centroid locations\n\n\n\n\nSo each blue point will effectively be replaced by the nearest red point. In this sense we have “quantized” the vectors (because we’ve quantized the space itself).\n\n\n\n\n\n\nTerminology\n\n\n\nThe set of locations of centroids is called the “codebook”. When we want to use an actual vector value (in space), we convert the codebook index into a (centroid) location by looking up the codebook.\n\n\n\nSo a full (albeit unweidly) picture showing the data points, the region indices, and the centroids, looks like this:\n\n\nShow the code\nplot_data_grid(data, n_grid=n_grid, show_indices=True, show_centroids=True, hide_tick_labels=False)\n\n\n\n\n\nFigure 5. Detailed figure showing data points, region indices, and centroid locations.\n\n\n\n\nFor our choice of coordinates, the codebook (i.e. the mapping of indices to centroid locations) looks like this:\n\n\nShow the code\nfrom IPython.display import display, Markdown\n\ndef generate_codebook(n_grid, n_dim=2, debug=False):\n    n_gridpoints = n_grid ** n_dim\n    if debug: print(f\"generate_codebook: n_grid = {n_grid}, n_dim = {n_dim}, n_gridpoints = {n_gridpoints}\")\n    centroids = np.empty((n_gridpoints, n_dim))\n    h = (DATA_MAX - DATA_MIN)/n_grid\n    for i in range(n_dim):\n        coords = np.linspace(DATA_MIN + h/2, DATA_MAX - h/2, n_grid)\n        coords = np.tile(coords, int(n_gridpoints / n_grid ** (i + 1)))\n        coords = np.repeat(coords, n_grid ** i)\n        centroids[:, i] = coords\n\n    return centroids\n\n\ncodebook = generate_codebook(n_grid)\n\n\n# print it nicely\ncb_str = f\"| index      | vector |\\n| ----------- | ----------- |\"\nfor i in range(codebook.shape[0]):\n    cb_str += f\"\\n| {i} | ({codebook[i,0]:.1f}, {codebook[i,1]:.1f}) |\"\ndisplay(Markdown(cb_str))\n\n\n\n\n\nindex\nvector\n\n\n\n\n0\n(-0.4, -0.4)\n\n\n1\n(-0.2, -0.4)\n\n\n2\n(0.0, -0.4)\n\n\n3\n(0.2, -0.4)\n\n\n4\n(0.4, -0.4)\n\n\n5\n(-0.4, -0.2)\n\n\n6\n(-0.2, -0.2)\n\n\n7\n(0.0, -0.2)\n\n\n8\n(0.2, -0.2)\n\n\n9\n(0.4, -0.2)\n\n\n10\n(-0.4, 0.0)\n\n\n11\n(-0.2, 0.0)\n\n\n12\n(0.0, 0.0)\n\n\n13\n(0.2, 0.0)\n\n\n14\n(0.4, 0.0)\n\n\n15\n(-0.4, 0.2)\n\n\n16\n(-0.2, 0.2)\n\n\n17\n(0.0, 0.2)\n\n\n18\n(0.2, 0.2)\n\n\n19\n(0.4, 0.2)\n\n\n20\n(-0.4, 0.4)\n\n\n21\n(-0.2, 0.4)\n\n\n22\n(0.0, 0.4)\n\n\n23\n(0.2, 0.4)\n\n\n24\n(0.4, 0.4)\n\n\n\n\n\n\nReconstruction Error\nWhen we do this quantization (i.e. replacing vectors by their nearest centroid), the centroid locations will naturally be a bit “off” compared to the original data vectors themselves. The finer the grid you make, the smaller the regions, and the less error. For a 2D grid, the error will be on the order of \\(h^2\\) where \\(h\\) is the grid spacing (\\(h\\) = 1/5 = 0.2 in our example). Let’s check how the error changes as we vary the grid spacing, i.e., as we vary n_grid.\n\n\nShow the code\ndef find_closest_centroids(data, codebook):\n    distances = np.sum((data[:, np.newaxis] - codebook) ** 2, axis=2)\n    closest_centroids = np.argmin(distances, axis=1)\n    return closest_centroids # indices not coordinates\n\ndef compute_error(data, codebook):\n    n_grid = int(np.sqrt(codebook.shape[0]))\n    h = 1/n_grid * (DATA_MAX - DATA_MIN) \n    quantized_ind = find_closest_centroids(data, codebook)\n    quantized_xy = codebook[quantized_ind]       \n    error = np.sqrt( np.sum( (data - quantized_xy)**2 ) )\n    return error \n\nn_points2 = 100\ndata2 = DATA_MIN + (DATA_MAX - DATA_MIN)*np.random.rand(n_points2, 2)\n\nerrors = []\ngrids = np.array([5,10,25,100,200])\nfor n_grid2 in grids:\n    codebook = generate_codebook(n_grid2)\n    error = compute_error(data2, codebook)               \n    errors.append(error)\n    \nfig, ax = plt.subplots(1,2, figsize=(10,4))    \nax[0].plot(grids, errors, 'o-')\nax[1].loglog(grids, errors, 'o-')\nfor i in range(2):\n    ax[i].set_ylabel('Error')\n    ax[i].set_xlabel('n_grid')\nplt.show()\nprint(f\"lowest error (for n_grid={grids[-1]}) = \",errors[-1])\n\n\n\n\n\nFigure 6. Plots of error vs. resolution with linear (left) and logarithmic (right) axes. Note that the computational cost will scale like the number of regions, which is n_grid\\(^2\\).\n\n\n\n\nlowest error (for n_grid=200) =  0.019903707672610238\n\n\nSo, the more “gridlines” you use, the lower the error, but at what cost? To get an error of 0.02 we need \\(200^2 = 400\\) regions. And in higher dimensions than 2, the “cost” of higher resolution / lower error goes up immensely: to double the resolution in \\(d\\) dimensions, the computational cost goes up by a factor of \\(2^d\\). (Imagine \\(d\\)=32, 64, 128,…)\nBut we don’t need to cover the entire space uniformly! That’s where Residual Vector Quantization comes in. Feel free to skip ahead to the section on RVQ. For the next bit we’ll take an optional aside to learn about an alternate way to partition space, known as a the “k-Means” algorithm.\n\n\n\n\n\n\nNote\n\n\n\nAnother key point: By replacing all the coordinate values for a vector (i.e. \\(d\\) floating point numbers) with a single integer, VQ achieves data compression by a factor of \\(d\\) (times however many bit floats take up compared to integers). For large numbers of dimensions – regardless of the partitioning scheme – this compression can be significant.\n\n\n\n\nk-Means (Partitioning Scheme)\n\n\n\n\n\n\nOptional\n\n\n\nThis discussion on k-Means is actually not crucial to understanding (R)VQ. At all. It’s quite skippable, to be honest. So…only read if you’re really curious. Otherwise skip down to the section on Residual Vector Quantization.\n\n\nThe k-Means algorithm is another method to partition space, a method that instead of static squares, will allow our regions and centroids to “follow the data”. Often k-Means is used to initialize an RVQ codebook for neural codecs (e.g. in SoundStream, Encodec, etc), after which the rest of the neural network’s training algorithm may refine the codebooks more.\nWe’ll start with a bunch of data points shown as small dark circles below, and a set of \\(k\\) “centroids” shown by the large colored circles. (They’re not actually “centroids” yet but we’ll get there).\n\n\nShow the code\n# lots of imports to try to get the plotly plots and animations to show!\nimport plotly.io as pio\npio.renderers.default='notebook'\nimport plotly.express as px \nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\nndim = 2  # easier to do in 2d at first\nnpoints = 100\nnp.random.seed(1)\ndatak = DATA_MIN + (DATA_MAX-DATA_MIN)*np.random.rand(npoints, ndim)   # make up some random data points\n\nk = 5        # k-Means parameter: number of clusters\n\n# start with random centroids\ncentroids = DATA_MIN + 0.2+0.6*(DATA_MAX-DATA_MIN)*np.random.rand(k, ndim) # don't let the starting centroids be on the far edges of the plot\norig_centroids = centroids.copy() # save these for later when we'll make a movie\n\n\ndef plot_kmeans(data, centroids, cluster_memb=None): # we'll leave a way to color the data points\n    all_data = np.concatenate((data,centroids))\n    is_centroid = np.array( [1 if i &gt;= npoints else 0 for i in range(all_data.shape[0])] , np.newaxis)\n    sizes = is_centroid*3 + 1\n    cs = cluster_memb if cluster_memb is not None else -1+np.zeros( (data.shape[0],1) ,dtype=int) \n    cs = np.append(cs, range(k)) # add the centroids\n    plot_data = np.column_stack( (all_data, sizes, cs) )\n    columns = ['x','y','sizes','cluster']\n    df = pd.DataFrame( plot_data,  columns=columns)\n    fig = px.scatter(df, x=\"x\", y=\"y\", color='cluster', size='sizes',  range_color=(-1,k),\n            width=420, height=360, color_continuous_scale='Turbo')\n    fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),)#template='plotly_dark',)\n    return fig\n\nplot_kmeans(datak, centroids)\n    \n\n\n\n\n                                                \nFigure 7. Initial state of k-Means, showing data without any ‘membership’ (dark dots) and random centroid locations (large colored dots)\n\n\n\nThat’s our starting points. Then what we’ll do is, we’ll color each point according to which centroid it’s closest to.\n\n\n\n\n\n\nWarning\n\n\n\nYes, this can be very slow for lots of points in many dimensions. There is a whole field of study of how to speed up “nearest neighbors” types of algorithms. We’re going to keep it simple for this demo. Further down, the library we’ll use will actually do an “approximate k-Means” for speed.\n\n\n\n\nShow the code\ndef distance(p1, p2):\n    \"euclidean distance squared. we don't need the square root b/c we're only interested in ordering by distance\"\n    return np.sum((p1-p2)**2, axis=1)\n\ndef calc_cluster_membership(data, centroids):\n    npoints = data.shape[0]\n    min_ds = 9999*np.ones(npoints)\n    cluster_memb = np.zeros(npoints, dtype=int)-1\n    for i, c in enumerate(centroids): # compute distances for all points\n        ds = distance(data, c)\n        inds = np.argwhere(ds &lt; min_ds)\n        if inds.size &gt; 0: \n            min_ds[inds] = ds[inds]\n            cluster_memb[inds] = i\n    assert len(cluster_memb)==npoints # we're not including the centroids themselves here\n    return cluster_memb\n\ncluster_memb = calc_cluster_membership(datak, centroids)\nplot_kmeans(datak, centroids, cluster_memb=cluster_memb)\n\n\n\n\n                                                \nFigure 8. First step in k-Means: Assign ‘membership’ of datapoints to different clusters based on which centroids (large dots) are nearest. Note that the ‘centroids’ in this graph aren’t true centroids yet. We’ll fix them in the next step after this.\n\n\n\nThe next step is to recalculate the centroid locations using the points that were assigned to each centroid. And these centroids are just the mean value of the points.\n\n\nShow the code\ndef move_centroids(centroids, data, cluster_memb):\n    for ki in range(centroids.shape[0]):\n        inds = np.argwhere( cluster_memb == ki)\n        if inds.size &gt; 0:\n            for dim in range(data.shape[-1]):\n                centroids[ki,dim] = np.mean(data[inds,dim], axis=0)\n    return centroids\n\ncentroids = move_centroids(centroids, datak, cluster_memb)\n#print(\"These centroids should be fairly 'centered' in their respective groups of data.\")\nplot_kmeans(datak, centroids, cluster_memb=cluster_memb)\n\n\n\n\n                                                \nFigure 9. Next step in k-Means: move centroids so they are in the middle of each cluster\n\n\n\n…But now, having moved the centroids, the nearest-neighbor membership of some of the points may have changed. So we recalculate those:\n\n\nShow the code\ncluster_memb = calc_cluster_membership(datak, centroids)\nplot_kmeans(datak, centroids, cluster_memb=cluster_memb)\n\n\n\n\n                                                \nFigure 10. Next step in k-Means: re-assign cluster membership of point based on new cluster locations\n\n\n\n…And we repeat the process until some stopping criterion is met. For example we may just set a maximum iteration number, or we may stop when the centroids stop moving, or the cluster membership stops changing, etc. For this demo, we’ll just use a maximum number of iterations.\nSo, a movie of the whole process can look like this:\n\n\nShow the code\ncentroids = orig_centroids.copy() # start over\ncluster_memb = -1+np.zeros( (datak.shape[0],1) ,dtype=int) \nmaxiter = 10\nplot_kmeans(datak, centroids, cluster_memb=cluster_memb)\nts = np.arange(2*maxiter,dtype=int)\n\ncolumns = ['t','x','y','sizes','cluster','point_number']\ntotal_points = npoints + k  # points + clusters\nmovie_data = np.zeros( (len(ts)*total_points, len(columns)) )  # giant data array for all data points at all times\nmovie_data[:,0] = np.repeat(ts, total_points)          # t\n\nxs = np.concatenate( ( datak[:,0], centroids[:,0] ) ) # initial xs\nys = np.concatenate( ( datak[:,1], centroids[:,1] ) ) # initial ys\nmovie_data[:,1] = np.tile(xs, len(ts))\nmovie_data[:,2] = np.tile(ys, len(ts))\n\nsizes = np.ones(total_points,dtype=int)\nsizes[-k:] = 3\nmovie_data[:,3] = np.tile(sizes, len(ts)) # sizes is repeated for all times\n\ncs = -2 + np.ones(total_points,dtype=int)\ncs[-k:] = np.arange(k, dtype=int)\nmovie_data[:,4] = np.tile(cs, len(ts))\n\nmovie_data[:,5] = np.tile(np.arange(total_points), len(ts))  # point_number is repeated for all time\n\nfor t in ts:\n    if t==0: \n        continue # leave initial data as step 0\n    elif t % 2 == 1: \n        cluster_memb = calc_cluster_membership(datak, centroids)\n    else:\n        centroids = move_centroids(centroids, datak, cluster_memb)\n\n    start, end = t*total_points, (t+1)*total_points  \n    movie_data[start:end, 1] = np.concatenate( (datak[:,0], centroids[:,0]) )    # xs\n    movie_data[start:end, 2] = np.concatenate( (datak[:,1], centroids[:,1]) )    # ys\n    movie_data[start:end, 4] = np.concatenate( (cluster_memb, np.arange(k, dtype=int)) )  # cluster membership\n    \n    \ndf = pd.DataFrame(movie_data, columns=columns)\n#df['cluster'] = df['cluster'].astype(int).astype(str) # use discrete colormap\n#display(df)\n\n\nfig = px.scatter(df, x=\"x\", y=\"y\", animation_frame=\"t\", animation_group=\"point_number\",\n           color=\"cluster\", hover_name=\"cluster\", hover_data=['point_number'],\n           size='sizes', width=420, height=460, range_color=(-1,k), color_continuous_scale='Turbo'\n           )\nfig.update_layout( margin=dict(l=20, r=20, t=30, b=10),)# template='plotly_dark',)\nfig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 0 # don't interpolate frames\ndisplay(fig)\n#print( \"Caption: In this movie, each step will either be the 'membership' step or \\nthe 'centroid moving' step.\")\n\n\n\n\n                                                \nFigure 11. (Interactive) In this movie, each time step is either the ‘cluster membership’ step or the ‘centroid moving’ step.\n\n\n\nFor those who are interested, I’ll put the code for a working k-Means algorithm below – it’s short!\n\ndef kmeans(data, k=5, n_iter=8):\n    n_dim, data_min, data_max = data.shape[-1], np.min(data), np.max(data)\n    centroids = data_min + 0.2+0.6*(data_max-data_min)*np.random.rand(k, n_dim) # random init\n    for it in range(n_iter):\n        cluster_memb = calc_cluster_membership(data, centroids)\n        centroids = move_centroids(centroids, data, cluster_memb)\n\n    return centroids, cluster_memb\n\nThe end result of k-Means, since it is a kind of “nearest neighbors” algorithm, is set of vectors grouped according to a “Vornonoi diagram” like the one shown near the beginning of this post, which we’ll show again for good measure:\n\n\n\nFigure 12. Space partitioning that can result from k-Means. (Credit: Aalto University, Finland)"
  },
  {
    "objectID": "posts/2023-06-12-RVQ.html#residual-vector-quantization-rvq",
    "href": "posts/2023-06-12-RVQ.html#residual-vector-quantization-rvq",
    "title": "Residual Vector Quantization",
    "section": "Residual Vector Quantization (RVQ)",
    "text": "Residual Vector Quantization (RVQ)\n\nBy the way, RVQ has been around for quite a while.\n\n\nBasic Idea: “Codebooks in Codebooks”\nThe trick with RVQ is, rather than having a single high-resolution codebook, to instead have “codebooks inside of codebooks”, or, if you like, “stacking codebooks”. Let’s say we want to quintuple the resolution of our initial 5x5 grid. Instead of using a 25x25 grid (which would be 25x the computational cost of the original), what if we put a little 5x5 grid “inside” the region to which a vector was quantized?\nFor example, in the “middle” region (region 12), we can do…\n\n\nShow the code\nfrom matplotlib import collections  as mc\n\n\ndef get_region_membership(data:np.array, h=0.2, codebook=None):\n    \"Tells which region each point is in. TBD: this is slow but it works! ;-) \"\n    memb = np.zeros(data.shape[0], dtype=np.int32)\n    if codebook is None:  # just assume basic squares\n        for di, p in enumerate(data):\n            i = (p[0]-DATA_MIN) // h\n            j = (p[1]-DATA_MIN) // h\n            ind = i + j*n_grid\n            memb[di] = ind\n    else: \n        memb = calc_cluster_membership(data, codebook)\n    return memb\n\n\ndef plot_data_grid_with_resids(data, n_grid=5, hide_tick_labels=True, show_indices=False, show_centroids=False, \n                   show_next_level_grid=False, show_resids=True, codebook=None, show_grid=True):\n    \"big ol' workhorse plotting routine that we'll progressively make use of as the lesson proceeds\"\n    fig, ax = plt.subplots(figsize=(3,3))\n    h = 1.0 / n_grid\n    ax.set_xlim(DATA_MIN, DATA_MAX)\n    ax.set_ylim(DATA_MIN, DATA_MAX)\n\n    if show_grid:\n        for i in range(n_grid + 1):  \n            ax.axhline(DATA_MIN + i*h, color='black')\n            ax.axvline(DATA_MIN + i*h, color='black')\n        \n    if show_next_level_grid: # draws lines in the middle\n        x_start = 2*h\n        y_start = -h/2\n        for i in range(n_grid): # horizontal lines\n            y = y_start + i*h/n_grid\n            ax.axhline(y, xmin=x_start, xmax=x_start+h, color='black')\n        y_start, x_start = x_start, y_start\n        for j in range(n_grid): # horizontal lines\n            x = x_start + j*h/n_grid\n            ax.axvline(x, ymin=y_start, ymax=y_start+h, color='black')\n        \n    if show_indices:\n        index = 0\n        for j in range(n_grid):\n            for i in range(n_grid):\n                x = DATA_MIN + (i + 0.5) / n_grid\n                y = DATA_MIN + 1 - (j + 0.5) / n_grid\n                ax.text(x, y, str(index), ha='center', va='center',fontsize=14)\n                index += 1\n\n    if show_centroids: \n        for j in range(n_grid):\n            for i in range(n_grid):\n                x = DATA_MIN + (i + 0.5)*h\n                y = DATA_MIN + (j + 0.5)*h\n                ax.plot(x, y, 'ro', markersize=6)\n                \n    if show_resids and codebook is not None:\n        memb = get_region_membership(data, codebook=codebook)\n        resids = data*0\n        lines = []\n        for i, p in enumerate(data):\n            #resids[i] = p - codebook[memb[i]] # don't actually need to compute resids for this\n            lines.append([p, codebook[memb[i]] ] )\n        lc = mc.LineCollection(lines, colors=(1,0,1,1), linewidths=2)\n        ax.add_collection(lc)\n        \n    if hide_tick_labels:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    ax.set_aspect('equal')\n    if data is not None:\n        plt.scatter(data[:, 0], data[:, 1], s=16)\n    plt.show()\n    \n    \nn_grid=5\ncodebook = generate_codebook(n_grid)\nplot_data_grid_with_resids(data, n_grid=n_grid, show_next_level_grid=True, show_centroids=True, \n                           hide_tick_labels=False, codebook=codebook, show_resids=False)\n\n\n\n\n\nFigure 13. Illustration of ‘codebook within a codebook’, with a smaller 5x5 codebook which will be relative to the middle region’s codebook.\n\n\n\n\nThe difference between that blue point in the middle “main” square and its corresponding red centroid will be the “residual”. We will also quantize that within the “little” 5x5 grid. This will serve as a codebook to use “after” the original codebook. And we’ll get the same resolution as if we had a 25x25 grid, except our computational cost will instead be 2*(5*5)=50 instead of 25*25=625! So our cost will be 12.5 smaller than the full-grid approach.\nAnd interestingly, if we only consider the residual, i.e. the difference between the main centroid and the vector in question, then we can use the same “next level” codebook for all points in the space! In the following figure, we show the residuals as purple line segments that run from each point to its corresponding nearest centroid:\n\n\nShow the code\nplot_data_grid_with_resids(data, n_grid=n_grid, show_next_level_grid=True, show_centroids=True, \n                           hide_tick_labels=True, codebook=codebook, show_resids=True)\n\n\n\n\n\nFigure 14. Illustration of redisuals, shown as purple line segments connecting vectors (blue points) with their nearest centroids (red points).\n\n\n\n\nAlso, because we cleverly set up our data to be centered around the origin \\((0,0)\\), we can treat the original data points as “residuals” relative to the “centroid” of the whole domain, namely the origin!\n\n\nShow the code\nplot_data_grid_with_resids(data, n_grid=n_grid, show_next_level_grid=False, show_centroids=False, \n                           hide_tick_labels=False, codebook=np.zeros((data.shape[0],2)), show_resids=True, show_grid=False)\n\n\n\n\n\nFigure 15. How we can treat the data points themselves as residuals relative to the origin, for a ‘level 0’ codebook\n\n\n\n\nAlso because of our clever choice of coordinates, for the next “level” of quantization, we can take the codebook at the next level to be just the previous codebook divided by n_grid! This won’t always be the case; I’m just feeling clever and lazy.\n\n\nQuantizer algorithm\nUp til now, we’ve hidden the code by default. But to really get the RVQ method, I’m going to show the code.\nLet’s write a general quantizer multiple “levels” of nested codebooks. It will take our data points and return the various levels of codebook indices.\nWill be following “Algorithm 1” from page 4 of Google’s 2021 paper “SoundStream: An End-to-End Neural Audio Codec”:\n\n\n\nFigure 16. SoundStream’s RVQ Algorithm\n\n\nBut I like the way I write it:\n\ndef quantizer(data, codebooks, n_grid=5):\n    \"this will spit out indices for residuals in a series of 'nested' codebooks\"\n    resids = data \n    indices = []\n    for cb in codebooks:\n        indices_l = get_region_membership(resids, codebook=cb)\n        resids = resids - cb[indices_l]\n        indices.append(indices_l)\n    return np.array(indices)\n\n# Make the nested codebooks\nn_codebooks = 3\ncodebook = generate_codebook(n_grid)\ncodebooks = [codebook/n_grid**level for level in range(n_codebooks)]\n\nindices = quantizer(data, codebooks)   # call the quantizer\ndisplay(indices)\n\narray([[10,  2,  5,  7,  5, 20,  4, 13, 19, 17, 16, 14, 24,  4, 22, 23,\n         4, 21, 19, 11, 13, 12,  4, 18,  4],\n       [10, 17,  3,  5, 17,  9,  3, 22,  7, 13, 14,  5,  3, 24, 18,  6,\n        24,  6, 17,  8, 10,  1, 21, 12,  3],\n       [11,  6, 12,  7, 15, 20, 23,  7, 17, 13,  8, 18,  2,  7, 15, 11,\n        16, 20, 23, 13, 11, 24, 18, 10, 20]])\n\n\nLet’s test this by trying to reconstruct our original data using each level of codebook. In the following, the original data will be in blue, and the other colors will show the results of quantization using an increasing number of codebooks:\n\n\nShow the code\nK = n_codebooks\nfig, ax_all = plt.subplots(nrows=1, ncols=K, figsize=(10+(K-3),3))\nrecon = data*0\nfor axi in range(K):\n    ax = ax_all[axi]\n    ax.scatter(data[:, 0], data[:, 1], s=16)\n    recon += codebooks[axi][indices[axi]]\n    ax.scatter(recon[:, 0], recon[:, 1], s=16, color='orange')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    hpos = 1/K*0.85 + 1/K*0.81*axi if K==4 else .25 + .27*axi\n    fig.text(hpos, .05, f\"{axi+1} codebook{'s' if axi&gt;0 else ''}\", ha='center')\n\n    error = ((recon - data)**2).mean()\n    fig.text(hpos, .0001, f\"Error = {error:.2e}\", ha='center')\n    \nplt.show()\n\n\n\n\n\nFigure 17. Reconstructing data (blue points) using multiple levels of RVQ codebooks (orange dots)\n\n\n\n\nWe see that the more (levels of) codebooks we use, the better we can approximate the original data. The rightmost image has an effective resolution of \\(5^3 = 125\\) little squares, but instead uses only \\(5x5x3 = 75\\). In two dimensions, this is not a huge savings, but let’s see how important this is for higher numbers of dimensions.\nLet \\(d\\) be the number of dimensions and \\(K\\) be the number of codebooks (no relation to the k from k-Means, I’m afraid). We’ll populate a d-dimensional hypercube with a lot of data, and subdivide it into nested groups of little hypercubes using RVQ, and compute the error – as well as the compuational cost “savings” from if we had used regular VQ instead of RVQ.\n\n\n\n\n\n\nNote\n\n\n\nUsing uniform square/(hyper)cube regions is a really dumb idea for this. Because the number of regions will scale like n_grid\\(^d\\), which can actually be much larger than the number of data vectors we have! We’ll try a more sophisticated partitioning scheme further below.\n\n\n\n\nShow the code\n#### label: fig-rvq-recon-highdim2\n#### fig-cap: \"Error for high-dimensional datasets using various levels of RVQ. 'cost savings factor' refers to the ratio of using regular VQ (at uniform resolution) vs RVQ\"\n\nd_choices = [2, 3, 4, 6]  # we can't go much higher with 5x5 uniform grids!\nK_choices = [1,2,3,4]     # variable numbers of codebooks\n\nnpoints_hd = 1000  # points in high-dim spaces\n\nprint(\"Here we show the error for high-dimensional datasets using various levels of RVQ.\")\nprint(\"'cost savings factor' refers to the ratio of using regular VQ (at uniform resolution)\\nvs RVQ.\")\n\nfor d in d_choices:\n    print(f\"\\nd = {d}:\")\n    np.random.seed(1)\n    data_hd = DATA_MIN + (DATA_MAX-DATA_MIN)*np.random.rand(npoints_hd, d)\n    codebook0 = generate_codebook(n_grid, n_dim=d)\n    codebooks = [codebook0/n_grid**level for level in range(max(K_choices))] # lets get this over with rather than in the loop\n    for K in K_choices: \n        indices = quantizer(data_hd, codebooks)\n        recon = data_hd*0\n        for lil_k in range(K):   # reconstruct using all codebooks\n            recon += codebooks[lil_k][indices[lil_k]]\n        error = ((recon - data_hd)**2).mean()\n        grid_0_points = n_grid**(d)\n        rvq_points = grid_0_points*K \n        uni_res = grid_0_points**K # comparable uniform resolution\n        savings = uni_res/rvq_points\n        print(f\"  K = {K}, error = {error:.2e}, cost savings factor = {savings:.1f}\")\n        pass\n\n\nHere we show the error for high-dimensional datasets using various levels of RVQ.\n'cost savings factor' refers to the ratio of using regular VQ (at uniform resolution)\nvs RVQ.\n\nd = 2:\n  K = 1, error = 3.41e-03, cost savings factor = 1.0\n  K = 2, error = 1.29e-04, cost savings factor = 12.5\n  K = 3, error = 5.26e-06, cost savings factor = 208.3\n  K = 4, error = 2.16e-07, cost savings factor = 3906.2\n\nd = 3:\n  K = 1, error = 3.37e-03, cost savings factor = 1.0\n  K = 2, error = 1.29e-04, cost savings factor = 62.5\n  K = 3, error = 5.31e-06, cost savings factor = 5208.3\n  K = 4, error = 2.18e-07, cost savings factor = 488281.2\n\nd = 4:\n  K = 1, error = 3.36e-03, cost savings factor = 1.0\n  K = 2, error = 1.32e-04, cost savings factor = 312.5\n  K = 3, error = 5.34e-06, cost savings factor = 130208.3\n  K = 4, error = 2.16e-07, cost savings factor = 61035156.2\n\nd = 6:\n  K = 1, error = 3.37e-03, cost savings factor = 1.0\n  K = 2, error = 1.33e-04, cost savings factor = 7812.5\n  K = 3, error = 5.35e-06, cost savings factor = 81380208.3\n  K = 4, error = 2.16e-07, cost savings factor = 953674316406.2\n\n\nThose “cost savings factors” were artifically high though, because we’re still using squares/hypercubes for our regions, and we don’t need to shape them that way and we don’t need that many of them. The great thing about (R)VQ is that you can specify how many centroids you want – i.e. how “long” you want your codebook to be – and you can keep that to some manageable number even as the number of dimensions skyrockets.\nSo, to tackle higher dimensions, we need to stop using uniform squares so we can have a codebook “length” of less than a few thousand centroids (instead of what just had, which was going into the hundreds of thousands, e.g. \\(5^8 = 390625\\)). To get our non-uniform regions that follow the data, we’ll use the k-Means method described above.\nLet’s look at how the reconstruction error behaves in high dimensions.\n\n\nError Analysis: Exponential Convergence\nWe can try a given number of (initially) random centroids, and try to match them to the data via k-Means.\n\n\n\n\n\n\nNote\n\n\n\nThe residuals at different levels of our RVQ calculations will likely have different data distributions. Which means that instead of “sharing (scaled) codebooks” like we did before, we’ll need to re-calculate a new codebook at each “level”. Otherwise we won’t see any advantage from RVQ (trust me I tried).\n\n\nIn the following calculation, we’ll vary the number of dimensions, the length of the codebooks, and the number of codebooks, and see how these all affect the reconstruction error.\nBut rather than using our code, use let’s use the wonderful repository by lucidrains: lucidrains/vector-quantize-pytorch\nThe following set of error values is a “wall of numbers” which may not interest you. Feel free to scroll past it and skip down to the graphical representation of (some of) the numbers.\n\n\nShow the code\nimport torch\nfrom vector_quantize_pytorch import ResidualVQ\nfrom functools import partial \n\ndef make_pretty(d,styler):\n    styler.set_caption(f\"d = {d}:\")\n    styler.format(lambda e: f'{10**e:0.1e}')   # undo the log op\n    styler.background_gradient(axis=None, vmin=-12, vmax=1, cmap=\"plasma\")\n    return styler\n\ndef show_colored_data(results_d, d, cb_lengths, K_choices):\n    \"displays a dataframe table but colors the background according to log10 of the numbers\"\n    rdf = pd.DataFrame(np.log10(results_d.numpy()), \n                       columns=[f'K = {k}' for k in K_choices], \n                       index=[f'cb_len = {c}' for c in cb_lengths])\n    ourfunc = partial(make_pretty,d)\n    display(rdf.style.pipe(ourfunc))\n    return\n\nn_dim = 256  # number of dimensions\ncb_len = 256 # codebook length\nK = 4        # number of codebooks\nnpoints_hd = 4096  # number of data points in high-dim space\n\nd_choices = [2, 3, 6, 8, 16, 32, 64, 128, 256, 512]  # dimensions to try\ncb_lengths = [25, 64, 256, 1024, 2048]                 # codebook lengths\nK_choices = [1,2,3,4,6,8,10]#,16]                           # variable numbers of codebooks\n\nresults = torch.empty( ( len(d_choices), len(cb_lengths), len(K_choices) )).cpu()\nfor q1, n_dim in enumerate(d_choices):\n    #print(f\"\\nd={n_dim:4}:       {' '.join([f'      K={i}' for i in K_choices])}\")\n    for q2, cb_len in enumerate(cb_lengths):\n        #print(f\"   cb_len={cb_len:4}: \",end=\"\")\n        for q3, K in enumerate(K_choices):\n            \n            residual_vq = ResidualVQ(\n                dim = n_dim,\n                codebook_size = cb_len,\n                num_quantizers = K,\n                kmeans_init = True,   # set to True\n                kmeans_iters = 10     # number of kmeans iterations to calculate the centroids for the codebook on init\n            )\n            torch.manual_seed(0)\n            x = torch.randn(1, npoints_hd, n_dim)\n            quantized, indices, commit_loss = residual_vq(x)\n            error = ((quantized - x)**2).mean()\n            #print(f\"  {error:.1e} \", end=\"\")\n            results[q1,q2,q3] = error\n        #print()\n    show_colored_data(results[q1], n_dim, cb_lengths, K_choices)     \n        \n\n\n\n\n\nd = 2:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n7.5e-02\n6.8e-03\n8.9e-04\n1.2e-04\n4.8e-06\n1.1e-07\n2.6e-09\n\n\ncb_len = 64\n3.1e-02\n1.7e-03\n1.3e-04\n1.3e-05\n2.1e-07\n3.5e-09\n6.9e-11\n\n\ncb_len = 256\n8.5e-03\n1.9e-04\n7.4e-06\n4.5e-07\n3.1e-09\n1.8e-11\n1.5e-13\n\n\ncb_len = 1024\n2.0e-03\n2.4e-05\n9.6e-07\n6.1e-08\n9.4e-11\n4.8e-14\n3.5e-15\n\n\ncb_len = 2048\n6.0e-04\n1.8e-06\n3.9e-08\n7.8e-10\n1.4e-12\n3.8e-15\n1.4e-15\n\n\n\n\n\n\n\n\nd = 3:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n1.8e-01\n3.5e-02\n7.7e-03\n1.8e-03\n1.3e-04\n1.0e-05\n9.1e-07\n\n\ncb_len = 64\n9.8e-02\n1.1e-02\n1.4e-03\n2.2e-04\n5.7e-06\n2.7e-07\n5.8e-09\n\n\ncb_len = 256\n3.6e-02\n1.7e-03\n9.8e-05\n9.6e-06\n1.1e-07\n1.8e-09\n4.2e-11\n\n\ncb_len = 1024\n1.2e-02\n2.7e-04\n8.0e-06\n4.5e-07\n1.5e-09\n4.2e-12\n2.4e-14\n\n\ncb_len = 2048\n4.4e-03\n4.7e-05\n9.3e-07\n2.5e-08\n7.5e-12\n3.8e-15\n1.8e-15\n\n\n\n\n\n\n\n\nd = 6:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n4.3e-01\n1.9e-01\n8.2e-02\n3.7e-02\n8.6e-03\n2.1e-03\n5.5e-04\n\n\ncb_len = 64\n3.2e-01\n1.0e-01\n3.3e-02\n1.1e-02\n1.3e-03\n1.7e-04\n2.2e-05\n\n\ncb_len = 256\n1.8e-01\n3.2e-02\n6.0e-03\n1.1e-03\n4.0e-05\n1.5e-06\n5.6e-08\n\n\ncb_len = 1024\n8.0e-02\n6.7e-03\n5.8e-04\n5.0e-05\n4.0e-07\n3.6e-09\n3.0e-11\n\n\ncb_len = 2048\n3.8e-02\n1.6e-03\n7.9e-05\n3.6e-06\n1.1e-08\n4.5e-11\n2.5e-13\n\n\n\n\n\n\n\n\nd = 8:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n5.3e-01\n2.8e-01\n1.5e-01\n8.2e-02\n2.4e-02\n7.7e-03\n2.6e-03\n\n\ncb_len = 64\n4.2e-01\n1.7e-01\n7.4e-02\n3.2e-02\n6.2e-03\n1.2e-03\n2.6e-04\n\n\ncb_len = 256\n2.7e-01\n7.1e-02\n1.9e-02\n5.2e-03\n3.9e-04\n3.0e-05\n2.2e-06\n\n\ncb_len = 1024\n1.3e-01\n1.8e-02\n2.5e-03\n3.5e-04\n6.6e-06\n1.3e-07\n2.7e-09\n\n\ncb_len = 2048\n6.6e-02\n4.7e-03\n3.4e-04\n2.8e-05\n1.8e-07\n1.6e-09\n1.5e-11\n\n\n\n\n\n\n\n\nd = 16:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n7.3e-01\n5.3e-01\n3.9e-01\n2.8e-01\n1.5e-01\n7.9e-02\n4.2e-02\n\n\ncb_len = 64\n6.4e-01\n4.0e-01\n2.6e-01\n1.6e-01\n6.6e-02\n2.7e-02\n1.1e-02\n\n\ncb_len = 256\n4.9e-01\n2.4e-01\n1.2e-01\n5.6e-02\n1.3e-02\n3.2e-03\n7.8e-04\n\n\ncb_len = 1024\n3.0e-01\n9.0e-02\n2.7e-02\n8.1e-03\n7.7e-04\n7.6e-05\n7.4e-06\n\n\ncb_len = 2048\n1.6e-01\n2.8e-02\n4.7e-03\n7.6e-04\n2.7e-05\n1.0e-06\n3.8e-08\n\n\n\n\n\n\n\n\nd = 32:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n8.5e-01\n7.2e-01\n6.1e-01\n5.1e-01\n3.7e-01\n2.6e-01\n1.9e-01\n\n\ncb_len = 64\n7.8e-01\n6.1e-01\n4.8e-01\n3.8e-01\n2.3e-01\n1.4e-01\n8.7e-02\n\n\ncb_len = 256\n6.7e-01\n4.4e-01\n3.0e-01\n2.0e-01\n9.0e-02\n4.1e-02\n1.8e-02\n\n\ncb_len = 1024\n4.6e-01\n2.2e-01\n1.0e-01\n4.7e-02\n9.9e-03\n2.3e-03\n5.1e-04\n\n\ncb_len = 2048\n2.7e-01\n7.1e-02\n1.9e-02\n5.1e-03\n4.2e-04\n3.6e-05\n2.6e-06\n\n\n\n\n\n\n\n\nd = 64:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n9.1e-01\n8.4e-01\n7.6e-01\n7.0e-01\n5.8e-01\n4.8e-01\n4.0e-01\n\n\ncb_len = 64\n8.7e-01\n7.6e-01\n6.7e-01\n5.8e-01\n4.5e-01\n3.5e-01\n2.7e-01\n\n\ncb_len = 256\n7.9e-01\n6.6e-01\n5.5e-01\n4.5e-01\n3.2e-01\n2.2e-01\n1.5e-01\n\n\ncb_len = 1024\n5.8e-01\n3.4e-01\n2.0e-01\n1.2e-01\n4.2e-02\n1.5e-02\n5.6e-03\n\n\ncb_len = 2048\n3.5e-01\n1.2e-01\n4.3e-02\n1.5e-02\n2.4e-03\n3.6e-04\n5.2e-05\n\n\n\n\n\n\n\n\nd = 128:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n9.5e-01\n9.1e-01\n8.6e-01\n8.2e-01\n7.4e-01\n6.7e-01\n6.0e-01\n\n\ncb_len = 64\n9.3e-01\n8.6e-01\n8.0e-01\n7.4e-01\n6.4e-01\n5.6e-01\n5.1e-01\n\n\ncb_len = 256\n8.6e-01\n7.9e-01\n7.2e-01\n6.5e-01\n5.4e-01\n4.4e-01\n3.6e-01\n\n\ncb_len = 1024\n6.5e-01\n4.7e-01\n3.4e-01\n2.5e-01\n1.3e-01\n6.0e-02\n2.6e-02\n\n\ncb_len = 2048\n4.1e-01\n2.0e-01\n9.3e-02\n4.4e-02\n8.3e-03\n1.8e-03\n8.4e-05\n\n\n\n\n\n\n\n\nd = 256:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n9.7e-01\n9.4e-01\n9.2e-01\n8.9e-01\n8.4e-01\n8.0e-01\n7.5e-01\n\n\ncb_len = 64\n9.5e-01\n9.1e-01\n8.7e-01\n8.4e-01\n7.8e-01\n7.4e-01\n7.1e-01\n\n\ncb_len = 256\n8.9e-01\n8.3e-01\n7.6e-01\n7.0e-01\n5.9e-01\n5.0e-01\n4.2e-01\n\n\ncb_len = 1024\n6.9e-01\n5.2e-01\n3.8e-01\n2.8e-01\n1.5e-01\n7.8e-02\n4.4e-02\n\n\ncb_len = 2048\n4.4e-01\n2.2e-01\n1.1e-01\n5.2e-02\n1.2e-02\n2.3e-03\n2.6e-04\n\n\n\n\n\n\n\n\nd = 512:\n\n\n \nK = 1\nK = 2\nK = 3\nK = 4\nK = 6\nK = 8\nK = 10\n\n\n\n\ncb_len = 25\n9.8e-01\n9.7e-01\n9.5e-01\n9.3e-01\n9.0e-01\n8.7e-01\n8.4e-01\n\n\ncb_len = 64\n9.7e-01\n9.4e-01\n9.3e-01\n9.1e-01\n8.8e-01\n8.4e-01\n8.0e-01\n\n\ncb_len = 256\n9.1e-01\n8.6e-01\n7.9e-01\n7.3e-01\n6.3e-01\n5.4e-01\n4.6e-01\n\n\ncb_len = 1024\n7.1e-01\n5.3e-01\n3.9e-01\n2.9e-01\n1.5e-01\n8.2e-02\n4.5e-02\n\n\ncb_len = 2048\n4.6e-01\n2.2e-01\n1.1e-01\n5.0e-02\n1.2e-02\n2.3e-03\n6.1e-04\n\n\n\n\n\n\nClarification: RVQ with K=1 is the same thing a regular VQ.\n\n\n\nShow the code\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default='notebook'\nimport plotly.express as px \nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\ndef plot_results(results, d_choices, cb_lengths, K_choices, log_xy=True, log_z=False, vary_K=False,\n                camera=dict( up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=-0.2), eye=dict(x=1.3, y=1.3, z=1.25)),\n                width=750,):\n    y, ylabel = np.array(d_choices), 'd'\n    if vary_K:\n        x, xlabel = np.array(K_choices), 'K'\n        z = results[:,-1,:] \n    else:\n        x, xlabel = np.array(cb_lengths), 'cb_len'\n        z = results[:,:,3] \n    z, zlabel = z.numpy(), 'error'\n\n    if log_xy:\n        if not vary_K: \n            x, xlabel = np.log10(x), f\"log10( {xlabel} )\"\n        y, ylabel = np.log10(y), f\"log10( {ylabel} )\"\n    if log_z:\n        z, zlabel = np.log10(z), f\"log10( {zlabel} )\"\n\n    X, Y = np.meshgrid(x, y)\n\n    fig = go.Figure(data=[go.Surface(x=X, y=Y, z=z, )], )\n    fig.update_layout(scene = dict(xaxis_title=xlabel,yaxis_title=ylabel,zaxis_title=zlabel))\n    fig.update_layout(template='plotly_dark',\n        autosize=False,\n        width=width,\n        height=400,\n        margin=dict(l=20,r=20,b=10,t=20,pad=4),\n        #paper_bgcolor=\"LightSteelBlue\",\n        scene_camera = camera,\n    )\n    return fig\n\nfig = plot_results(results, d_choices, cb_lengths, K_choices, log_xy=True, log_z=False)\nfig.show()\n\n\n\n\n                                                \nFigure 18. (Interactive) Surface plot of reconstruction error for 4 codebooks with various data dimensions and codebook lengths.\n\n\n\nSo as we expect, longer codebooks help, and at higher dimensions, error tends to be larger.\nHere’s the same thing but with a logarithm axis for the error:\n\n\nShow the code\ncamera=dict( up=dict(x=0, y=0, z=1), center=dict(x=-0.15, y=0, z=-0.2), eye=dict(x=1.2, y=-1.4, z=1))\nfig = plot_results(results, d_choices, cb_lengths, K_choices, log_xy=True, log_z=True, camera=camera)\nfig.show()\n\n\n\n\n                                                \nFigure 19. (Interactive) Surface plot of reconstruction error for 4 codebooks with various data dimensions and codebook lengths. (Logarithmic z-axis)\n\n\n\nAnd now let’s look at the error for a codebook length of 2048, where we vary the number of codebooks:\n\n\nShow the code\ncamera=dict( up=dict(x=0, y=0, z=1), center=dict(x=-0.1, y=0, z=-0.22), eye=dict(x=0.3, y=-1.8, z=1))\nfig = plot_results(results, d_choices, cb_lengths, K_choices, vary_K=True, log_xy=True, log_z=True, camera=camera)\nfig.show()\n\nfig, ax = plt.subplots(figsize=(6.5,4))\nourdata = results[:,-1,:] # max cb_len\nfor q1, d in enumerate(d_choices):\n    x = np.array(K_choices)\n    y = ourdata[q1]\n    ax.semilogy(x,y,'o-',label=f\"d = {d}\")\n    \nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nax.set_xlabel(\"K\")\nax.set_ylabel(\"error\")\nplt.show()\n\n\n\n\n\n\n\n                                                \n(a) (Interactive) 3D Surface Plot\n\n\n\n\n\n\n\n\n(b) Line plot\n\n\n\n\nFigure 20. Reconstruction error for codebook length of 2048 with various data dimensions and number of codebooks (K). (Logarithmic z-axis)\n\n\n\nNote that in the last figure, we see straight lines1 across as we vary K, and the K-axis is linear and the error axis is logarithmic.\\(^*\\) What this means is – and this is a major takeway:\n\n\n\n\n\n\nBig Idea\n\n\n\nAs we add codebooks to the RVQ algorithm, the error decreases exponentially!\n\n\nThis is a big “selling point” for the RVQ method: you get an exponential payoff in error reduction for a linear increase in computational cost."
  },
  {
    "objectID": "posts/2023-06-12-RVQ.html#footnotes",
    "href": "posts/2023-06-12-RVQ.html#footnotes",
    "title": "Residual Vector Quantization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, the lines flatten out when we reach machine precision of \\(10^{-15}\\); don’t worry about that.↩︎\nBrand new” as in “came out while I was finishing this post”!↩︎"
  },
  {
    "objectID": "posts/2022-11-17-byol.html",
    "href": "posts/2022-11-17-byol.html",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "",
    "text": "“In this work, we thus tasked ourselves to find out whether…negative examples are indispensable to prevent collapsing while preserving high performance.” – the BYOL paper"
  },
  {
    "objectID": "posts/2022-11-17-byol.html#references",
    "href": "posts/2022-11-17-byol.html#references",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "References:",
    "text": "References:\nThis post is indebted to * The Bootstrap Your Own Latent (BYOL) paper by Grill et al – you could just stop reading my blog and read that, it’s not a scary paper! (And yes, actually use the arXiv version because it’s got extra figures not in the NeurIPS version I found.)\n* The wonderful tutorial from The AI Summer by Nikolas Adaloglou * The “BYOL-A: BYOL for Audio” paper by Nizumi et al * The byol-pytorch code repository by Phil Wang aka lucidrains"
  },
  {
    "objectID": "posts/2022-11-17-byol.html#review",
    "href": "posts/2022-11-17-byol.html#review",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "Review",
    "text": "Review\nPreviously, on “Trying to Understand Embeddings, with Scott”, i.e. Part 3 of my blog series, we’d worked out way to think of embeddings, and contrastive losses, and even built a toy model.\nIn the toy model there were pairwise losses (boo!) and triplet losses (yay!), and even an “Attract Only” option whereby we got rid of ‘repulsion’ entirely. After the “Attract Only” ran, we would rescale the answers and that rescaling would produce a kind of “repulsion”. In that sense, the “Attract Only” method was one way to “remove the contrastive loss” thing.\n…uh… but as far as I know, nobody does that. The SimCLR (“sim-clear”) method mentioned a bit in earlier posts and elsewhere is one way of dealing with the problem of finding “challenging” negative examples, by working on a kind of “attraction”, but not as naive as the toy model I made.\nBYOL is another way to simplify ‘contrastive’ learning and avoid hard-negative mining and it seems a bit like “attract only” in that it no longer means explicitly including a respulsive term in the loss function, but BYOL different from SimCLR and not as naive as my own scheme. Instead, BYOL, uses an another network to do some comparisons.\nRecal that the goal of these systems is to get “good”, “semantically meaningful” representations, however we can. If it takes multiple networks to do that, no worries.\nIn Part 2 of this blog series, we looked at Siamese Networks, where two copies of the same network are employed for pairwise contrastive learning. With BYOL however, the two networks have the same architectures but different weights, and this difference helps to force “semantically interesting” embedding choices.\n\nAnthropomorphism: The use of two very different networks to try to arrive at similar embedding points is akin to having two very different people talk about something (while each trying on lots of very different funny-colored eyeglasses!) and iteratively refine their understanding through discussion until they can come to (some sufficient level of) agreement."
  },
  {
    "objectID": "posts/2022-11-17-byol.html#strategy-how-were-going-to-do-this",
    "href": "posts/2022-11-17-byol.html#strategy-how-were-going-to-do-this",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "Strategy: How we’re going to do this",
    "text": "Strategy: How we’re going to do this\nI’m a firm believer in toy models, so my plan is to use the Fashion-MNIST dataset and then BYOL-embed a 3-dimensional set of represenations that we can look at and play with.\nOh, and since BYOL is a self-supervised method, we’re going to throw away the labels from Fashion-MNIST ;-)."
  },
  {
    "objectID": "posts/2022-11-17-byol.html#how-byol-works",
    "href": "posts/2022-11-17-byol.html#how-byol-works",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "How BYOL Works",
    "text": "How BYOL Works\n\nIn Diagrams\nFirst let’s steal multiple diagrams that all attempt to show the same thing.\nFrom the original BYOL paper, we have this one: \n\n“BYOL’s goal is to learn a representation \\(y_\\theta\\) which can then be used for downstream tasks.” – the BYOL paper.\n\n\nSo beyond the “representation” parts we want to ultimately use, we’ll tack on additional “projection” parts (and even a “prediction” part) to facilitate the training.\n\nLater in the BYOL paper (Figure 8), we have this version: \nAnd from the BYOL-A paper we have this version: \nIn each case, what we see are 3 main parts:\n\nA single input (\\(x\\)) gets modified in two different ways (\\(v\\) and \\(v')\\). The two different ways are termed views.\n\nEach view is sent through a different network (“online” or “target”) and gets mapped to the same embedding space.\n\nThen the loss/training is about minimizing the distance between those points in the embeddings space.\n\n\n\n\n\n\n\nNote\n\n\n\nthere is only minimizing, i.e. “attraction” going on. There is no “repulsion”.\n\n\nThose were broad strokes. What about the details? What about the “exponential moving average” bit, and the \\(q_\\theta(z_\\theta)\\) and \\(z'_\\xi\\), and that…equation? We’ll get there.\n\nNote also that we don’t “want to keep” those points \\(q_\\theta(z_\\theta)\\) and \\(z'_\\xi\\), they’re just used along the way to help us learn the representations \\(y_\\theta\\).\n\n\n\nContrastive Learning Without Contrastive Loss, via Different Networks\nThe two networks aren’t totally different. If you look at the second diagram above (with the dogs), you’ll see that the the first couple layers (in yellow) are of the same types: ResNet then MLP. They don’t have the same weights, but the weights are “related”.\nAnd one of the networks (the “target”) learns “slower” than the other (“online”) network… in a sense. This is the “exponential moving average” (EMA) part. EMA gets used in many contexts in machine learning (ML) to try to help keep things stable so that the system doesn’t jump around too much, i.e. to keep the system from behaving erratically. Think of reinforcement learning, where you want your robot to smoothly improve its position information instead of undergoing wild overcorrections.\n\n\n\n\n\n\nNote\n\n\n\nIn some ML contexts, the slower-learning network is called the “teacher” rather than the “target”, and the “online” network is termed the “student”. Here’s a link for an influential paper on “Teacher-Student” models. I find this terminology to be counter-intutitive because in the BYOL case the “student” would be teaching the “teacher”.\n\n\nThe target network gets its weights only from the EMA of the corresponding weights in the online network. The target weights are not obtained via gradient descent; only the online weights are updated via gradient descent.) In other words, if the online weights are \\(\\theta\\) and the target weights are \\(\\xi\\), then the EMA operation consists of\n\\[\\xi \\leftarrow \\tau \\xi + (1 - \\tau) \\theta, \\] for some choice of the “EMA spread/strength” (hyper)parameter \\(\\tau\\).\nThe terms “target” and “online” can also refer to the representation “points” in the embedding space. Using such terminology, the BYOL paper explains the method this way:\n\n“the core motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation. From there, we can expect to build a sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as new target networks for further training…”\n\n…i.e. we update the target (a bit, using the EMA) and do it all again.\nOk, so then what’s with the extra “projection” and “prediction” layers?\n\n\nThe Mapping Functions\n\n\n\n\n\n\nNote\n\n\n\nThe BYOL authors use the subscript “\\({}_\\theta\\)” to refer to weights in the online network, and the subscript “\\({}_\\xi\\)” to refer to weights in the target network. Vectors in the target network are also denoted via primes, e.g., \\(v'\\), \\(y'\\), \\(z'\\).\n\n\n\nEncoder \\(f\\) (\\(f_\\theta\\) and \\(f_\\xi\\)): Views (i.e., \\(v\\) and \\(v'\\), i.e., augmented versions of the input \\(x\\)) are mapped to embeddings \\(y\\) (\\(y_\\theta\\) in the online network) via the “encoder” function \\(f\\) (\\(f_\\theta\\) online). And remember, “BYOL’s goal is to learn a representation \\(y_\\theta\\) which can then be used for downstream tasks.” For images, \\(f\\) is typically a ResNet.\nProjector \\(g\\) (\\(g_\\theta\\) and \\(g_\\xi\\)): Maps the embeddings \\(y\\) to points \\(z\\) in the space where loss will be evaluated. In particular, \\(z'_\\xi\\) is important because it’s a point output by the target network, which the online network is going to try to “predict”. \\(g\\) can just be an MLP (though see below for comments about BatchNorm).\nPredictor \\(q_\\theta\\): is only on the online network. The predictors output \\(q_\\theta(z_\\theta)\\) is the online network’s prediction of the target network’s output \\(z'_\\xi\\).\n\n\n\nHold up: Questions\n\nWhy’s the predictor there at all? In other words, why can’t we just compare \\(z_\\theta\\) and \\(z'_\\xi\\) without this additional \\(q_\\theta\\) function?\nAnd for that matter, why can’t we just compare \\(y_\\theta\\) and \\(y'_\\xi\\) directly?\n\nLet’s answer these in reverse order:\n\n\nComparing \\(y_\\theta\\) and \\(y'_\\xi\\) is what we were already doing before with ordinary contrastive losses.\n\n\nAnd then SimCLR came along and introduced an additional mapping function akin to our “projector” \\(g\\) in which we could compare \\(z_\\theta\\) and \\(z'_\\xi\\) – so that’s been tried already. And it does work quite well for assisting with contrastive representation learning without having to worry to much about finding “hard negatives”.\n\n\nBut now we’re trying something different, with the goal of avoiding negative examples (i.e. contrastive losses) and the goal of…beating SimCLR. ;-) So bear with this discussion!\n\n\nThe Loss\nWe define a loss in the “projected” space between the points \\(q_\\theta(z_\\theta)\\) and \\(z'_\\xi\\), that’s just the ordinary mean L2 norm (“Euclidean distance”) between them. So\n\\[\\mathcal{L}_{\\theta\\xi}= ||\\bar{q_{\\theta}}(z_\\theta) - \\bar{z}'_\\xi||_2^2\\]\nOr you can write it in terms of a dot product normalized by the magnitudes, which is what we see written in the BYOL paper:\n\\[\\mathcal{L}_{\\theta\\xi} = 2 - 2\\cdot\\frac{\\langle q_\\theta(z_\\theta),  z'_\\xi \\rangle }{\\big\\|q_\\theta(z_\\theta)\\big\\|_2\\cdot \\big\\|z'_\\xi\\big\\|_2  }\n\\]\nIf that reminds you of a cosine similarity – good, because that’s exactly what it is. See, the graph of \\(2(1-\\cos x)\\) has a nice minimum when its argument is zero, kind of like a parabola on a certain domain:\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport matplotlib.pyplot as plt\nimport numpy as np \nx = np.linspace(-np.pi, np.pi,num=100)\nplt.xlabel('x')\nplt.ylabel('2 - 2 cos(x)')\nplt.plot(x, 2*(1-np.cos(x)))\n\n\n\n:::\nOne other thing they do is to “symmetrize” the loss by also passing \\(v'\\) through the online network and passing \\(v\\) through the target network, to compute what they call \\(\\tilde{\\mathcal{L}}_{\\xi\\theta}\\), and then the full loss is the sum of these two losses:\n\\[\\mathcal{L}^{\\rm BYOL} = \\mathcal{L}_{\\theta\\xi} + \\tilde{\\mathcal{L}}_{\\xi\\theta}\\]\n\n\nFurther Theory-Reading\n\nStill you may ask, “But whyyyyyyy does this work?” Here’s a good paper that tries to answer just that: “Understanding self-supervised Learning Dynamics without Contrastive Pairs” by Tian et al (2021).\n“What’s up with the batch norm / group norm stuff that appears in some discussions of BYOL?” (which I may or may not have mentioned above, LOL): See “BYOL works even without batch statistics” by Richemond et al (2020)."
  },
  {
    "objectID": "posts/2022-11-17-byol.html#lets-go-quick-implemenation-for-images",
    "href": "posts/2022-11-17-byol.html#lets-go-quick-implemenation-for-images",
    "title": "BYOL - Contrastive Representation Learning without Contrastive Losses",
    "section": "Let’s Go! Quick Implemenation for Images",
    "text": "Let’s Go! Quick Implemenation for Images\nIn a later post we can talk about writing our own implmentation from scratch (e.g. for something other than images, such as audio). But to just get started with all this, what better place to start a coding implementation than lucidrains’ repository? It’s super easy to install:\n\n%pip -qq install byol-pytorch\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n…and we can just “tack it on” to whatever network/task we might have. He provides a sample use case in his README which we’ll modify slightly. First, he sets up a simple test using random images, which we’ll run a version of now:\n\nimport torch\nfrom byol_pytorch import BYOL \nfrom torchvision import models, transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm_notebook\n\n\nresnet = models.resnet50(weights=True) # this will download resnet50 weights. \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nif torch.device('cpu') == device: print(\"Warning: Running on the CPU.\")\n\nimage_size = 28   # size for fashion mnist images\n\nlearner = BYOL(     # lucidrains' class\n    resnet,\n    image_size = image_size,\n    hidden_layer = 'avgpool'\n).to(device)\n\nopt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n\ndef sample_unlabelled_images():\n    return torch.randn(20, 3, image_size, image_size).to(device)  # make batch of 20 RGB images from random pixels.\n\nfor i in tqdm_notebook(range(50)):\n    images = sample_unlabelled_images()\n    loss = learner(images)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    learner.update_moving_average() # update moving average of target encoder\n\n\n\n\nGreat! It works!\nNow, rather than using random images, we’ll use Fashion-MNIST. Let’s get the data…\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\nbatch_size = 128\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n\nAnd… we should note that we don’t have to use ResNet50 – in fact, we don’t have to use ResNet-Anything! We could specify some other model, which for our dataset, a very simple model could suffice.\nAnd/or, rather than a classifcation model, we could choose something like a U-Net, and then try to get the “interior” represenation of the U-Net to offer a more interesting represenation than it otherwise might.\nFor now, just to avoid having to deviate from lucidrains’ demo much, we will stick with pretrained ResNet and just “drop down” in complexity to resnet18:\n\nresnet = models.resnet18(weights=True) # reset resnet weights. \n\nlearner = BYOL(\n    resnet, \n    image_size=28, \n    hidden_layer = 'avgpool', # activations from this layer will be used as y_theta!\n    use_momentum = True # set to false for 'SimSiam' variant. https://arxiv.org/abs/2011.10566\n).to(device)\n\ndef train_it(learner, lr=3e-4, epochs=5, steps=200):\n    opt = torch.optim.Adam(learner.parameters(), lr=lr)\n    for e in range(epochs): \n        pbar = tqdm_notebook(range(steps), desc=f\"Epoch {e}/{epochs}: \")\n        for i in pbar:\n            images, labels = next(iter(train_dataloader))\n            images = images.to(device).tile([1,3,1,1]) # put on GPU & create RGB from greyscale\n            loss = learner(images)\n            pbar.set_postfix({'loss':f\"{loss.detach():.3g}\"})\n            pbar.refresh()\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            learner.update_moving_average() # update moving average of target encoder\n            \ntrain_it(learner) # operates on learner & resnet in-place\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspecting Our Results\nHow do we access and inspect the representations learned from this? lucidrains’ README tells us that we already specified that:\n\nthe name (or index) of the hidden layer, whose output is used as the latent representation used for self-supervised training.\n\n…So we specified the layer named “avgpool” as the layer of our network resnet whose activations will serve as our learned representations. We can print out the names of the layers to see where avgpool is (look way near the bottom):\n\nresnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\nSo pretty much all the way at the end, just before the last Linear layer. Let’s see how we can get these layer outputs / activations:\n\n# get some mo' images\nimages, labels = next(iter(train_dataloader))\nimages = images.to(device).tile([1,3,1,1]) # put on GPU & create RGB from greyscale\nimages.shape\n\ntorch.Size([128, 3, 28, 28])\n\n\nOne way is to use some code we can find in lucidrains’ source code…\n\nreps = learner.online_encoder.get_representation(images)\nreps.shape\n\ntorch.Size([128, 512])\n\n\nBut the ‘classic’ way to do this in PyTorch is to register a “forward hook”, as in:\n\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output.detach()\n    return hook\n\nresnet.avgpool.register_forward_hook(get_activation('avgpool'))\noutput = resnet(images)\nreps = activation['avgpool'].squeeze()\nreps.shape\n\ntorch.Size([128, 512])\n\n\nNote that our images are 28x28=784 monochrome pixels, so a “representation” via 512 points does not make much of a compression. ….But at this point, you can see the basics of how this works.\nNo promises, but I may do a later version of this blog where we write our BYOL code from scratch, and/or use a U-Net or some other architecture, and/or look more closely into BYOL-A, but for now, this seems like a reasonable stopping point. :-)"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html",
    "href": "posts/Transformers2-MusicBox.html",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "",
    "text": "Musicbox-Transformer image, via Stable Diffusion XL"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#managing-expectations",
    "href": "posts/Transformers2-MusicBox.html#managing-expectations",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "1.1 Managing Expectations",
    "text": "1.1 Managing Expectations\nWhat we’ll end up with is something akin to producing garbled Shakespeare, except it’s going to be musical. Garbled music seems much harder to tolerate than garbled Shakespeare, which is why I have delayed releasing this lesson for multiple months – I’ve kept trying to get better results. At this point, I’m resigned to the fact that this is a hard problem (even for MIDI!), and my current outputs are about “as good as they’re going to get”:\n\n\nIt's way harder to train a simple MIDI transformer to sound OK than a next-character GPT-type transformer to create text that makes at least basic sense. MIDI tokenizer choice is a big deal here also.\n\n— Mateusz Modrzejewski (@mamodrzejewski) November 26, 2023\n\n\n…and we’re going to keep the tokenizer simple.\n– Scott H. Hawley, December 2023\n\n\n\n\n\n\nNote\n\n\n\nIn what follows, I’ve often put sections and subsections in expandable “Details” blocks, to enable readers to pick and choose the level of depth they want to see. Enjoy!"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#the-cost-of-attention",
    "href": "posts/Transformers2-MusicBox.html#the-cost-of-attention",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.1 The Cost of Attention",
    "text": "2.1 The Cost of Attention\nThe attention mechanism per se that we covered in the previous post has a serious drawback: the computational cost scales like the sequence length squared, i.e. for a sequence length of \\(N\\), the cost is 𝒪(\\(N^2)\\). This is one reason that Large Language Models can be “Large” – trying to model long sequences results in big matrices!\n\n\nDetails\n\nVarious schemes have been proposed to reduce this cost, such as only applying attention over nearby tokens (so-called “sliding window” attention) or other methods that fall under the general heading of “matrix sparsification”. Here’s a slide from a talk I gave a few years ago, illustrating a few of these schemes:\n\n\n\nFigure 2. Various schemes for reducing the 𝒪(\\(N^2)\\) cost of Attention. Many such schemes involve replacing the Linear layers with convolution operations, such as with the local receptive fields shown in b) and c). (Source: S.H. Hawley, “Learning Tunable Audio Effects via Neural Networks with Self-Attention”, AES Virtual Symposium: Applications of Machine Learning in Audio, Sep 28, 2020.)\n\n\nFor this lesson, we’ll see how far we can get with the basic \\(N^2\\) attention. Beyond that, I encourage you to check the literature for whatever the latest/hottest cost-reducing model might be. Popular candidates include FlashAttention and PercieverIO. Even sliding windows are still in use in ways that some people find significant."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#positional-encoding-aka-positional-embeddings",
    "href": "posts/Transformers2-MusicBox.html#positional-encoding-aka-positional-embeddings",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.2 Positional Encoding (aka Positional Embeddings)",
    "text": "2.2 Positional Encoding (aka Positional Embeddings)\nMLPs and/or ConvNets don’t come with (much of) a sense of position, but giving them one can improve performance on tasks where ordering and position matters. There many ways people have have endowed such models with position-awareness and… TBH there’s not necessarily a “silver bullet” solution, so a variety of approaches can end up performing about the same. In what follows we’ll describe various schemes for doing PEs.\n\n\nDetails on PEs\n\n\n2.2.1 Some Options for PEs\n\nYou could just concatenate a “linear ramp” of values to provide some position values, but… you wouldn’t want to do that. You won’t get very good gradients, especially for long sequences.\nThere’s the orignal positional encoding from the AIAYN paper, which uses a set of sines & cosines of expontially-increasing wavelength. They tweaked these wavelengths for their use case and ended up with the following equations: \\[\n\\begin{aligned}\nPE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\n\\end{aligned}\n\\] But the authors of the Transformer paper said they tried a few different PE methods/equations – including letting the model learn the PEs – but found no significant impact on performance between them. So let’s add that to list, btw:\nYou could let the positional embeddings be learned via some trainable mapping. This is the approach that Karpathy uses in his mini-GPT tutorial, and it’s the approach we’ll use in this lesson as well.\nFrancois Fleuret in his “AttentionToy1D” demo, used a simplification of approach #2 by essentially turning it into a set of binary sequences that correspond to the vertices of a hypercube.\n“ALiBi” (Attention with Linear Biases) is a method whereby you don’t do positional encoding of the inputs, rather you insert the positional information into the attention calculation itself. It is slightly faster to execute than traditional PE and (probably?) offers less memory overhead, and yields comparable results. (I tried it, it didn’t make any difference so I left it out of htis lesson.)\nOne scheme, “Relative Position Embeddings,” was reported to make a big difference and it arose in the Music Transformer paper itself. Should we do this? Yes, probably. Are going to do that in this notebook? …Uh….no, but that would be a great exercise for a student!\n\n\n\n2.2.2 PEs: To Add or Concatenate?\nHere’s a design question: Should we add these PEs to our inputs or concatenate them? There are a variety of views on this, such as those described in this video.\nHere’s my opinion on this:\nYour token embeddings (for the words, or notes, or whatever) are already going to be very high-dimensional. Now, for low-dimensional spaces, adding things up can sort of “destroy” or “obscure” information, but when you’re upwards of 128, 256, 1028 dimensions, etc., then adding ends up being akin to a kind of concatenation anyway (just in some wonky subspace). If you were to concatenate, then sure, your positional info always stays separate from the content. But if you’re going to allow your overall (high-dimensional) embeddings to be learned anyway, then you might as well let the learning dynamic find useful ways to combine the position info with the content info. Otherwise you’re kind “wasting space” (with concatenation) and doing more calculations than you’d need compared to the “compressed” representation gained by adding learned embeddings.\n…But perhaps this is a matter of taste. I’m open to being convinced otherwise!\nIn the code that follows below, all our embeddigns will be learned and added together, and the position info be treated similarly. One exception may be an option to include the “ALiBi” scheme that essentially just moves the PEs to inside the Attention computation.\n\n\n\n\n\n\n\nTip\n\n\n\nA great source of instruction about all things related to Positional Encodings is the YouTube channel, “AI Coffee Break with Letitia”."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#residual-connections",
    "href": "posts/Transformers2-MusicBox.html#residual-connections",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.3 Residual Connections",
    "text": "2.3 Residual Connections\nIn the Transfomer model diagram of Figure 1, wherever you see arrows going around something and connecting to a yellow “Add & Norm” block, that is indicative of a “skip residual” or just “residual” connection. Readers will likely have seen residual / skip connections in other contexts, so we’ll make this section “Details”-expandable for those who haven’t encountered them.\n\n\nDetails\n\nThis is a pretty simple idea that yields payoffs in many areas of numerical modeling: You model the change in something instead of modeling the whole thing. The following flowchart illustrates this:\n\n\n\n\n\nFigure 3. Flowchart of a residual connection\n\n\nWith neural networks, this residual scheme has the added benefit of allowing for efficient backpropagation. There are claims that “skip residual” connections also help smooth the loss surface, such as this figure in a N(eur)IPS paper from 2017:\n\n\n\nFigure 4. Loss landscape picture purporting to show the “smoothing” effects of residual connections. (Source: Li et al.)"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#multi-head-attention",
    "href": "posts/Transformers2-MusicBox.html#multi-head-attention",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.4 Multi-Head Attention",
    "text": "2.4 Multi-Head Attention\nAs described in the previous lesson, Multi-Head Attention simply involves allowing differently-weighted attention operators (called “heads”) to perform attention operations, allowing them to focus on different “senses” of phrases, parts of speech, or in our case, musical structures.\n\n\n\n\nFigure 5. Diagram of Multi-Head Attention. The multiple attention “heads” are shown below as depthwise semi-transparent copies, which are then concatenated into “one big long thing” via the Concat operation, and then mapped into the the output for the module via the last Linear layer: (Source: AIAYN.)"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#layer-normalization",
    "href": "posts/Transformers2-MusicBox.html#layer-normalization",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.5 Layer Normalization",
    "text": "2.5 Layer Normalization\nThis alternative to Batch Normalization (in which we perform elementwise operations where we subtract the mean across the batch dimension and divide by the variance). Instead of doing this across the batch dimension, we do it across the feature dimension(s).\n\n\n\n\nFigure 6. (Mouse-over or press to expand.) Batch normalization vs. Layer normalization. (Source: PapersWithCode.)\n\n\nOne big advantage of doing our normalization across layers instead of batches is that it allows for small batch sizes. Language models are often large, so they take up a lot of memory, which means you might need very small batches. But small batches will not allow you to do very good batch normalization. So, LayerNorm helps us get good statistics for the mean and variance that we use to normalize."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#stacking-blocks-embeddings-of-embeddingsof-embeddings",
    "href": "posts/Transformers2-MusicBox.html#stacking-blocks-embeddings-of-embeddingsof-embeddings",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "2.6 Stacking Blocks: Embeddings of Embeddings…of Embeddings",
    "text": "2.6 Stacking Blocks: Embeddings of Embeddings…of Embeddings\nBy stacking layers of our transformers, i.e. feeding embeddings into layers that will render them as new embeddings, we wil increase the representational power of our model. In terms of code, this is easy to do by looping over a list of layers."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#learning-about-midi-data",
    "href": "posts/Transformers2-MusicBox.html#learning-about-midi-data",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "3.1 Learning About MIDI Data",
    "text": "3.1 Learning About MIDI Data\n\n\nDetails\n\nThe MAESTRO data wasn’t overly challenging if I only focussed on modeling the pitches, but music is pitch + timing (+ more). What’s more, I’d only ever seen MIDI music “quantized” into a grid-based “piano roll” format with a known time-signature, not realizing that that’s not what MIDI data really is. For our purposes, it’s worth noting that MIDI encodes several pieces of information on a per-note basis:\n\nWhat instrument played the note\nWhat pitch was played – an integer between 0 and 127.\nThe start time of the note – in units of the “MIDI clock” which usually ticks at 50,000 times a second. This is typically rendered as a floating-point number when using Python MIDI-processing packages such as Colin Raffel’s pretty-midi (which we will use)\nThe end time of the note - another float.\nThe “velocity” of the note – e.g. how hard a piano key was struck.\n\nSo, no time signature, no quarter-note, half-note, etc. But fine, taking a lead from the Google Tensorflow Demo for RNN Modeling, we can reduce each note to 3 numbers: 1. pitch 2. “step” in time since the previous note 3. duration of the note (i.e. end time minus start time).\n…and focus on single-instrument work, and disregard the velocity.\nEVEN THAT was too hard for the code below do with MAESTRO. I then noticed a paper by the Google team pointed out that quantizing the time into bins of 8 milliseconds was an ok simplification to make, but even this proved too hard for my model. (I could do 16 ms, but that sounded weird.)\n…so…. after surveying other things, I went with Bach Chorales, which actually are presented in piano-roll format quantized to 16th note time intervals. Then I had to convert those from JSON to true MIDI, because by then the rest of my code expected MIDI.\nI’ll spare you the details of that process, but note that the data link below is not the official dataset link, it’s my private stash where I converted the JSON to .mid."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#install-and-import",
    "href": "posts/Transformers2-MusicBox.html#install-and-import",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "3.2 Install and Import",
    "text": "3.2 Install and Import\n\n\nShow the code for system-wide installs\n# Somehow you need to install fluidsynth\n\nimport platform\nimport os\n\ndef install_package(package_name):\n    \"For installing system binaries on Linux (Ubuntu/Debia) or Mac (via Homebrew)\"\n    if platform.system() == 'Darwin':\n        os.system(f'brew install {package_name}')\n    elif platform.system() == 'Linux':\n        import getpass  # NOTE: colab doesn't need a password for sudo\n        password = getpass.getpass()\n        command = f\"sudo -S apt-get install {package_name}\" \n        os.popen(command, 'w').write(password+'\\n')\n    else:\n        print(\"Unsupported Operating System\")\n\n# commenting out b/c I keep re-running this notebook\n#install_package('fluidsynth')\n\n\n\n\nShow the code for pip installs\n!pip install -qq gdown pyfluidsynth pretty_midi torch numpy pandas midi-player multiprocess\n\n\n\n\nShow the code for imports\n# import (most of) what we'll need for the entire lesson\nimport collections\nimport datetime\nimport fluidsynth\nimport glob\nimport pandas as pd\nimport pretty_midi\n\nimport torch\nfrom torch import nn, einsum\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom typing import Optional\nfrom glob import glob\nfrom pathlib import Path\n\nimport multiprocess as mp   # multiprocess is a Jupyter-compatible fork of multiprocessing\nfrom functools import partial\n\nfrom tqdm import tqdm  # note: Quarto blog won't print output from tqdm.notebook\n#from tqdm.contrib.concurrent import process_map  # process_map throws errors on Mac :'-( \n\nimport random\nimport numpy as np \nimport pprint"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#download-and-inspect-the-data",
    "href": "posts/Transformers2-MusicBox.html#download-and-inspect-the-data",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "3.3 Download and Inspect the Data",
    "text": "3.3 Download and Inspect the Data\n\n\nShow the code for downloading the dataset\n# Get the data\ndata_source = 'jsb_chorales_midi'\n# TODO: add options for MAESTRO, others\n\ndata_dir = Path('midi_data') # generic name for whatever midi we might want\nREST_PITCH = 127  # special code  used to denote rests\n\n!gdown -O {data_source}.tgz 1MdJiNEgtkvCx9tnyQWcnEE5GPMY6ADUb\n!tar zxf {data_source}.tgz; rm -rf midi_data; ln -s {data_source} midi_data ;\n\n# get the list of MIDI filenames\nfilenames = sorted(glob(str(data_dir/'**/*.mid*'),recursive=True))\nprint('Number of files:', len(filenames))\n\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1MdJiNEgtkvCx9tnyQWcnEE5GPMY6ADUb\nTo: /Users/shawley/github/blog/posts/jsb_chorales_midi.tgz\n100%|█████████████████████████████████████████| 137k/137k [00:00&lt;00:00, 113MB/s]\nNumber of files: 382\n\n\nLet’s inspect one of the files – using the MIDIPlayer object created especially for this lesson!\n\n\nShow the code for MIDI player usage\nfrom midi_player import MIDIPlayer\nfrom midi_player.stylers import general, dark # I like dark mode\n\nmidi_file = filenames[0]\nMIDIPlayer(midi_file, 360, styler=dark, title=f\"midi_file = {midi_file}\")\n\n\n\n'\n            \nFigure 7. Example of the first MIDI file on our list. (Press the play button to listen.) These particular notes really were arranged on a “grid,” however I converted them to a series of MIDI “notes,” occurring one after the other in the file, with simultaneously-starting notes sorted in ascending order of pitch.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes the MIDIPlayer’s piano roll view will appear blank if the notes are below the bottom of the visible frame. Scroll downward inside the player to reveal all the notes. The piano roll also scrolls left & right.\n\n\nLet’s load a single file and convert it to a PyTorch tensor called notes_tensor, and we’ll go ahead and convert “start” and “end” times “step” and “duration” times. So, displaying the first several notes (using a pandas dataframe for style) we see…\n\n\nShow the code\ndef quantize_times(times, \n                   res_ms=8, # resolution in milliseconds. 8ms was deemed sufficient for MAESTRO.\n                   clamp_range_s=[0.0,4.0]):\n    quant_step = res_ms / 1000.0\n    q_times = torch.round(times / quant_step) * quant_step\n    if clamp_range_s is not None: \n        q_times = torch.clamp(q_times, clamp_range_s[0], clamp_range_s[1])\n    return q_times\n\ndef midi_file_to_tensor(filenames, i=None, keep_start_end=False, rest_pitches=[REST_PITCH]):  # filenames could just be a single filename\n    midi_file = filenames if i is None or type(filenames)==str  else filenames[i]\n    \"reads a single midi file and converts it to tensor with elements (pitch, step, duration)\"\n    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this can be very slow for long MIDI files (e.g. in MAESTRO)\n\n    # Sort the notes first by start time (and then by pitch if two notes start at the same time)\n    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n    #notes = torch.empty( (len(sorted_notes), 3 + 2*keep_start_end), dtype=torch.float32 ) # allocate storage\n    \n    prev_start = sorted_notes[0].start\n    notes = []\n    for i, note in enumerate(sorted_notes):\n        new_note =  torch.empty( (3 + 2*keep_start_end) , dtype=torch.float32)\n        if int(note.pitch) in rest_pitches: continue  # we can actually delete the rests! \n        new_note[0] = note.pitch\n        new_note[1] = note.start - prev_start  # step, i.e. time since start of previous note\n        new_note[2] = note.end - note.start    # duration\n        if keep_start_end:                     # might find it useful be able to keep these for easier analysis later\n            new_note[3] = note.start\n            new_note[4] = note.end\n        prev_start = note.start\n        notes.append(new_note)\n\n    notes = torch.vstack(notes)\n    notes[:,1:] = quantize_times(notes[:,1:])\n    return notes\n\nnotes_tensor = midi_file_to_tensor(midi_file)\npd.set_option('display.notebook_repr_html', True)\nnotes_df = pd.DataFrame(notes_tensor, columns=['pitch','step','duration']) # actually df's look nicer\n\n\n# making it look better in the blog: adjust width of pandas dataframes\ndisplay(HTML('''\n&lt;style&gt;\n.dataframe {\n    width: 40%; /* Adjust this value as needed */\n    margin-left: auto;\n    margin-right: auto;\n}\n.dataframe th, .dataframe td {\n    text-align: center;\n    width: auto !important;\n}\n\ntable {\n    margin-left: auto;\n    margin-right: auto;\n}\nth, td {\n    text-align: center;\n    min-width: 100px; /* Or any other width */\n}\n&lt;/style&gt;\n'''))\nHTML(notes_df[:8].to_html(index=False))\n\n\n\n\n\n\n\n\n\nTable 1. First 8 notes of notes_tensor\n\n\npitch\nstep\nduration\n\n\n\n\n53.0\n0.00\n0.48\n\n\n57.0\n0.00\n0.48\n\n\n60.0\n0.00\n1.44\n\n\n65.0\n0.00\n0.48\n\n\n52.0\n0.48\n0.48\n\n\n55.0\n0.00\n0.48\n\n\n72.0\n0.00\n0.24\n\n\n70.0\n0.24\n0.24\n\n\n\n\n\n\nWe’ll convert those floating point numbers to integers via the “tokenizer” below.\nNow we’ll load each song into an entry in a list called notes_tensor_list:\n\n\nShow the code\ndef files_to_tensor_list(filenames, keep_start_end=False, serial=True):\n    \"Reads MIDI files in parallel so should be reasonably fast. JSB Chorales are no prob but for MAESTRO you want this\"\n    # tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1) # Doesn't work on Mac\n    tensor_list = []\n    max_ = len(filenames)\n    if serial:\n        for i, filename in enumerate(tqdm(filenames)):\n            tensor_list.append(midi_file_to_tensor(filename,  keep_start_end=keep_start_end))\n    else:\n        with mp.Pool(processes=mp.cpu_count()) as p:\n            with tqdm(total=max_) as pbar:\n                for r in p.imap_unordered(partial(midi_file_to_tensor, filenames, keep_start_end=keep_start_end), range(0, max_)):\n                    tensor_list.append(r)\n                    pbar.update()\n    return  tensor_list\n\n\n\n\nnotes_tensor_list_filename = f'{data_source}_tensor_list.pt'         # we'll save the result to a file for easier re-reading next time\nread_all_midi_files = True # not os.path.exists(notes_tensor_list_filename) # check to see if it already exists\n\nif read_all_midi_files:\n    notes_tensor_list = files_to_tensor_list(filenames, serial=True)\n    torch.save(notes_tensor_list, notes_tensor_list_filename) # save for next time\nelse:\n    notes_tensor_list = torch.load(notes_tensor_list_filename) # just read from the last time we made one\n    \nprint(f\"\\nlen(notes_tensor_list) = {len(notes_tensor_list)}\")\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████| 382/382 [00:02&lt;00:00, 155.96it/s]\n\n\n\nlen(notes_tensor_list) = 382\n\n\nAlso, let’s make a single tensor all_notes out of all the notes. We’re only going to use this for analysis purposes; for everything else we’ll use the notes_tensor_list, which…I’m now going to abbreviate as notes_tl\n\nnotes_tl = notes_tensor_list\nall_notes = torch.vstack(notes_tl).type(torch.float32)\nprint(\"all_notes.shape = \",all_notes.shape)\n\nall_notes.shape =  torch.Size([78349, 3])"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#making-a-tokenizer",
    "href": "posts/Transformers2-MusicBox.html#making-a-tokenizer",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "3.4 Making a Tokenizer",
    "text": "3.4 Making a Tokenizer\nRather than try something sophisticated like calling MidiTok, we’re going to treat this like a “char-level RNN” and just regard each note as a “parallel” group of 3 tokens (one token for pitch, step, and duration). This means that we will need 3 “codebooks” that can encode & decode between true values and their tokens.\n\n\nDetails\n\nFor the pitch values, it’s pretty easy since there are up to 128 pitches. Probably they won’t all be used, so we could limit the range of pitches, however, I’m going to want to do some pitch-bending data augmentation, so the pitch tokens will just be int versions of their floats, indices in a 128-dimensional space.\nFor the timing – which I don’t plan to do any augmentation of – we’ll just write down what unique values are present. These will form the “vocabularies” that we often see used in NLP settings.\nOh, also: The timing values end up being reaaalllly long in some cases. Like, notes with 15 seconds in duration? I’m going to go ahead an clamp those to a maximum value of 4 seconds.\n\n\nShow the code\nfrom collections import defaultdict \n\ndef make_codebooks(all_notes, verbose=False):\n    codebooks = []\n    n_codebooks = all_notes.shape[-1] # should be 3\n    for i in range(n_codebooks): \n        if i==0:  # i=0 means pitch\n            cb_vals = torch.arange(128)  # just use all possible pitches \n        else:     # i!=0 means timing\n            cb_vals = all_notes[:,i].unique().sort()[0] \n        if verbose: print(f\"\\n---\\ncb {i}: cb_vals = {cb_vals}\")\n        codebooks.append({'encode':{np.round(k.item(),3): int(v) for v, k in enumerate(cb_vals)}, # codebooks go both ways\n                          'decode':{int(v): k for v, k in enumerate(cb_vals)}})\n        if verbose: print(f\" cb {i}: cb keys = {codebooks[-1]['encode'].keys()}\")\n    return codebooks\n\nall_notes[:,1:] = quantize_times(all_notes[:,1:])\ncodebooks = make_codebooks(all_notes)\n\nvocab_sizes = [len(cb['encode']) for cb in codebooks]\ndisplay(HTML(f\"vocab_sizes = {vocab_sizes}\"))\n\n# Just a nice way to display the codebooks in the blog post: \ndf1 = pd.DataFrame( [[k,v] for k,v in codebooks[1]['encode'].items()], columns=['steps: value','steps: index'])\ndf2 = pd.DataFrame( [[k,v] for k,v in codebooks[2]['encode'].items()], columns=['durs: value','durs: index'])\ndisplay(HTML(f\"Codebooks for steps & durs are {'' if df1.equals(df2) else 'not '}the same:\"))\n\nhtml1, html2 = df1.to_html(index=False),  df2.to_html(index=False)\n\n# Combine HTML strings with a space in between\ncombined_html = f'''&lt;div style=\"width: 100%; text-align: center;\"&gt;\n    &lt;table style=\"margin-left: auto; margin-right: auto;\"&gt; &lt;tr&gt;\n    &lt;td style=\"vertical-align: top;\"&gt;{html1}&lt;/td&gt;\n    &lt;td style=\"width: 50px;\"&gt;&lt;/td&gt;\n    &lt;td style=\"vertical-align: top;\"&gt;{html2}&lt;/td&gt;\n    &lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;'''\n\n# Display the combined HTML\ndisplay(HTML(combined_html))\n\n\n\nvocab_sizes = [128, 15, 28]\n\n\nCodebooks for steps & durs are not the same:\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nsteps: value\nsteps: index\n\n\n\n\n0.00\n0\n\n\n0.12\n1\n\n\n0.24\n2\n\n\n0.36\n3\n\n\n0.48\n4\n\n\n0.72\n5\n\n\n0.96\n6\n\n\n1.20\n7\n\n\n1.44\n8\n\n\n1.68\n9\n\n\n1.92\n10\n\n\n2.40\n11\n\n\n2.88\n12\n\n\n3.84\n13\n\n\n4.00\n14\n\n\n\n\n\n\n\ndurs: value\ndurs: index\n\n\n\n\n0.12\n0\n\n\n0.24\n1\n\n\n0.36\n2\n\n\n0.48\n3\n\n\n0.60\n4\n\n\n0.72\n5\n\n\n0.84\n6\n\n\n0.96\n7\n\n\n1.08\n8\n\n\n1.20\n9\n\n\n1.32\n10\n\n\n1.44\n11\n\n\n1.56\n12\n\n\n1.68\n13\n\n\n1.80\n14\n\n\n1.92\n15\n\n\n2.04\n16\n\n\n2.16\n17\n\n\n2.40\n18\n\n\n2.52\n19\n\n\n2.64\n20\n\n\n2.88\n21\n\n\n3.12\n22\n\n\n3.24\n23\n\n\n3.36\n24\n\n\n3.60\n25\n\n\n3.84\n26\n\n\n4.00\n27\n\n\n\n\n\n\n\n\n\nNow we need routines to convert back and forth between values and codebook indices. We’ll make a single function remap_vals() that can either go “forward” for encoding, or “backward” for decoding:\n\n\nShow the code\ndef remap_vals(seq, encdec_str, dtype=torch.long):\n    out = torch.zeros_like(seq, dtype=dtype)\n    for cb in range(seq.shape[-1]): \n        dict_map = codebooks[cb][encdec_str]\n        default_map = max(codebooks[cb][encdec_str].values())\n        out[:,cb] = torch.tensor([dict_map.get(np.round(x.item(),3),default_map) for x in seq[:,cb]], dtype=dtype) # that np.round(,3) is good for avoiding KeyErrors\n        #out[:,cb] = torch.tensor([dict_map[np.round(x.item(),3)] for x in seq[:,cb]], dtype=dtype)\n    return out \n\n# We'll be calling these 'encode' and 'decode functions', which really just pull up the applicable part of the codebooks\nencode = lambda s: remap_vals(s, 'encode')\ndecode = lambda s: remap_vals(s, 'decode', dtype=all_notes.dtype)\n\n\n# And let's do a little test\nmidi_seq = all_notes[0:6].clone()\nprint(\"Before encoding, midi_seq =\\n\",midi_seq)\ntoken_list = encode(midi_seq)\nprint(\"After encoding, token_list =\\n\",token_list)\nreturn_seq = decode(token_list)\nprint(\"After decoding, return_seq =\\n\",return_seq)\nassert torch.equal(midi_seq, return_seq), f\"Oops. midi_seq={midi_seq}, but return_seq={return_seq}. Should be the same\"\n\n\nmidi_seq[-1,-1] = 100.0  # give the last time a huge value to check that our mapper won't crash\ntoken_list = encode(midi_seq) # if it doesn't crash, we're good\nassert token_list[-1,-1] == list(codebooks[-1]['encode'].values())[-1], \"Big value should have gotten the last spot in the last codebook\"\n\nprint(\"Checks pass! :-)\")\n\n\nBefore encoding, midi_seq =\n tensor([[53.0000,  0.0000,  0.4800],\n        [57.0000,  0.0000,  0.4800],\n        [60.0000,  0.0000,  1.4400],\n        [65.0000,  0.0000,  0.4800],\n        [52.0000,  0.4800,  0.4800],\n        [55.0000,  0.0000,  0.4800]])\nAfter encoding, token_list =\n tensor([[53,  0,  3],\n        [57,  0,  3],\n        [60,  0, 11],\n        [65,  0,  3],\n        [52,  4,  3],\n        [55,  0,  3]])\nAfter decoding, return_seq =\n tensor([[53.0000,  0.0000,  0.4800],\n        [57.0000,  0.0000,  0.4800],\n        [60.0000,  0.0000,  1.4400],\n        [65.0000,  0.0000,  0.4800],\n        [52.0000,  0.4800,  0.4800],\n        [55.0000,  0.0000,  0.4800]])\nChecks pass! :-)\n\n\n\n3.4.1 More Checks:\nBefore moving on, it’s a good idea to double check: Can we really encode and decode an entire midi sequence? Let’s write some more utility files. Notably the midiplayer() routine which will allow us to play our PyTorch tensors directly in this notebook.\n\n\nShow the code\ndef get_startend(notes:torch.Tensor) -&gt; torch.Tensor:\n    \"integrates (step,duration) timing pairs to recover (start,end) info. concats them as new columns\"\n    newnotes = torch.zeros((len(notes), 5), dtype=notes.dtype, device=notes.device)\n    newnotes[:,:3] = notes[:,:3]\n    prev_start = 0.0\n    for i, note in enumerate(notes): \n        step, dur = note[1], note[2]\n        start = step  + prev_start \n        end   = start + dur\n        newnotes[i,3], newnotes[i,4] = start, end\n        prev_start = start   \n    return newnotes\n    \n\ndef notes_to_midi(notes:torch.Tensor, \n                  time_rescale=None, \n                  out_file: str = '',\n                  instrument_name: str = 'Acoustic Grand Piano',\n                  velocity: int = 64,  # default loudness for all notes\n                 ) -&gt; pretty_midi.PrettyMIDI:\n    notes = notes.clone() # just to avoid weird overwrites of memory addresses\n    if notes.min() &lt; 0.0:\n      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n      notes = notes * (notes &gt;= 0)\n    if time_rescale is not None: # just added this because sometime I want to slow/speed up\n        notes[:,1:] = notes[:,1:] *time_rescale\n        \n    pm = pretty_midi.PrettyMIDI()\n    instrument = pretty_midi.Instrument(\n        program=pretty_midi.instrument_name_to_program(\n            instrument_name))\n    \n    if notes.shape[-1] &lt; 5: notes = get_startend(notes)\n        \n    notes = notes.cpu().numpy()\n    prev_start = 0.0\n    for note in notes: \n        pitch, start, end = int(note[0]), note[3], note[4] \n        midi_note = pretty_midi.Note( velocity=velocity, pitch=pitch, start=start, end=end, )\n        instrument.notes.append(midi_note)\n        prev_start = start   \n        \n    pm.instruments.append(instrument)\n    if out_file: pm.write(out_file)\n    return pm\n\n\n\ndef midiplayer(notes_tensor, height=400, time_rescale=None, midi_file=\"/tmp/tmp.mid\", title='', styler=dark):\n    \"MIDIplayer that writes input tensor to temporary file\"\n    pm = notes_to_midi(notes_tensor, time_rescale=time_rescale, out_file=midi_file)\n    return MIDIPlayer(midi_file, height, styler=dark, dl=True, title=title)\n\n\nWe’re going to encode & decode, and write to a MIDI file, and play that midi file. Using the same file as above:\n\nmidiplayer(decode(encode(midi_file_to_tensor(filenames[0]))), title='Encode-Decode Test')\n\n'\n            \n\n\nYay! Moving on…"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#making-pytorch-datasets",
    "href": "posts/Transformers2-MusicBox.html#making-pytorch-datasets",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "3.5 Making PyTorch Dataset(s)",
    "text": "3.5 Making PyTorch Dataset(s)\nOur particular dataset already comes with a test/train/valid split in its subdirectores, so let’s use that split. We’ll go ahead and re-load the files:\n\n\nShow the code\n# tensor lists\ntrain_filenames = [x for x in filenames if '/train' in x]\nval_filenames   = [x for x in filenames if '/val'   in x]\ntest_filenames  = [x for x in filenames if '/test'  in x]\n\ntrain_notes_tl = files_to_tensor_list(train_filenames, serial=True)\nval_notes_tl   = files_to_tensor_list(val_filenames, serial=True)\ntest_notes_tl  = files_to_tensor_list(test_filenames, serial=True)\n\nfor name, tl in zip(['train','val','test'],[train_notes_tl, val_notes_tl, test_notes_tl]):\n    stack = torch.vstack(tl)\n    print(f\"{len(tl)} songs in {name}, {stack.shape[0]} notes\")\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:01&lt;00:00, 148.65it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:00&lt;00:00, 152.29it/s]\n100%|███████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00&lt;00:00, 140.92it/s]\n\n\n229 songs in train, 46660 notes\n76 songs in val, 15052 notes\n77 songs in test, 16637 notes\n\n\n…that’s not a ton of data. We should incude some data augmentation.\n\n3.5.1 Data Augmentation\nWe’ll augment the pitch values by raising & lowering them within an octave, except we’ll leave the rest pitch (=127 for the JSB chorales) alone since that sometimes gets used as a rest. And we’ll leave the timing values alone.\n\n\nShow the Data Augmentation code\ndef augment_data(data, pitch_shift=12, debug=True, extra_augs=False):\n    datanew = data.clone()                                     # avoid overwriting memory of data\n    # pitch\n    change = torch.randint(-pitch_shift, pitch_shift, (1,))    # how many semitones to change all the pitches\n    datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((change, 0, 0))  # change the pitches\n    if not extra_augs: return datanew   #only do pitches\n\n    if torch.rand(1) &lt; 0.2:                         # sometimes invert pitches? Probably not useful but anyway\n        datanew[ datanew[:,0] != REST_PITCH ] *= torch.tensor((-1, 1, 1))\n        datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((127, 0, 0))\n    \n    # time - if we sometimes increase each non-zero time-token by one, that should be ok, right? \n    if torch.rand(1) &lt; 0.2: # do step\n        datanew[ datanew[:,1] &gt; 0]  += torch.tensor((0,1,0))\n    if torch.rand(1) &lt; 0.2: # do duration\n        datanew[ datanew[:,2] &gt; 0]  += torch.tensor((0,0,1))\n        \n    # extra 'safety' constraint: clamp to range of valid values (of tokens)\n    for i, cb in enumerate(codebooks):\n        datanew[:,i] = torch.clamp(datanew[:,i], 0, len(cb['encode'])-1)\n    return datanew\n\n\n# testing code: \ntorch.manual_seed(3) # setting this just  to make sure something happens ;-) \ndata = torch.tensor([[54,12,6],[61,0,40],[127,14,4],[86,0,12],[126,7,12]])\nprint(\"data.shape = \",data.shape)\nprint(\"original data = \\n\",data)\naug = augment_data(data)\nprint(\"augmented data = \\n\",aug) # \n\nassert not torch.equal(aug[:,0], data[:,0]), \"Oops, nothing changed\"\nassert aug[2,0]==data[2,0], \"Oops,  The 127 got changed\"\nprint(\"Checks passed! :-) \")\n\n\ndata.shape =  torch.Size([5, 3])\noriginal data = \n tensor([[ 54,  12,   6],\n        [ 61,   0,  40],\n        [127,  14,   4],\n        [ 86,   0,  12],\n        [126,   7,  12]])\naugmented data = \n tensor([[ 52,  12,   6],\n        [ 59,   0,  40],\n        [127,  14,   4],\n        [ 84,   0,  12],\n        [124,   7,  12]])\nChecks passed! :-) \n\n\n\n\n3.5.2 Dataset Object Definition\nHere we go with the dataset:\n\n\nShow the Pytorch Dataset code\nclass NotesDataset(Dataset):\n    \"simple custom dataset of sliding windows\"\n    def __init__(self, \n                 tensor_list, \n                 seq_length:int, \n                 tokenizer=encode,\n                 codebooks=codebooks, \n                 aug_callback=augment_data,\n                 len_mult=100,  # factor to 'fudge' the dataset length when it's inspected by DataLoaders\n                 pad=True, # pad end with rests\n                ):\n        super().__init__()\n        self.sl = seq_length\n        self.len_mult = len_mult\n        self.data_list = [tokenizer(t) for t in tensor_list] # encoded tokens are all we'll use\n        if pad:\n            rests = torch.tensor([REST_PITCH,1,1]).unsqueeze(0).tile((seq_length,1))\n            self.data_list = [torch.cat((toks,rests), dim=0) for toks in self.data_list]\n        self.aug_callback = aug_callback\n\n    def __len__(self):\n        \"\"\"we're going to be grabbing random windows from the data, so just the len of the tensor \n        list will be too small for large batch sizes, hence we multiply by len_mult\"\"\"\n        return len(self.data_list)*self.len_mult  # this will keep the DataLoader going longer\n\n    def __getitem__(self, idx, shift=1) -&gt; (torch.Tensor, torch.Tensor):\n        \"grabs a random 'window' from a random song, with an offset of `shift` tokens between inputs and targets\"\n        i_song = torch.randint(0, len(self.data_list), (1,)) # pick a song\n        ix =  torch.randint(0, len(self.data_list[i_song]) - self.sl - 1, (1,))  # start of window within song\n        data_block = self.data_list[i_song][ix:ix+self.sl+1]  # grab window plus an extra character\n        if self.aug_callback is not None: \n            data_block = self.aug_callback(data_block)\n        inputs, targets = data_block[:self.sl], data_block[shift:self.sl+shift]\n        return inputs, targets\n    \n\nseq_length = 64\ntrain_ds = NotesDataset(train_notes_tl, seq_length)\nval_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n# save test_ds for Evaluation section, later\nlen(train_ds), len(val_ds)\n\n\n(22900, 76000000)\n\n\nAnd before we use it, let’s seed all the Random Number Generators for the sake of reproducibility\n\n\nShow the Pytorch Dataset code\n#  set RNG seeds for reproducibility.\ndef set_seeds(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) \n        \nset_seeds(0)\n\nbatch_size = 128 # We may change this further down, for now it's just a test\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, )\n\nbatch_x, batch_y = next(iter(train_dl))\nprint(\"batch_x.shape, batch_y.shape = \",batch_x.shape, batch_y.shape)\n\n\nbatch_x.shape, batch_y.shape =  torch.Size([128, 64, 3]) torch.Size([128, 64, 3])\n\n\nOk. Now that we have a feel for how to handle our data, let’s get setup for the Transformer!"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#set-up-the-gpucpu-device",
    "href": "posts/Transformers2-MusicBox.html#set-up-the-gpucpu-device",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "6.1 Set Up the GPU/CPU device",
    "text": "6.1 Set Up the GPU/CPU device\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(\"device is\",device)\n\ndevice is mps"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#prompt-for-demos",
    "href": "posts/Transformers2-MusicBox.html#prompt-for-demos",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "6.2 Prompt for Demos",
    "text": "6.2 Prompt for Demos\nLet’s create a “prompt” from the validation dataset so we can monitor the models capabilities in producing “demos” of the music\n\n\nShow the code\ndemo_prompt_idx = 0    # file index from which to pull the demo prompt\ndemo_target = val_notes_tl[demo_prompt_idx]\ndisplay(midiplayer(demo_target, title=f\"Full demo 'target', {len(demo_target)} notes in length\"))\n\ndemo_prompt_length = 16  # number of notes in demo prompt context\ndemo_max_new_tokens = min(150, len(demo_target))  # try to make the whole song, but don't go on too long\nprompt = demo_target[:demo_prompt_length]\ndisplay(midiplayer(prompt, title=f\"{demo_prompt_length}-note 'prompt' for demos\"))\nprompt_tokens = encode(prompt).unsqueeze(0).to(device)\n\n\n'\n            \n\n\n'"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#re-init-things",
    "href": "posts/Transformers2-MusicBox.html#re-init-things",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "6.3 (Re-)Init Things",
    "text": "6.3 (Re-)Init Things\nJust to keep things ‘clean’, re-setup the datasets & loaders for good measure, instantiate a model and optimizer.\n\nset_seeds(0)\n\ntrain_ds = NotesDataset(train_notes_tl, config.seq_length, len_mult=config.batch_size)\nval_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n\ntrain_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\nval_dl   = DataLoader(val_ds,   batch_size=config.batch_size, shuffle=False)\n\nmodel = Transformer(config)\nmodel = model.to(device)\ntotal_params_str = f'{sum(p.numel() for p in model.parameters())/1e6} M'\nprint(total_params_str,'parameters in the model')  \n\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\n0.843947 M parameters in the model"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#optional-setup-wandb-run-tracking",
    "href": "posts/Transformers2-MusicBox.html#optional-setup-wandb-run-tracking",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "7.1 Optional: Setup WandB Run Tracking",
    "text": "7.1 Optional: Setup WandB Run Tracking\n\nuse_wandb = True  # Tip: leave this off at first, until you're sure everything's working!\nif use_wandb:\n    import wandb \n    \n    wandb.login()\n    wandb.init(project=\"musicbox-jsb-tutorial\", config=config)\n\nwandb: Currently logged in as: drscotthawley. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.15.4\n\n\nRun data is saved locally in /Users/shawley/github/blog/posts/wandb/run-20231213_201630-okamoljp\n\n\nSyncing run winter-rain-29 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/drscotthawley/musicbox-jsb-tutorial\n\n\n View run at https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/okamoljp"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#training-loop-go",
    "href": "posts/Transformers2-MusicBox.html#training-loop-go",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "7.2 Training Loop: Go!",
    "text": "7.2 Training Loop: Go!\n\n\nShow the code\nval_every  = 1 # in steps, evaluate loss on val dataset\ncp_every   = 60 # in epochs, checkpoint every\ndemo_every = 4 # in epochs, make a midi player demo\n\nbest_loss = 999\n\nema_weight, loss_ema, val_loss_ema = 0.95, None , None # exponential moving averages for loss reporting\nstep = 0 \nlosses = {}\nepochs = config.epochs\nfor epoch in range(1,epochs+1):\n    pbar = tqdm(total=len(train_dl), desc=f\"Epoch {epoch}/{config.epochs}\", dynamic_ncols=False, ncols=100) # progress bar, per epoch\n    for bi, batch in enumerate(train_dl):\n        step += 1\n        xb, yb = batch[0].to(device), batch[1].to(device)\n\n        logits, loss = model(xb, yb) # evaluate the loss\n        loss_ema = (1.0-ema_weight)*loss.item() + ema_weight*loss_ema if loss_ema is not None else loss.item()\n        losses['train'], losses['train_ema'] = loss.item(), loss_ema\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        # status / diagnostics:\n        if (step % val_every == 0):\n            with torch.no_grad():\n                model.eval()\n                xvb, yvb = next(iter(val_dl))\n                val_logits, val_loss = model( xvb.to(device), yvb.to(device) ) \n                val_loss_ema =  (1.0-ema_weight)*val_loss.item() + ema_weight*val_loss_ema if val_loss_ema is not None else val_loss.item() \n                losses['val'], losses['val_ema'] = val_loss.item(), val_loss_ema\n            model.train()\n            \n        wbl_dict = {'step':step, 'epoch':epoch} | losses   # dict for logging losses, midi examples, etc to wandb\n        pbar.set_postfix( dict((k,losses[k]) for k in ['train', 'val','val_ema'])) # loss info for progress bar\n        pbar.update(1)\n        if use_wandb and wbl_dict != {}: wandb.log(wbl_dict)\n            \n    #--- end of epoch ---\n\n    if losses['val_ema'] &lt; best_loss: # Tracking best val_loss_ema for checkpointing purposes\n        best_loss = losses['val_ema']\n        pbar.set_postfix(dict( (k,losses[k]) for k in ['train', 'val']) | {'BEST val_ema':best_loss})\n        if (epoch) % cp_every==0:   # occasionally save a checkpoint of best model/optimizer states\n            cp_file = f\"musicbox-jsb\" #    -{step}\" # let's leave out step to avoid filling disk\n            print(f\"Saving a checkpoint to {cp_file}\")\n            torch.save({ 'step': step, 'model_state_dict': model.state_dict(), 'loss': loss,\n                         'optimizer_state_dict': optimizer.state_dict(),}, cp_file)\n    pbar.refresh()\n    pbar.close()\n    \n    if (epoch % demo_every == 0) or (epoch==epochs):  # demo of midi generation\n        with torch.no_grad():\n            model.eval()\n            new_notes = decode( model.generate(prompt_tokens, max_new_tokens=demo_max_new_tokens, temperature=1)[0].cpu() )\n            p2 = midiplayer(new_notes,title=f\"Demo on val dataset, Epoch={epoch}\")\n            display(p2)\n            if use_wandb: wandb.log( {'step':step, 'player':wandb.Html(p2.html)} )              \n        model.train()\n\n\nEpoch 1/20: 100%|████████| 229/229 [00:47&lt;00:00,  4.81it/s, train=2.03, val=1.97, BEST val_ema=2.04]\nEpoch 2/20: 100%|████████| 229/229 [00:46&lt;00:00,  4.89it/s, train=1.74, val=1.71, BEST val_ema=1.72]\nEpoch 3/20: 100%|█████████| 229/229 [00:47&lt;00:00,  4.82it/s, train=1.44, val=1.54, BEST val_ema=1.5]\nEpoch 4/20: 100%|█████████| 229/229 [00:47&lt;00:00,  4.84it/s, train=1.48, val=1.44, BEST val_ema=1.4]\nEpoch 5/20: 100%|████████| 229/229 [00:49&lt;00:00,  4.67it/s, train=1.37, val=1.33, BEST val_ema=1.34]\nEpoch 6/20: 100%|█████████| 229/229 [00:48&lt;00:00,  4.73it/s, train=1.31, val=1.22, BEST val_ema=1.3]\nEpoch 7/20: 100%|████████| 229/229 [00:48&lt;00:00,  4.69it/s, train=1.28, val=1.24, BEST val_ema=1.27]\nEpoch 8/20: 100%|████████| 229/229 [00:46&lt;00:00,  4.88it/s, train=1.25, val=1.14, BEST val_ema=1.24]\nEpoch 9/20: 100%|████████| 229/229 [00:47&lt;00:00,  4.80it/s, train=1.18, val=1.25, BEST val_ema=1.23]\nEpoch 10/20: 100%|████████| 229/229 [00:48&lt;00:00,  4.72it/s, train=1.24, val=1.2, BEST val_ema=1.21]\nEpoch 11/20: 100%|████████| 229/229 [00:47&lt;00:00,  4.81it/s, train=1.12, val=1.24, BEST val_ema=1.2]\nEpoch 12/20: 100%|███████| 229/229 [00:50&lt;00:00,  4.55it/s, train=1.15, val=1.15, BEST val_ema=1.19]\nEpoch 13/20: 100%|████████| 229/229 [00:48&lt;00:00,  4.75it/s, train=1.12, val=1.2, BEST val_ema=1.19]\nEpoch 14/20: 100%|███████| 229/229 [00:48&lt;00:00,  4.71it/s, train=1.15, val=1.19, BEST val_ema=1.18]\nEpoch 15/20: 100%|███████| 229/229 [00:48&lt;00:00,  4.72it/s, train=1.06, val=1.24, BEST val_ema=1.18]\nEpoch 16/20: 100%|████████████| 229/229 [00:46&lt;00:00,  4.96it/s, train=1.02, val=1.22, val_ema=1.18]\nEpoch 17/20: 100%|████████████| 229/229 [00:48&lt;00:00,  4.70it/s, train=1.05, val=1.22, val_ema=1.18]\nEpoch 18/20: 100%|█████████████| 229/229 [00:47&lt;00:00,  4.82it/s, train=1.02, val=1.2, val_ema=1.19]\nEpoch 19/20: 100%|████████████| 229/229 [00:47&lt;00:00,  4.79it/s, train=1.04, val=1.21, val_ema=1.19]\nEpoch 20/20: 100%|████████████████| 229/229 [00:47&lt;00:00,  4.82it/s, train=1, val=1.15, val_ema=1.2]\n\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n  …and that’s about as good as it’s going to get for now. 😕 Moving on…"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#sample-generations",
    "href": "posts/Transformers2-MusicBox.html#sample-generations",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "8.1 Sample Generations",
    "text": "8.1 Sample Generations\nFirst we’ll set a “prompt” from the test set to see how the model continues it.\n\n\nShow the code\nfile_ind = 1  \noriginal = test_notes_tl[file_ind]\nnum_tokens = len(original)\ndisplay(midiplayer(original, title=f\"Full Evaluation Target, {num_tokens} notes long\"))\n\n\nprompt_len = 21\nprompt = original[:prompt_len] \ndisplay(midiplayer(prompt, title=f\"Evaluation Prompt, {prompt_len} notes long\"))\nprompt_tokens = encode(prompt).unsqueeze(0).to(device)\n\n\n'\n            \n\n\n'\n            \n\n\nIn what follows, we’ll vary the “temperature” parameter of the model (see the bottom of the previous lesson), to see its effect on what comes out.\n\nnew_tokens = num_tokens - prompt_len\n\nfor temperature in [ 0.7, 0.85, 0.92, 1.0, 1.2, 1.5]:\n    set_seeds(1337) # same temp for same seed will yield same output\n    notes = decode( model.generate(prompt_tokens, max_new_tokens=new_tokens, temperature=temperature)[0].cpu() )\n    display(midiplayer(notes, title=f\"Temperature = {temperature}\"))\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n'\n            \n\n\n'"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#perplexity-score",
    "href": "posts/Transformers2-MusicBox.html#perplexity-score",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "8.2 Perplexity Score",
    "text": "8.2 Perplexity Score\n“Perplexity” is a common metric for language models which comes from the field of information theory. It is often described as a measure of how “surprised” your model is by novel data.\nOperationally, it’s just the exponential of the value of the loss function – like, …yea, that’s it.\nWhy do people report perplexity instead of just the loss value (since the exponential function is monotonic)? Well…because of custom, and because perplexity has some natural relation to entropy, and because it relates to something called the “branching factor” of a language… but “because custom” is enough.\nSo, all we’re going to do is feed the test set into our model and measure the loss, and take the exponential of that and… that’s our perplexity! Lower is better.\n\n\nShow the code\ntest_ds = NotesDataset(test_notes_tl, config.seq_length, len_mult=config.batch_size, pad=False)\ntest_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=False)\n\ntotal, batches = 0, 0 \nwith torch.no_grad():\n    model.eval()\n    for i, batch in enumerate(tqdm(test_dl, ncols=100)): \n        batches +=1 \n        xb, yb = [q.to(device) for q in batch]\n        logits, loss = model(xb, yb)\n        total = total + loss.cpu().item()\nperplexity = np.exp(total/batches) \nprint(f\"(Average) Perplexity = {perplexity:5.3f}\")\n\n\n100%|█████████████████████████████████████████████████████████████| 229/229 [00:07&lt;00:00, 30.59it/s]\n\n\n(Average) Perplexity = 2.393\n\n\nInteresting. That seems really low. Perhaps all it means is that once you’ve seen a couple hundred Bach chorales, new ones aren’t very surprising! ;-)"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#accuracy-score",
    "href": "posts/Transformers2-MusicBox.html#accuracy-score",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "8.3 Accuracy Score",
    "text": "8.3 Accuracy Score\nThis is pretty simple: How often did the model guess the “correct” next note? …Uh… TODO later."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#data-distributions",
    "href": "posts/Transformers2-MusicBox.html#data-distributions",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "8.4 Data Distributions",
    "text": "8.4 Data Distributions\nLet’s compare statistics of true points vs. generated points. We’ll generate a lot of data, and then compare histograms:\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nall_test_notes = torch.vstack(test_notes_tl).type(torch.float32)\nall_test_notes = all_test_notes[:len(all_test_notes)//4]  # to save time, let's not do stats on ALL-all of them, just a quarter\n\n# generate a big batch of fake data. \n# WARNING: this is super slow because our generator only works for batches of one, oops\nprompt_len, max_new_tokens = 12, 54          # I just made these numbers up. not great science yet, sorry.\nnew_notes_list = []\nprint(\"Generating new notes...\") \nfor i, notes in enumerate(tqdm(test_notes_tl, ncols=100)): \n    ptoks = encode(notes[:prompt_len]).unsqueeze(0).to(device)\n    # generate more notes, chop off prompt before adding to list of all generated notes\n    new_notes_list.append( decode( model.generate(ptoks, max_new_tokens=max_new_tokens)[0].cpu()[prompt_len:] ) )\nall_new_notes = torch.vstack(new_notes_list).type(torch.float32)\n\nprint(f\"# of test notes = {len(all_test_notes)},  # of generated notes = {len(all_new_notes)} --- Close enough!\")\n\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nnames = ['pitch', 'step', 'duration']\nfor i in range(3):\n    bins = 32 if i !=0 else 0.5 + 2*np.arange(128//2) \n    ax[i].hist(all_test_notes[:,i].numpy(), bins=bins, label='test data', alpha=0.6)\n    ax[i].hist(all_new_notes[:,i].numpy(),  bins=bins, label='generated', alpha=0.6)\n    ax[i].set_title(names[i])\n    if i==2: ax[i].legend()\n    ax[i].set_xlabel('Value'+f'{\" (s)\" if i&gt;0 else \"\"}')\n    if i==0: ax[i].set_ylabel('Count')\nplt.show()\n\n\nGenerating new notes...\n# of test notes = 4159,  # of generated notes = 4158 --- Close enough!\n\n\n100%|███████████████████████████████████████████████████████████████| 77/77 [02:10&lt;00:00,  1.70s/it]\n\n\n\n\n\nHey! The generated and test data distributions look prety similar. The generated pitches have a slighly larger spread, which is consistent with what we observe in the generated examples, and may be due to the pitch-shifting in our data augmentation.\nI’m curious why the step values seem to be shifted to the right in the generated data; that’s a bit “sus” to me. But, that’ll be an investigation for another time. We need to wrap this up!"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#change-hyperparamters-dataset",
    "href": "posts/Transformers2-MusicBox.html#change-hyperparamters-dataset",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.1 Change Hyperparamters / Dataset",
    "text": "10.1 Change Hyperparamters / Dataset\nI tried a lot of hyperparameter tuning already. Now it’s your turn. Maybe even trying WandB’s “Sweeps”feature. Here’s a config suggestion that I used for the MAESTRO dataset:\n\n\nWatch out, there’s a Monster in this code!\n@dataclass\nclass LargeConfig:  # \"Monster\" size model ;-) \n    # model architecture details\n    seq_length: int = 128\n    batch_size: int = 256\n    n_embd:     int = 256     \n    n_heads:    int = 16      \n    n_blocks:   int = 8       \n    dropout:  float = 0.5    \n    bias:      bool = False\n\n    # training details\n    learning_rate: float = 0.001\n    weight_decay:  float = 0.04   # 0.01 is pytorch default\n    epochs:          int = 12\n\n    # other handy bookkeeping\n    vocab_sizes: tuple = tuple(vocab_sizes)"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#report-individual-parts-of-the-loss",
    "href": "posts/Transformers2-MusicBox.html#report-individual-parts-of-the-loss",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.2 Report Individual Parts of the Loss",
    "text": "10.2 Report Individual Parts of the Loss\nCurrently we’re not reporting the contributions of pitch, step, and durations to the loss, separately. Doing so could be instructive!"
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#improving-generation-by-implementing-beam-search",
    "href": "posts/Transformers2-MusicBox.html#improving-generation-by-implementing-beam-search",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.3 Improving Generation by Implementing Beam Search",
    "text": "10.3 Improving Generation by Implementing Beam Search\nRight now we’re just sampling a single token from the predicted probability distribution. If we instead predicted the most likely estimates by looking two or three steps into the future, that could greatly improve our results. Check out “beam search” algorithms."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#using-huggingfaces-transformers-instead",
    "href": "posts/Transformers2-MusicBox.html#using-huggingfaces-transformers-instead",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.4 Using Huggingface’s transformers instead",
    "text": "10.4 Using Huggingface’s transformers instead\ntrasformers is a package that is optimized for general applications and would probably outperform the above code in numerous ways."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#pitch-only-estimation",
    "href": "posts/Transformers2-MusicBox.html#pitch-only-estimation",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.5 Pitch-Only Estimation",
    "text": "10.5 Pitch-Only Estimation\nThe timing can be the most challenging part. If you take that out and do only pitches (i.e. assume all notes have the same spacing and duration), you may like what you hear."
  },
  {
    "objectID": "posts/Transformers2-MusicBox.html#raw-audio",
    "href": "posts/Transformers2-MusicBox.html#raw-audio",
    "title": "Understanding Transformers, Part 2: Wee Music Box",
    "section": "10.6 Raw Audio",
    "text": "10.6 Raw Audio\nIf you were to instead use raw audio, I’d recommend converting it to spectrograms, treating each column of the spectrogram as a “word vector.” However, there’d likely be a lot of “wasted space” so you’d probably want to (invertibly) compress the dimensionality of your “audio-word vectors” via something like (Residual) Vector Quantization, an autoencoder, or UMAP. …And then you’d need to expand/decode it on the output side to be able to listen. Relatedly, a pretrained sytem like Meta’s EnCodec would likely provide such functions nicely."
  },
  {
    "objectID": "posts/2018-12-16-Parallelizing-Python-Simplified.html",
    "href": "posts/2018-12-16-Parallelizing-Python-Simplified.html",
    "title": "Parallelizing Python, Simplified",
    "section": "",
    "text": "So you have some serial task that takes forever, and you’re thinking it should be parallelizable, but you find the documentation on this to be obtuse? Yea.\nUsually I’m interested in either creating lots of data in parallel, or inputting lots of data in parallel, and it’s often something that I first implemented as a loop but got tired of how slow it runs. These involve embarrassingly parallel tasks in that they don’t depend on one another.\nThere’s a simple prescription for parallelizing most of these kinds of tasks in Python. It goes as follows: 1. Have some kind of task performed in a for loop. 2. Write a function that does what you want for one “instance.” For example, take what’s inside one of your for loops, put all that in a separate function. 3. As a check, keep your loop but use only the function call. Make sure it produces the same results as the original version of your code. 4. Use functools.partial to create a wrapper for your function. 5. Replace the loop with a call to Pool.map().\nIn the following, we’ll cover 3 examples for parallel tasks: 1. Generate a bunch of files 2. Read a bunch of files into a list 3. Filling a numpy array"
  },
  {
    "objectID": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-1-generate-a-bunch-of-files",
    "href": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-1-generate-a-bunch-of-files",
    "title": "Parallelizing Python, Simplified",
    "section": "Example 1: Generate a bunch of files",
    "text": "Example 1: Generate a bunch of files\nLet’s say you have some important synthetic data that you want to generate lots of instances of. For now, for simplicity, we’re just going to generate images of, let’s say, random noise. And to make it interesting we’ll generate 2000 of them.\nHere’s the serial for-loop version:\nimport numpy as np\nimport cv2\n\nn_images = 2000\nsize_x, size_y = 100, 100\nfor i in range(n_images):\n    arr = 255*np.random.rand(size_x,size_y)\n    filename = 'image_'+str(i)+'.png'\n    print(\"writing file \",filename)\n    cv2.imwrite(filename,arr)\nNow we write a dedicated function, put it in a partial wrapper, and call it as follows:\nimport numpy as np\nimport cv2\nfrom functools import partial\n\ndef write_one_file(size_x, size_y, name_prefix, i):\n    arr = 255*np.random.rand(size_x,size_y)\n    filename = name_prefix + str(i) + '.png'\n    print(\"writing file \",filename)\n    cv2.imwrite(filename,arr)\n\nn_images = 2000\nsize_x, size_y = 100, 100\n\nwrapper = partial(write_one_file, size_x, size_y, 'image_')\nfor i in range(n_images):\n    wrapper(i)\nFinally we replace the loop with a multiprocessing pool. We can either use all the cpus on the machine (which is the default) or specify how many to use, by giving an argument to Pool():\nimport numpy as np\nimport cv2\nfrom functools import partial\nimport multiprocessing as mp\n\ndef write_one_file(size_x, size_y, name_prefix, i):\n    arr = 255*np.random.rand(size_x,size_y)\n    filename = name_prefix + str(i) + '.png'\n    print(\"writing file \",filename)\n    cv2.imwrite(filename,arr)\n\nn_images = 2000\nsize_x, size_y = 100, 100\n\nwrapper = partial(write_one_file, size_x, size_y, 'image_')\n\nnum_procs = mp.cpu_count() # or can replace with some number of processes to use\npool = mp.Pool(num_procs)\nindices = range(n_images)\nresults = pool.map(wrapper, indices)\npool.close()\npool.join()\nThere are other ways you can do this to get more control, e.g. to have each process in the pool receive a particular range of indices, but this basic setup will get the job done. And if you turn off the printing to screen and time the execution, you’ll see the speedup."
  },
  {
    "objectID": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-2-read-a-bunch-of-files-into-a-list",
    "href": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-2-read-a-bunch-of-files-into-a-list",
    "title": "Parallelizing Python, Simplified",
    "section": "Example 2: Read a bunch of files into a list",
    "text": "Example 2: Read a bunch of files into a list\nThis example is actually of limited utility and you may want to just skip down to “Example 3: Filling a numpy array,” but it’s still an illustrative example that motivates Example 3, and offers a bit of variety in how one might do things. In this case we’re not going to use Pool.map; instead we’re going to use a context manager for the particular datatype of list.\nLet’s try to load in all the image files we just generated, into a list. Here’s the serial version:\nimport glob\nimport cv2\n\nname_prefix = 'image_'\n# we'll use glob to get the list of available files\n# note that glob order isn't...easily discernible, so we'll sort.\nimg_file_list = sorted(glob.glob(name_prefix+'*.png'))\nn_files = len(img_file_list)\nprint(n_files,\"files available.\")\n\nimg_data_list = []\nfor i in range(n_files):\n    filename = name_prefix + str(i) + '.png'\n    print(\"Reading file\",filename)\n    img = cv2.imread(filename)\n    img_data_list.append(img)\n\nprint(len(img_data_list),\"images in list.\")\n(If we wanted to, we could easily convert this list of images to a numpy array. But let’s hold off on that.)\nThis time, we’ll split up the tasks manually into equal numbers for each process. Parallelizing this can take the following form:\nfrom multiprocessing import Process, Manager, cpu_count\nimport glob\nimport cv2\n\n\ndef load_one_proc(img_data_list, img_file_list, iproc, per_proc):\n    istart, iend = iproc * per_proc, (iproc+1) * per_proc\n    for i in range(istart,iend):    # each process will read a range of files\n        filename = img_file_list[i]\n        print(\"Reading file\",filename)\n        img = cv2.imread(filename)\n        img_data_list.append(img)\n    return\n\nname_prefix = 'image_'\n# we'll use glob to get the list of available files\n# note that glob order isn't...easily discernible, so we'll sort.\nimg_file_list = sorted(glob.glob(name_prefix+'*.png'))\nn_files = len(img_file_list)\nprint(n_files,\"files available.\")\n\n# We'll split up the list manually\nnum_procs = cpu_count()\nprint(\"Parallelizing across\",num_procs,\"processes.\")\n\nper_proc = n_files // num_procs  # Number of files per processor to load\nassert n_files == per_proc * num_procs  # Make sure tasks divide evenly. Obvously one can do something more sophisticated than this!\n\nwith Manager() as manager:\n    img_data_list = manager.list()\n    processes = []\n    for iproc in range(num_procs):\n        p = Process(target=load_one_proc, args=(img_data_list, img_file_list, iproc, per_proc))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n    outside_list = list(img_data_list)   # Copy out of the Manager context (there may be a better way to do this)\n\nprint(len(outside_list),\"images in list.\")\nOkay, great. The thing is, that set of processes operates asynchronously, so there’s no telling what order the final list is going to be in. Maybe you don’t care, but sometimes I care. One way of dealing with this is to add an index item within the list for each item, and then sort on that index.\nBut most of the time what I really want in the end is a numpy array. So let’s just look at how to fill one of those, directly."
  },
  {
    "objectID": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-3-filling-a-numpy-array",
    "href": "posts/2018-12-16-Parallelizing-Python-Simplified.html#example-3-filling-a-numpy-array",
    "title": "Parallelizing Python, Simplified",
    "section": "Example 3: Filling a NumPy array",
    "text": "Example 3: Filling a NumPy array\nData scientist Jonas Teuwen made a great post which got me started on how to do this, but then it seems I uncovered a bug in numpy’s garbage collection for which there’s now a patch. Even without the patch, there are a couple workarounds one can use, and I’ll choose the simpler of the two workarounds.\nLet’s load all those images into a numpy array instead of a list. First the serial version:\nimport numpy as np\nimport glob\nimport cv2\n\nname_prefix = 'image_'\nimg_file_list = sorted(glob.glob(name_prefix+'*.png'))\nn_files = len(img_file_list)\n\nfirst_image = cv2.imread(img_file_list[0])\nprint(n_files,\"files available.  Shape of first image is\",first_image.shape)\n\nprint(\"Assuming all images are that size.\")\nimg_data_arr = np.zeros([n_files]+list(first_image.shape))  # allocate storage\n\nfor i in range(n_files):\n    filename = img_file_list[i]\n    print(\"Reading file\",filename)\n    img_data_arr[i] = cv2.imread(filename)\n\nprint(\"Finished.\")\nFor the parallel version, we’re going to have to use a global variable. Sorry, there’s no away around it, because of Python’s Global Interpreter Lock (GIL).\nWithout further ado, here’s the parallel, numpy version of the ‘loading a list of images’ shown earlier in Example 2. (One other change: rather than specifying ranges of images for each processor – which I did just for the sake of variety – this time we’ll let Pool.map decide how to – evenly – distribute the tasks.)\nimport numpy as np\nimport glob\nimport cv2\nfrom multiprocessing import Pool, sharedctypes, cpu_count\nfrom functools import partial\nimport gc\n\nmp_shared_array = None                               # global variable for array\ndef load_one_proc(img_file_list, i):\n    global mp_shared_array\n\n    # tmp will end up pointing to the memory address of the shared array we want to populate\n    tmp = np.ctypeslib.as_array(mp_shared_array)\n\n    filename = img_file_list[i]\n    print(\"Reading file\",filename)\n    tmp[i] = cv2.imread(filename)   # assign the values into the memory of the shared array\n    return\n\nname_prefix = 'image_'\nimg_file_list = sorted(glob.glob(name_prefix+'*.png'))\nn_files = len(img_file_list)\n\nfirst_image = cv2.imread(img_file_list[0])\nprint(n_files,\"files available.  Shape of first image is\",first_image.shape)\nprint(\"Assuming all images are that size.\")\nimg_data_arr = np.zeros([n_files]+list(first_image.shape))  # allocate storage\ntmp = np.ctypeslib.as_ctypes(img_data_arr)                  # tmp variable avoids numpy garbage-collection bug\n\nprint(\"Allocating shared storage for multiprocessing (this can take a while)\")\nmp_shared_array = sharedctypes.RawArray(tmp._type_, tmp)\n\nnum_procs = cpu_count()\nprint(\"Parallelizing across\",num_procs,\"processes.\")\np = Pool(num_procs)\n\nwrapper = partial(load_one_proc, img_file_list)\nindices = range(n_files)\nresult = p.map(wrapper, indices)                # here's where we farm out the op\nimg_data_arr = np.ctypeslib.as_array(mp_shared_array, shape=img_data_arr.shape)  # this actually happens pretty fast\np.close()\np.join()\n\n# Next couple lines are here just in case you want to move on to other things\n#   and force garbage collection\nmp_shared_array = None\ngc.collect()\n\nprint(\"Finished.\")\nSo that’s the basic implementation. Let me know in the comments if you have suggestions for improvements, or other ideas!"
  },
  {
    "objectID": "posts/2018-12-16-Parallelizing-Python-Simplified.html#p.s.--final-remarks",
    "href": "posts/2018-12-16-Parallelizing-Python-Simplified.html#p.s.--final-remarks",
    "title": "Parallelizing Python, Simplified",
    "section": "P.S.- Final Remarks",
    "text": "P.S.- Final Remarks\nAdded in a couple thoughts, post-facto:\n1. Performance. Notice that what gets passed to p.map() is an iterator, which typically you’ll use as either the indices of the members of an array (as we did just now), or it as a range over the number of processors (kinda like we did in Example 2). In the former case, the system is likely to spawn lots and lots of processes (not all at the same time, but as one ‘job’ finishes the system will spawn a new one, and will keep the “pool” going), which will have a bit of overhead – i.e. latency – each time these start and stop. It’s not much though, and so you probably won’t notice if your goal is merely, “I’m doing this so that I only have to wait 5 minutes instead of an hour to get something done.” If instead you make the indices in the map over the range of processors on your machine and manually break up the array indices into chunks (sort of like we did in Example 2), then you won’t be spawning as many processes and so your latency will be considerably lower. But the gain may be small enough (depending on your system) that you may not notice the difference in performance. Still, if you want to go all-out for performance, then make the pool.map go over the number of procs you want. Otherwise, feel free to trust the system to do its thing for you and just use the array (or list) indices for the iterator.\n2. When Processes Die. Debugging multiprocessing runs is a pain. If one process dies (crashes, seg faults, generates any kind of “Error”), it will hang the entire pool and you won’t know why because the error messages won’t come to stdout or stderr. Look elsewhere for tutorials on tools for debugging multiprocessing runs. Good news is that regular print statements still come to stdout for all processes, so one way of debugging is the age-old method of just loading your code with print statements."
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html",
    "href": "posts/2021-10-15-geometric-algebra.html",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "",
    "text": "“…for geometry, you know, is the gate of science, and the gate is so low and small that one can only enter it as a little child.” – William K. Clifford"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#motivations",
    "href": "posts/2021-10-15-geometric-algebra.html#motivations",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Motivations",
    "text": "Motivations\n\nThe recent paper, “Lightweight Convolutional Neural Networks By Hypercomplex Parameterization”. i.e, the potential for interesting (multichannel audio) signal processing applications of deep learning + hypercomplex numbers.\nThe fact that, even as theoretical physicist (of classical field theory), I never learned enough about particle physics / QFT because I hated Dirac spinors, Pauli matrices, quaternions, and such. Thus this is an attempt to finally overcome my ignorance and aversion to an important and useful area of mathematics/physics!"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#references-im-finding-to-be-helpful",
    "href": "posts/2021-10-15-geometric-algebra.html#references-im-finding-to-be-helpful",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "References I’m finding to be helpful",
    "text": "References I’m finding to be helpful\n\nThe following video by “sudgylacmoe” using the “manim” presentation style developed by 3blue1brown. Watch this!\n\n\n\n\n        \n        \n\n\n\nAnother video: historical perspective: “The Vector Algebra War” (Oh, and there’s a paper from IEEE Access to go with it):\n\n\n\n\n        \n        \n\n\n\n“Clifford Algebra: A visual introduction”, blog post by “slehar”\nThis book!: \nFrom “the man” himself: David Hestenes’s Oersted Lecture\n“Vectors and Beyond: Geometric Algebra and its Philosophical Significance” by Peter Simons, Dialectica, Vol. 63, N° 4 (2010), pp. 381–395.\nThese slides by Joan Lasenby (Signal Processing Group, Trinity College, Cambridge) )\nAlan Macdonald’s textbook(s) and YouTube videos\nJohn Denker’s write-up\nThis very handy list of properties"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#other-resources-i-have-yet-to-read",
    "href": "posts/2021-10-15-geometric-algebra.html#other-resources-i-have-yet-to-read",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Other resources I have yet to read",
    "text": "Other resources I have yet to read\n\nHestenes’ mechanics texbook: New Foundations for Classical Mechanics, 1999.\nPyTorch Quaternion Neural Networks\n“Embed Me If You Can: A Geometric Perceptron”\nThis book chapter: “Outlook for Clifford Algebra Based Feature and Deep Learning AI Architectures”\nMichael Reed’s Grassmann.jl Julia package looks amazing\nSo does all the code & resources at bivector.net! Including the “shop”, haha!\n“Imaginary Numbers are not Real — the Geometric Algebra of Spacetime” by S. F. Gull, A. N. Lasenby and C. J. L. Doran, Found. Phys. 23(9), 1175-1201 (1993).\nSomething Twitter-y…?\n….your suggestions?"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#summary-of-my-thoughts-so-far",
    "href": "posts/2021-10-15-geometric-algebra.html#summary-of-my-thoughts-so-far",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Summary of My Thoughts So Far",
    "text": "Summary of My Thoughts So Far\n\nWhy didn’t anybody teach me this stuff before??? It makes so many things in physics make more sense.\nCross products are like the Easter Bunny. Quaternions are like the Bohr Atom. Should we still teach young people these concepts? If the people in other communities are still using stone tools, should students be deprived of learning to work with bronze or iron?\nCross products are neither necessary for introductory physics instruction nor easier than outer/exterior/wedge products. In 3D, the two are isomorphic, so why not teach the one that’s actually more useful? When we encounter cross products in physics it is always the rotating-area aspect that’s relevant, not the pseudovector aspect (see #2 above).\n\nThere’s nothing inherently “quantum mechanical” about Pauli matrices & Dirac spinors, or quaternions. Rather they are just manifestations of the geometry of 3D space! Futhermore, the fact that Pauli matrices are typically taught in such a unmotivated,/“here memorize these” kind of way is anathema — or perhaps you benefited from the “Interlude: now let’s inject a bunch of Group Theory” approach — whereas with the GA approach they arise naturally.\nIt’s not yet clear to me how bivectors are to be represented in (computational) linear algebra: square, antisymmetric matrices, perhaps?\nThe relation between complex numbers and GA in 2D is cool but there are parts that can be a bit tricky conceptually. I think the reason is that (apart from GA) one typically deals with either complex numbers or 2d vectors (/phasors), and the question of multiplying a 2d vector by a complex number doesn’t really come up – the tricky bit being is that in GA, the pseudoscalar anti-commutes with vectors, whereas one normally thinks of \\(i\\) as being an ordinary scalar that should commute.\nNotation choices: I’m a bit confused when in 2D is that “\\(I\\)” is not the identity operator, rather it is a pseudoscalar/bivector that’s akin to “\\(i\\)” (see point #6 above). But I’m getting used to it. Other choices such as \\(i\\) and \\(j\\) (in EE) are similarly…interesting. Also noticing that the Brits seem to prefer \\(\\sigma_i\\) for basis vectors whereas I was raised on \\(\\hat{e}_i\\). I’m working on a dynamic “choose your own notation” interactive version of the materials I’m writing up, so that notation won’t be a sticking point for anyone, they can just choose what they prefer.\nTerminology: I find it to be a huge stumbling block ( / language barrier) when people say “outer product” in GA, b/c that term already means something different to the rest of the world – namely the “tensor product” aka “Kronecker product” – something that does’t (necessarily) anticommute! Now, if the point is that we’re looking for a “unified language,” let’s use a unified language and not conflate in one term two very different concepts, because this is bound to confuse students (and I confess, physics professors too). Just say “wedge product.” Even “exterior product” is potentially confusing to students, since “exterior” is just a synonym of “outer”.\nRe. (anti-)commutivity: many expressions one sees are written in terms of orthonormal vectors, and thus it’s not always clear when these statements apply to general vectors/bivectors/etc. When in doubt, the thing to do is (presumably) to write out each product in terms of an othonormal basis (\\(\\hat{e}_1, \\hat{e}_2\\), etc.) and multiply and/or reverse things until you can show what you want to show.\n\nIn E&M, GA gives you the relativistic Faraday tensor without even any relavity! Again it’s just a manifestation of geometry.\nThis quote from McRobie and Joan Lasenby – engineers, BTW!: &gt; “The principal argument for the adoption of geometric algebra is that it provides a single, simple mathematical framework which eliminates the plethora of diverse mathematical descriptions and techniques it would otherwise be necessary to learn.” (F. A. McRobie and J. Lasenby, Simo-Vu Quoc Rods using Clifford Algebra. Int. J. Numer. Meth. Eng. 45 (1999), 377-398.) As quoted by Alan Macdonald"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#topicchapter-breakdown",
    "href": "posts/2021-10-15-geometric-algebra.html#topicchapter-breakdown",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Topic/Chapter Breakdown",
    "text": "Topic/Chapter Breakdown\nIs there really that much that would need to change in a (‘second year”/diff-eq-based) mechanics course’? Not really. As follows…\n\nIntro/“Chapter 2”: A little extra content in the usual “vector math” chapter (typically Chapter 2):\n\nAfter teaching the dot product, teach the/Grassman’s wedge product. (Which is isomorphic in 3D to the cross product.) This means discussing bivectors, and how they add.\nAfter the wedge product, teach the/Clifford’s general vector product. Assuming orthonormal basis vectors have already been introduced by then, show that vector products of basis vectors are the same as wedge products. Then show how everything you need to compute/derive/ can be figured out by yourself, in terms of products when writing things out in terms of bases.\nMention the cross product, and what it is, and that other areas of physics & engineering may still use cross-products, and that in terms of components it’s the same as the wedge product.\nMaybe mention in passing / as a footnote: in GA the vector product has an inverse (i.e. you can divide vectors) whereas it’s not defined with Gibbs’ system. Applications for this appear in computer graphics (cf. Joan Lazenby) but not really in physics so, skipping it.\n\nEarly coverage of rotational motion:\n\nAngular momentum as a wedge product, i.e. as an oriented surface showing the plane and direction of rotation, i.e. a bivector.\nRemind students that this is the same as a cross-product, component wise.\nSame with torque: it’s a bivector.\nLook at precessional motion of gyroscope in gravitational field, viewed via bivector vs. cross-product paradigms.\nNo other changes to standard curriculum.\n\nSmall Oscillations / Simple Harmonic Motion:\n\nUsing complex numbers for oscillations and/or Fourier transforms is SUPER standard. Depriving students of this approach (just to shoehorn in GA) would be inadvisable. …Ok, no problem!\nReally it just comes to your interpretation of “i”:: i.e., whether it’s interpreted as an “imaginary number” or as the pseudoscalar. The mathematical calculations are the same either way.\nViewing in terms of pseudoscalar would tie things in to the fact that we’re talking about rotations of phase. So this’ll be mentioned but not stressed. It’s not a big deal either way.\nSo, this chapter could be “agnostic” about the nature of i.\nSee link below to the paper about GA for teaching about AC circuits.\nNow if we were to also mention (as many instructors already do) that is a generator of rotations, then this sets us up for all kinds of subsequent interesting discussions, including when we get to Rigid Body Rotation….\n\nRigid Body Rotation:\n\nSadly we usually have to skip this topic/chapter anyway at Belmont University due to time & pace of course – we have to “go slower” at Belmont compared to some other schools, given our lax math prereqs. It’s a major omission that I’d like to remedy someday.\nActually, GA could help by requiring less new formalism in this chapter: Students wouldn’t need to learn Euler angles (which lead to gimbal lock) or quaternions (which have negative magnitude and are janky IMHO). Everything could just be done in terms of rotation matrices (which students would be learning either way) and/or rotors.\nThese sorts of rotations would set them up for Quantum Mechanics and provide them exactly what they need, even providing a route “around” Pauli spinors, etc. since they’d now know GA anyway.\n\n…And that’s all we’d change in Mechanics! (When we cover Lagrangian mechanics, we never get to Noether’s Theorem. This is still only an undergrad class. That could be a grad school thing.)\nLooking ahead to later classes:\n\nE&M: Maxwell’s equations in GA are the best “parlor trick” I’ve seen. On a practical level, what of gradient, divergence, curl? Hestenes 2003:\n\n\n“GA reduces ‘grad, div, curl and all that’ to a single vector derivative that, among other things, combines the standard set of four Maxwell equations into a single equation and provides new methods to solve it.’’\n\n(For more, one could see Jancewicz 1989, “Multivectors and Clifford Algebras in Electrodynamics.)\n\nQuantum Mechanics: I mean…what’s not to like?? We’re no longer forced to puzzle over why \\(i\\) is an imaginary number that suddenly appears for no good reason (other than to make the algebra work out right, which GA does for you as well yet with spatially-meaningful objects instead)? (Simons 2010: “…the puzzle about the otherwise unmotivated introduction of complex numbers disappears.”) We get rotors that naturally lead to spinors? We get a geometric interpretation of spin? We still get all the algebra and symmetries that we need?? Uh… win-win, right?"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#who-else-is-teaching-this",
    "href": "posts/2021-10-15-geometric-algebra.html#who-else-is-teaching-this",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Who Else Is Teaching This?",
    "text": "Who Else Is Teaching This?\nHave others tried teaching GA approaches? And if so, have they done it with undergrads or only grad students? And have these been successful or not – i.e. have they been tried and found lacking, or just not tried much? And how would success even be measured?\nNames/Works:\n\nDavid Hestenes’s Mechanics textbook, 1999.Hestenes has done a lot, and is essentially the “father of the (modern) field” / primarily responsible for disseminating GA. My question is: Who besides Hestenes?\nThere’s a LinkedIn group on Pre-University Geometric Algebra, i.e. for high schools! Run by educator James A. Smith, whose works are published on Vixra.org\nQuirino Sugon, Jr. writes in thisPhysicsStackExchangethread, “For several years I have been teaching Clifford (geometric) algebra as part of the Vector Analysis Course for undergraduate physics majors in Ateneo de Manila University…”\nNot mechanics, but: “GeometricAlgebraforTeachingACCircuitTheory”, by Francisco G. Montoya et al, Int. J. Circ. Theor. Appl., Sep. 2021. Excerpt:\n\n“It requires the students to learn the basics of geometric algebra, but this effort is clearly compensated with the advantages derived from the possibility of using easy identifiable geometrical concepts. In fact, it is shown that the application of GA overcomes some limitations derived from the use of complex algebra and its phasor-based notation.”\nWhy aren’t more instructors doing this? I believe it’s because they just haven’t been exposed to it: “Clifford Algebra” mostly shows up in high-level theoretical physics discussions and/or pure-math stuff (ring theory + whatever my math colleague Brad Schleben does ;-) ). It’s been growing in popularity since the 1990s, and Doran & the Lazenbys have popularized it a bit in physics, engineering & computer graphics/vision, but kinda.. beyond that it’s still largely viewed as an obscure pure-theory thing rather than “the way things should be.” We believe this will and should change, and we’re willing to be a part of it. Others will say, “I guess this is interesting but why should we change? Cross products, quaternions, spinors, four-vectors, etc. are janky as hell but we learned them and we turned out fine!”"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#education-research-studies",
    "href": "posts/2021-10-15-geometric-algebra.html#education-research-studies",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Education Research Studies?",
    "text": "Education Research Studies?\nDavid Hestenes mentioned in his OerstedMedallecture (ca. 2002) that the physics-ed community needs to do the education research to show how/whether teaching GA in physics courses is a good idea….and AFAIK his call has not been taken up, at all. Hestenes received AAPT’s2014 ”ExcellenceinPhysicsEducation”award for his work with teaching GA, but still I can’t find any learning outcome results. My literature searches turned up zero studies about this. (Closest thing I found merely mentions Hestens but no explicit GA. Found one listservdiscussion but no results/outcomes.)\nUpdate: One small study: James A. Smith shared with me one study by Ramón López Escudero and Philippe Eenens, which can be translated to English via Google Translate: “Enseñar el Álgebra Geométrica en el Primer Semestre de Ingenierías” (“Teach Geometric Algebra in the First Semester of Engineering”). Assessment consisted of two application problems and a test at the end of the course, as well as interviewing students. “The test result also turned out to be good.”\nThis merits revisiting an earlier question:"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#how-would-success-be-measured",
    "href": "posts/2021-10-15-geometric-algebra.html#how-would-success-be-measured",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "How Would “Success” Be Measured?",
    "text": "How Would “Success” Be Measured?\nIdeas:\n\nLack of complaints/negative results. Seriously. If we can introduce this material and nobody says boo, that can count as success. If people don’t show negative results (e.g. students later report “I got to grad school and wished we’d spent a whole 50 on cross-products instead of 10 minutes”), if whatever “scores” we have don’t go down.. That’s a plus.\nActual positive anecdotal reports (which “count” in education circles), e.g. someone says “I loved the GA stuff” – a question about this could be added to the end of course eval reports that students answer: “How did you feel about the GA content?” We could perhaps somehow ask students later in Quantum if they felt the GA background helped them (it should!)\nImproved scores on our standard metrics / exams? Hard to insist on a causal relationship, but see #1, re not-going-down as justification.\n\n…and if we’ve only got an enrollment of at most 10 undergrads in the class, then how would we ever turn this into a legit study, e.g. something one might present at a Lily Conference or somesuch? I know nothing about proper education research (but Danielle and/or the Teaching Center probably do! ;-) ) .\nSugon: “I am the only one teaching geometric algebra in my university, so I could not follow-up my students.”"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#main-issues-with-doing-ed-research-for-me",
    "href": "posts/2021-10-15-geometric-algebra.html#main-issues-with-doing-ed-research-for-me",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "Main Issues with Doing Ed Research for Me:",
    "text": "Main Issues with Doing Ed Research for Me:\n\nTracking Downstream Effects/Cohorts: The benefits of the GA approach will be mostly “downstream,” i.e., when students take more advanced courses – it’s hard to imagine students seeing “the unity of physics” from within only their very first semi-advanced course, on one topic! There will be a few benefits within mechanics itself, but I speculate that they may not be enough to appear significant on their own. Dr. Danielle Garrett (sci-ed expert) suggested tracking cohorts of students (and also mentioned that tracking student data would need IRB approval). That begs the question of whether other physics faculty at my university will be “on board” with using GA in, say, quantum & E&M, and/or whether we’d track the 1 or 2 of our students each year who go on to graduate school. (Note also that GA is seeing huge inroads into computer graphics, so, for the few students who go on to advanced CG work,…?)\nMy Time & Other Commitments: I don’t need or really want a publication in some physics-ed journal; I’ve got enough other projects to keep me busy already. (I’ve got a book to work on that has nothing to do with GA!). But…it seems the community could really benefit from a study of this – and again, Hestenes’ call has gone largely unanswered.\n\nRelated: Becoming an Admin for a Large Project: Maybe a networking approach would be needed, in which multiple faculty from multiple institutions commit to sharing data in some kind of standardized format in order to put together a study. That sounds like something a “bigger team” at a major university would/should/could do. But perhaps a consortium of “small college people” could still achieve something. Do I want to drop everything else in order to become the PI coordinating such an effort (even seek NSF funding, offer summer assistantships, etc)? …TBH, not really. But perhaps I could assist in motivating someone else to do so, and then I could “assist”. ;-)"
  },
  {
    "objectID": "posts/2021-10-15-geometric-algebra.html#to-do-extras",
    "href": "posts/2021-10-15-geometric-algebra.html#to-do-extras",
    "title": "Learning Some Geometric/Clifford Algebra",
    "section": "TO-DO / Extras:",
    "text": "TO-DO / Extras:\n\nListoflotsofpeople’spersonalwebsites by instructors, to check to see if they talk about pedagogy"
  },
  {
    "objectID": "posts/2017-11-22-convexity-loss-classification.html#plot-the-full-error-surface",
    "href": "posts/2017-11-22-convexity-loss-classification.html#plot-the-full-error-surface",
    "title": "Notes on Convexity of Loss Functions for Classification",
    "section": "Plot the full error surface",
    "text": "Plot the full error surface\n\nnum_a = 100\n# try experimenting: play around with the max & min of the a & b values to see the surface\na_arr = np.linspace(-a_guess/2,a_guess*2,num_a)\nb_arr = np.linspace(b_guess*2,-b_guess/2,num_a)\nA, B = np.meshgrid(a_arr, b_arr)\n\ndef plot_loss_surf(A, B, x_arr, loss='SE'):\n    Z = np.zeros([len(A), len(B)])\n    if ('SE' == loss):\n        for i in range(len(A)):\n            for j in range(len(B)):\n                h_arr = h(x_arr, a_arr[i], b_arr[j])\n                Z[j, i] = calc_se_loss(y_arr, h_arr)\n    else:\n        for i in range(len(A)):\n            for j in range(len(B)):\n                h_arr = h(x_arr, a_arr[i], b_arr[j])\n                Z[j, i] = calc_ce_loss(y_arr, h_arr)\n\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n    surf = ax.plot_surface(A, B, np.log(Z), cmap=cm.coolwarm,\n                           linewidth=0, antialiased=False)\n    ax.set_xlabel('a')\n    ax.set_ylabel('b')\n    ax.set_zlabel('log('+loss+')')\n    #ax.set_zlim(2, 2.5)\n    ax.set_title(loss+' loss')\n    #fig.colorbar(surf, shrink=0.5, aspect=5)       # Add a color bar which maps values to colors.\n    plt.show()\n    \nplot_loss_surf(A,B,x_arr,loss='SE')\nplot_loss_surf(A,B,x_arr,loss='CE')"
  },
  {
    "objectID": "posts/2017-11-22-convexity-loss-classification.html#the-verdict",
    "href": "posts/2017-11-22-convexity-loss-classification.html#the-verdict",
    "title": "Notes on Convexity of Loss Functions for Classification",
    "section": "The Verdict",
    "text": "The Verdict\nSo, although the individual terms \\((y_i - h_i)^2\\) and/or \\([y_i\\log(h_i)+(1-y_i)\\log(1-h_i)]\\) are individually convex, the sum for either type of loss terms is actually non-convex for this problem. Although neither give rise to unwanted local minima for this problem.\nThe SE loss, while at least not having any non-global minima, still has multiple significant flat regions that would prove tedious for gradient descent optimiazation, whereas in contrast, the CE loss is smoother and is strictly monotonic on either side of the global minimum. The the CE loss (and/or KL divergence) would be preferable for this problem – you could do it with SE loss assuming you had momentum or some fancy optimization algorithm, but it would take longer and why bother?"
  },
  {
    "objectID": "posts/2017-11-22-convexity-loss-classification.html#aside-solve-the-logistic-regression-problem-using-scikit-learn",
    "href": "posts/2017-11-22-convexity-loss-classification.html#aside-solve-the-logistic-regression-problem-using-scikit-learn",
    "title": "Notes on Convexity of Loss Functions for Classification",
    "section": "Aside: Solve the logistic regression problem using scikit-learn",
    "text": "Aside: Solve the logistic regression problem using scikit-learn\n\nfrom sklearn import linear_model\nclf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')\nclf.fit(x_arr[:, np.newaxis], y_arr)\na_fit, b_fit = clf.coef_[0][0], clf.intercept_[0]\nprint(\"Fit paramters: a =\",a_fit,\", b =\",b_fit)\nprint(\"Predicted threshold: x = \",-b_fit/a_fit)\n\nFit paramters: a = 32.449347939033615 , b = -20.23838504197379\nPredicted threshold: x =  0.6236915786412106"
  },
  {
    "objectID": "posts/2017-11-22-convexity-loss-classification.html#afterward-but-you-still-havent-found-the-global-minimum",
    "href": "posts/2017-11-22-convexity-loss-classification.html#afterward-but-you-still-havent-found-the-global-minimum",
    "title": "Notes on Convexity of Loss Functions for Classification",
    "section": "Afterward: “But you still haven’t found the global minimum!”",
    "text": "Afterward: “But you still haven’t found the global minimum!”\nIn the surface plots above, we see the minimum of the surface going lower and lower – even lower than the supposedly ‘optimum’ parameters we just found via scikit-learn. The reason is that there is no optimal paramter combination: The steepness parameter \\(a\\) of the sigmoid function \\(h(x)\\) is only bounded from below by the data in this problem. Thus there is no upper bound. The data will constrain the center of the sigmoid \\(x_0 = -b/a\\) to some extent (i.e. it needs to lie between two values of \\(x_i\\)), but other than that…\nSo how then does the loss function seem to get lower and lower? The steeper the sigmoid function, the more closely its values will approach 0 and 1 on either side. Thus for this problem, the optimal solution is \\(a \\rightarrow \\infty\\), with \\(b = -({\\rm threshold})/a\\)."
  },
  {
    "objectID": "posts/2017-05-04-Learning-Room-Shapes.html",
    "href": "posts/2017-05-04-Learning-Room-Shapes.html",
    "title": "Learning Room Shapes (with Neural Networks)",
    "section": "",
    "text": "‘Deep’ neural network learns (boxy) room shape given mode frequencies, or vice versa.\nThis is just a quick note, sharing a new “toy” result, that relates to function spaces, acoustics, and machine learning. I never bothered to publish it, because I was just “playing around”. The real trick would be to extend this to “non-boxy” room shapes, which…turns out to be hard! haha\nMain Python code for this is here."
  },
  {
    "objectID": "posts/2017-05-04-Learning-Room-Shapes.html#motivation",
    "href": "posts/2017-05-04-Learning-Room-Shapes.html#motivation",
    "title": "Learning Room Shapes (with Neural Networks)",
    "section": "Motivation",
    "text": "Motivation\nDuring Karlheinz Brandenburg’s visit, he remarked that learning room shapes from the sound of the room is still an open question. So yesterday, “for fun” I decided to try the easiest possible thing I could think of, the most “ideal” if you will: wrote a NN system that learns to use a set of room mode frequencies to predict (boxy) room dimensions, or vice versa, i.e. it learns the “Rayleigh Equation” for 3D standing waves…\n\n…both ‘forwards’ or ‘backwards’: [L,W,H] &lt;–&gt; [list of mode frequencies] (given speed of sound v_s)."
  },
  {
    "objectID": "posts/2017-05-04-Learning-Room-Shapes.html#results",
    "href": "posts/2017-05-04-Learning-Room-Shapes.html#results",
    "title": "Learning Room Shapes (with Neural Networks)",
    "section": "Results",
    "text": "Results\nSeems to learn within +/- 5%. I don’t have a graphical representation of going from mode frequencies to room dimensions (any ideas?). For mapping dimensions to frequencies, here’s a sample room shown in the picture (red=target, green=predicted), used a training set of 200,000 random rooms. It even learns to sort the freqs in ascending order.\n\n\n\nSample plot of mode freqs\n\n\nInterestingly, trying to ‘help’ the network by using squared frequencies, inverse-(squared)-dimensions, etc. actually gave worse performance than letting the network learn the nonlinearities itself. (???)\nOf what possible practical utility is this? Not really sure. Although, it does a fairly decent job learning the room shape even using a smaller random subsample of the available mode frequencies. :-) If anything, it highlights a weakness of data-driven modeling: there’s no way you’ll measure 200,000 rooms in real life!"
  },
  {
    "objectID": "posts/2017-05-04-Learning-Room-Shapes.html#future-work",
    "href": "posts/2017-05-04-Learning-Room-Shapes.html#future-work",
    "title": "Learning Room Shapes (with Neural Networks)",
    "section": "Future Work",
    "text": "Future Work\nTaking this further, would require analyzing the weights of the network in detail, to see how it represents the function approximation. Which would be worth doing!\nMoving on to non-boxy rooms would be relevant and challenging. Some means of encoding the room geometry would be needed, as well some way of simulating (or measuring?) resonant frequencies or impulse responses."
  },
  {
    "objectID": "posts/2017-02-06-Machine-Learning-Reference-List.html",
    "href": "posts/2017-02-06-Machine-Learning-Reference-List.html",
    "title": "Machine Learning Reference List",
    "section": "",
    "text": "This has been my personal reading list, first compiled ca. February 2016 & updated very infrequently (e.g. Oct 2016, Feb 2017, Sept 2017). The field moves so quickly, much of this may have been superseded by now. If you find it useful as well, that’s great.\nI’m mostly interested in audio processing, so…"
  },
  {
    "objectID": "posts/2017-02-06-Machine-Learning-Reference-List.html#in-physics",
    "href": "posts/2017-02-06-Machine-Learning-Reference-List.html#in-physics",
    "title": "Machine Learning Reference List",
    "section": "In Physics:",
    "text": "In Physics:\n\n“Fast cosmological parameter estimation using neural networks”, T. Auld, M. Bridges, M.P. Hobson and S.F. Gull, MNRAS. 000, 1–6 (2004), https://arxiv.org/pdf/astro-ph/0608174.pdf\n“Parameterized Neural Networks for High-Energy Physics”, Baldi, P., Cranmer, K., Faucett, T., Sadowski, P., Whiteson, D. The European Physical Journal C. 76, 235, 1-7, May 2016, 2016, https://arxiv.org/pdf/1601.07913.pdf\n\n\nHardware:\n\nAmazon Web Services (AWS):\n\nStanford disk image (AMI) with everything preinstalled: https://cs231n.github.io/aws-tutorial/\n…or just grab some other “Community AMI” with CUDA etc installed\nAnother AWS setup: https://github.com/andreasjansson/simple-aws-gpu-setup\nTensorFlow on AWS (tutorial video): https://www.youtube.com/watch?v=1QhCsO4jmoM\nMy AWS Aetup: ami-a96285c4 ( AMI: old cuda but works: ami-63bf8209 do not like: 11777_MML (ami-37a58f5d) or DeepestLearning ) When u create your own AMI it brings your server down. :-(\n\n\npip install --upgrade pip\npip install -U numpy\nsudo pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\nsudo pip install awscli h5py\ngit clone &lt;https://github.com/fchollet/keras.git&gt;\ncd keras\nsudo python setup.py install\nFACTOR OF 10 SPEEDUP using the g2.xlarge GPUs vs my Macbook Pro (no GPU)!!\n\n\nCheckpointing:\nrun ‘watch’ command to execute AWS transfer to S3 ever  seconds …and spot instance went down without any checkpoint to allow uploading from EC2 to S3 it’s convoluted: install aws cli create an “IAM” user. Grant the user permissions to upload to s3 via https://forums.aws.amazon.com/thread.jspa?messageID=600007 aws configure …good to go.\nwatch -n 550 aws s3 cp /tmp/weights.hdf5 s3://hawleymainbucket\n\nTheano GPU setup guide: https://github.com/andreasjansson/simple-aws-gpu-setup\nOpenMP: Don’t forget to enable multiple OpenMP threads! Can get you at least a factor of 2 speedup!\n\nIn most ‘modern’ Python installations (e.g. anaconda) OpenMP is automatic\n\nMy proposed PC build: https://pcpartpicker.com/user/drscotthawley/saved/bFZ8dC\n\n\n\n\nSelf-Organizing Maps:\n\n“Unsupervised Classification of Audio Signals by Self-Organizing Maps and Bayesian Labeling”: http://link.springer.com/chapter/10.1007%2F978-3-642-28942-2_6\n“Visualization of Tonal Content in the Symbolic and Audio Domains“ http://www.ccarh.org/publications/cm/15/cm15-10-toiviainen.pdf\n\n\n\n“Weird Stuff”:\n\nStochastic path Deep NN for image rec: http://arxiv.org/pdf/1603.09382v1.pdf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scott H. Hawley: Physics prof / Tinkerer.My blog about code & ideas.",
    "section": "",
    "text": "Understanding Transformers, Part 2: Wee Music Box\n\n\n\n\n\n\n\nnlp\n\n\narchitectures\n\n\ntransformers\n\n\nmusic\n\n\nmidi\n\n\n\n\nToy Transformer Tunes\n\n\n\n\n\n\nDec 15, 2023\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nTo Understand Transformers, Focus on Attention\n\n\n\n\n\n\n\nnlp\n\n\narchitectures\n\n\n\n\nIf you can get this one thing, the rest will make sense.\n\n\n\n\n\n\nAug 21, 2023\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nResidual Vector Quantization\n\n\n\n\n\n\n\ncompression\n\n\nssl\n\n\n\n\nAn exploration of the heart of neural audio codecs\n\n\n\n\n\n\nJun 12, 2023\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nBYOL - Contrastive Representation Learning without Contrastive Losses\n\n\n\n\n\n\n\nssl\n\n\n\n\nNow that we understand contrastive losses and what they’re good for – let’s get rid of them!\n\n\n\n\n\n\nNov 17, 2022\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nShell Volumes and Distances in High-Dimensional Spaces\n\n\n\n\n\n\n\nssl\n\n\nmath\n\n\n\n\nGiven enough dimensions, does it really matter which distance norm you use?\n\n\n\n\n\n\nNov 15, 2022\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nLikelihood of Vector Orthogonality in High-Dimensional Spaces\n\n\n\n\n\n\n\nssl\n\n\nmath\n\n\n\n\nDepends on whether you’re using unit vectors or not.\n\n\n\n\n\n\nJan 24, 2022\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nLearning Some Geometric/Clifford Algebra\n\n\n\n\n\n\n\nmath\n\n\n\n\nClifford algebra isn’t so scary\n\n\n\n\n\n\nOct 15, 2021\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nHandy Bash Functions for ML & Data Science\n\n\n\n\n\n\n\ncomputer_usage\n\n\n\n\nOnce I figure out a complicated command, I make a bash function for it\n\n\n\n\n\n\nSep 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTypical (Neural-Network-Based) Classification vs. Zero-Shot, Part 3 - Live Demo of CL Toy\n\n\n\n\n\n\n\nssl\n\n\nclassification\n\n\n\n\nTriplet Loss kicks butt\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTypical (NN-Based) Classification vs. Zero-Shot, Part 2 - An Example Using FastAI\n\n\n\n\n\n\n\nclassification\n\n\nssl\n\n\n\n\nWith my new data-visualization Callback!\n\n\n\n\n\n\nJun 17, 2021\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nTypical (Neural-Network-Based) Classification vs. Zero-Shot, Part 1 - The Joy of 3D\n\n\n\n\n\n\n\nclassification\n\n\nssl\n\n\n\n\nVisualizations in 3 (and 4) dimensions\n\n\n\n\n\n\nMay 4, 2021\n\n\nScott H. Hawley\n\n\n\n\n\n\n  \n\n\n\n\nScottergories: Making GPT-3 Generate 3 of a Kind\n\n\n\n\n\n\n\nclassification\n\n\nlanguage_models\n\n\n\n\nIn which we test GPT-3’s ability to generate members of a category\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\nNaughty by Numbers: Classifications at Christmas\n\n\n\n\n\n\n\nclassification\n\n\nfoundations\n\n\n\n\nA snarky interactive lesson on classification and surveillance\n\n\n\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCitations in Fastpages via BibTeX and jekyll-scholar\n\n\n\n\n\nSupporting scholarly blogging by drawing references from your database.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMac =&gt; Linux Switch Notes\n\n\n\n\n\n\n\ncomputer_usage\n\n\n\n\nMac and I aren’t breaking up, we’re just seeing other people.\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPrincipal Component Analysis (PCA) from Scratch\n\n\n\n\n\n\n\nmath\n\n\nfoundations\n\n\n\n\nThere are many tutorials on PCA, but this one has interactive 3D graphs!\n\n\n\n\n\n\nDec 21, 2019\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started in ML+Audio\n\n\n\n\n\n\n\nfoundations\n\n\n\n\nFor audio students interested in machine learning\n\n\n\n\n\n\nDec 6, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMy First NN Part 3. Multi-Layer Networks and Backpropagation\n\n\n\n\n\n\n\nfoundations\n\n\n\n\nMulti-Layer Networks and Backpropagation\n\n\n\n\n\n\nFeb 8, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMy First Neural Network, Part 2. Bias and CE Loss\n\n\n\n\n\n\n\nfoundations\n\n\n\n\nBias and cross-entropy loss\n\n\n\n\n\n\nFeb 4, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMy First Neural Network, Part 1\n\n\n\n\n\n\n\nfoundations\n\n\n\n\nFirst in a series on understanding neural network models.\n\n\n\n\n\n\nJan 30, 2019\n\n\n\n\n\n\n  \n\n\n\n\nParallelizing Python, Simplified\n\n\n\n\n\n\n\ncomputer_usage\n\n\n\n\nBasic use of multiprocessing.\n\n\n\n\n\n\nDec 16, 2018\n\n\n\n\n\n\n  \n\n\n\n\nResolving Mac OS X Aliases in Python\n\n\n(or “How I Spent Monday Night”)\n\n\n\n\ncomputer_usage\n\n\n\n\nMac OSX aliases are not symbolic links. Trying to read one will probably crash your code.\n\n\n\n\n\n\nFeb 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Convexity of Loss Functions for Classification\n\n\n\n\n\n\n\nmath\n\n\n\n\nFollowing up on a question that arose in Week 3 of Andrew Ng’s Machine Learning course.\n\n\n\n\n\n\nNov 22, 2017\n\n\n\n\n\n\n  \n\n\n\n\nCrash Course On GANs\n\n\n\n\n\nThis post is not necessarily a crash course on GANs. It is at least a record of me giving myself a crash course on GANs. Adding to this as I go.\n\n\n\n\n\n\nMay 5, 2017\n\n\n\n\n\n\n  \n\n\n\n\nLearning Room Shapes (with Neural Networks)\n\n\n\n\n\n\n\nphysics\n\n\nacoustics\n\n\n\n\nDeep neural network learns (boxy) room shape given mode frequencies, or vice versa.\n\n\n\n\n\n\nMay 4, 2017\n\n\n\n\n\n\n  \n\n\n\n\nFollowing Gravity - ML Foundations Part I\n\n\n\n\n\n\n\nfoundations\n\n\n\n\nFirst in a series on Machine Learning Foundations, which applies to much of science and statistics as well. .\n\n\n\n\n\n\nFeb 23, 2017\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning Reference List\n\n\n\n\n\n\n\nfoundations\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2017\n\n\n\n\n\n\n  \n\n\n\n\nHow to Port-Forward Jupyter Notebooks\n\n\n\n\n\n\n\ncomputer_usage\n\n\n\n\n…to enable efficient remote-working.\n\n\n\n\n\n\nFeb 3, 2017\n\n\n\n\n\n\nNo matching items"
  }
]