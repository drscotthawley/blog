<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-02-08">
<meta name="description" content="Multi-Layer Networks and Backpropagation">

<title>blog - My First NN Part 3. Multi-Layer Networks and Backpropagation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - My First NN Part 3. Multi-Layer Networks and Backpropagation">
<meta name="twitter:description" content="Multi-Layer Networks and Backpropagation">
<meta name="twitter:image" content="https://i.imgur.com/WjaQDnW.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">My First NN Part 3. Multi-Layer Networks and Backpropagation</h1>
                  <div>
        <div class="description">
          Multi-Layer Networks and Backpropagation
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">foundations</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 8, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-backpropagation" id="toc-what-is-backpropagation" class="nav-link active" data-scroll-target="#what-is-backpropagation">What is Backpropagation?</a></li>
  <li><a href="#a-multi-layer-network" id="toc-a-multi-layer-network" class="nav-link" data-scroll-target="#a-multi-layer-network">A Multi-Layer Network</a></li>
  <li><a href="#semantics-what-is-a-layer" id="toc-semantics-what-is-a-layer" class="nav-link" data-scroll-target="#semantics-what-is-a-layer">Semantics: What is a Layer?</a></li>
  <li><a href="#figuring-out-dimensions-of-the-weights" id="toc-figuring-out-dimensions-of-the-weights" class="nav-link" data-scroll-target="#figuring-out-dimensions-of-the-weights">Figuring out dimensions of the weights</a></li>
  <li><a href="#backpropagating-theory" id="toc-backpropagating-theory" class="nav-link" data-scroll-target="#backpropagating-theory">Backpropagating: Theory</a></li>
  <li><a href="#writing-the-backprop-code" id="toc-writing-the-backprop-code" class="nav-link" data-scroll-target="#writing-the-backprop-code">Writing the Backprop Code</a></li>
  <li><a href="#solving-xor" id="toc-solving-xor" class="nav-link" data-scroll-target="#solving-xor">Solving XOR</a></li>
  <li><a href="#same-thing-using-neural-network-libraries-keras-pytorch." id="toc-same-thing-using-neural-network-libraries-keras-pytorch." class="nav-link" data-scroll-target="#same-thing-using-neural-network-libraries-keras-pytorch.">Same thing using neural network libraries Keras &amp; PyTorch.</a>
  <ul class="collapse">
  <li><a href="#keras-version" id="toc-keras-version" class="nav-link" data-scroll-target="#keras-version">Keras version</a></li>
  <li><a href="#pytorch-version" id="toc-pytorch-version" class="nav-link" data-scroll-target="#pytorch-version">PyTorch version</a></li>
  </ul></li>
  <li><a href="#exercise-exploring-hidden-layers." id="toc-exercise-exploring-hidden-layers." class="nav-link" data-scroll-target="#exercise-exploring-hidden-layers.">Exercise: Exploring Hidden Layers.</a>
  <ul class="collapse">
  <li><a href="#more-with-the-7-segment-display" id="toc-more-with-the-7-segment-display" class="nav-link" data-scroll-target="#more-with-the-7-segment-display">More with the 7-segment display</a>
  <ul class="collapse">
  <li><a href="#a.-explore-hidden-layer-sizes-activations" id="toc-a.-explore-hidden-layer-sizes-activations" class="nav-link" data-scroll-target="#a.-explore-hidden-layer-sizes-activations">A. Explore hidden layer sizes &amp; activations</a></li>
  <li><a href="#b.-explore-multiple-hidden-layers" id="toc-b.-explore-multiple-hidden-layers" class="nav-link" data-scroll-target="#b.-explore-multiple-hidden-layers">B. Explore multiple hidden layers</a></li>
  <li><a href="#assignment" id="toc-assignment" class="nav-link" data-scroll-target="#assignment">Assignment:</a></li>
  </ul></li>
  <li><a href="#preview-of-next-lesson-mnist" id="toc-preview-of-next-lesson-mnist" class="nav-link" data-scroll-target="#preview-of-next-lesson-mnist">Preview of next lesson: MNIST</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Links to lessons: <a href="https://drscotthawley.github.io/blog/2017/02/23/Following-Gravity-Colab.html">Part 0</a>, <a href="https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html">Part 1</a>, <a href="https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html">Part 2</a>, <a href="https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html">Part 3</a></p>
<section id="what-is-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="what-is-backpropagation">What is Backpropagation?</h2>
<p>First watch this <a href="https://www.youtube.com/watch?v=q555kfIFUCM">5-minute video on backprop by Siraj Raval</a>.</p>
<p><strong>EDIT 2/20/2020:</strong> ^Ravel was later revealed to be plagiarizing content. I will look for an alternative link. The format in <a href="https://hedges.belmont.edu/~shawley/PHY2895/">my (“flipped”) ML course last year</a> involved reading things, watching <em>brief</em> videos, and modifying code. Siraj’s video fit the bill last year as being <em>brief</em> &amp; <em>good</em>.</p>
</section>
<section id="a-multi-layer-network" class="level2">
<h2 class="anchored" data-anchor-id="a-multi-layer-network">A Multi-Layer Network</h2>
<p>Between the input <span class="math inline">\(X\)</span> and output <span class="math inline">\(\tilde{Y}\)</span> of the network we encountered earlier, we now interpose a “hidden layer,” connected by two sets of weights <span class="math inline">\(w^{(0)}\)</span> and <span class="math inline">\(w^{(1)}\)</span> as shown in the figure below. This image is a bit more complicated than diagrams one might typically encounter; I wanted to be able to show and label all the different “parts.” We will explain what the various symbols mean as we continue after the figure. <img src="https://i.imgur.com/WjaQDnW.png" class="img-fluid" alt="image of multi-layer network"></p>
<p>As before, the combined operation of weighted sum and nonlinear activation is referred to as a(n artificial) neuron; in the diagram above there are 4 “hidden neurons” — or equivalently “4 neurons in the hidden layer” — as well as one neuron for output. (The activations <span class="math inline">\(f^{(0)}\)</span> and <span class="math inline">\(f^{(1)}\)</span> may be the same, or they may be different.)</p>
</section>
<section id="semantics-what-is-a-layer" class="level2">
<h2 class="anchored" data-anchor-id="semantics-what-is-a-layer">Semantics: What is a Layer?</h2>
<p>The term “layer” in regards to neural network is not always used consistently. You may find it used in different senses by different authors.</p>
<ol type="1">
<li>Some users of the term will only use it with repect to <em>weight matrices</em>, (since these are the parts of the network which are adjusted in learning).</li>
<li>Others will refer to the input and (predicted) output as layers, and may or may not include the weights as layers..</li>
<li>Others will only count additional “hidden layers” between the inputs and outputs, and these “layers” are <em>connected by</em> multiple weight matrices.<br>
</li>
<li>Some will speak of “activation layers.” In software libraries like Keras, many different types of operations and storage are referred to as layers.</li>
</ol>
<p>For the work we’ve done so far, we’ve had inputs and outputs connected by one weight matrix, subject to a nonlinear activation function. Is this a two-layer network made of input and output “layers,”” or is it a single-layer network, because there is only one weight matrix? What about the activation layer? This is to some degree a semantic issue which one does not need to get hung up on.</p>
<p><strong>For our purposes</strong> it is convenient to refer to the inputs <span class="math inline">\(X\)</span>, the ‘activated’ hidden states <span class="math inline">\(H\)</span>, and the output <span class="math inline">\(\tilde{Y}\)</span> as “layers”, numbering them 0, 1, and 2 respectively, and using the script notation <span class="math inline">\(\mathcal{L}^l\)</span> to denote each layer, where the layer index <span class="math inline">\(l=0..2\)</span>, so that</p>
<p><span class="math display">\[
\mathcal{L}^{(0)} = X, \ \ \ \ \mathcal{L}^{(1)} = H,\ \ \ \ \mathcal{L}^{(2)} = \tilde{Y}
\]</span></p>
<p>This makes it easy to write the value of higher-numbered layers in terms of lower-numbed layers, i.e., <span class="math display">\[
\mathcal{L}^{(l+1)} = f^{(l)}\left( {\mathcal{L}^{(l)}}^T \cdot w^{(l)} \right),
\]</span> where the dot <span class="math inline">\(\cdot\)</span> denotes a matrix product. This is often referred to as a <strong>“feed foward”</strong> operation because values are fed from left to right in the above diagram, “forward” through the network. (Backpropagation will involve feeding values from right to left.)</p>
<p><strong><em>Response to student question(s): </em></strong> <em>“What <em>are</em> neurons? Like, what does this mean in terms of matrices?”</em></p>
<p>This will serve as a review of the <a href="https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html">Part 1</a> lesson. Using the above notation, the operations from the input to the hidden layer look like this in matrix form:</p>
<p><img src="https://i.imgur.com/hgtEpVH.png" class="img-fluid" alt="matrix form of first calcs"> …where the lines in dark red and cyan are simply to indicate sample calculations which are part of the matrix multiplication.</p>
</section>
<section id="figuring-out-dimensions-of-the-weights" class="level2">
<h2 class="anchored" data-anchor-id="figuring-out-dimensions-of-the-weights">Figuring out dimensions of the weights</h2>
<p>When we learned about matrix multipliation, we remarked that most of the time in machine learning, “the trick is to get the inner dimensions to match.”</p>
<p>Let’s say there are <span class="math inline">\(N\)</span> different input data “points” consisting of <span class="math inline">\(M\)</span> values each. So the input <span class="math inline">\(X\)</span> is an <span class="math inline">\(N\times M\)</span> matrix. And let the output <span class="math inline">\(\tilde{Y}\)</span> be a <span class="math inline">\(NxP\)</span> matrix (in our example, <span class="math inline">\(P=1\)</span>). If we were just connecting <span class="math inline">\(X\)</span> and <span class="math inline">\(\tilde{Y}\)</span> with no hidden layer, the single weights matrix would be a <span class="math inline">\(M\times P\)</span> matrix:</p>
<p><span class="math display">\[
(\color{blue}N\times \color{red}M)\cdot(\color{red}M\times \color{green}P) = (\color{blue}N\times \color{green}P)
\]</span> (The nonlinear activation doesn’t change the dimensions of the matrices.)</p>
<p>Adding a hidden layer with <span class="math inline">\(Q\)</span> number of neurons means we will still have <span class="math inline">\(N\)</span> different activations for each neuron (i.e.&nbsp;for each datapoint), so that <span class="math inline">\(H\)</span> is a <span class="math inline">\(N\times Q\)</span> matrix. Thus the dimensions of <span class="math inline">\(w^0\)</span> must “match” between these two matrices, and so <span class="math inline">\(w^0\)</span> must be a <span class="math inline">\(M\times Q\)</span> matrix:</p>
<p><span class="math display">\[
(\color{blue}N\times \color{red}M)\cdot(\color{red}M\times \color{purple}Q) = (\color{blue}N\times \color{purple}Q)
\]</span> Similarly <span class="math inline">\(w^1\)</span> must be a <span class="math inline">\(Q\times P\)</span> matrix, and the full operation in terms of matrix dimensions is</p>
<p><span class="math display">\[
(\color{blue}N\times \color{red}M)\cdot(\color{red}M\times \color{purple}Q)\cdot(\color{purple}Q\times \color{green}P) = (\color{blue}N\times \color{green}P).
\]</span> Compare this to the diagram above for <span class="math inline">\(P=1\)</span>, <span class="math inline">\(Q=4\)</span>.</p>
<p><em>Note: If you add bias terms to your model, you may need to remember that the number of columns in both the input <span class="math inline">\(X\)</span> and hidden layer <span class="math inline">\(H\)</span> are greater by one, i.e.&nbsp;<span class="math inline">\(\color{red}{M}\rightarrow \color{red}{M+1}\)</span>, etc.</em></p>
<p>##..a bit of code</p>
<p>The layers <span class="math inline">\(\mathcal{L}^l\)</span> can be represented in Python a list called <code>layers</code> which has a of length 3. Similarly, our weights can be items in a list called <code>weights</code>. Returning to our first sample problem from Part 1:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Training data: input and target </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([  [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>] ])</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]]).T</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># define auxiliary variables</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>N, M, P  <span class="op">=</span> X.shape[<span class="dv">0</span>], X.shape[<span class="dv">1</span>], Y.shape[<span class="dv">1</span>]    <span class="co"># infer matrix shape variables from training data</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>Y_tilde <span class="op">=</span> np.zeros((N,P))                        <span class="co"># setup storage for network output </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Hidden layers</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="dv">4</span>                     <span class="co"># number of hidden neurons, i.e. "size of hidden layer"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.zeros((N,Q))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># weight matrices</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((M,Q)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((Q,P)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Make lists for layers and weights</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X, H, Y_tilde]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [w0, w1]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Just try a sample calculation with random intialization to see how this works </span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed-forward (with linear activation):</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  layers[l<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> np.dot(layers[l], weights[l])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"layers ["</span>,l<span class="op">+</span><span class="dv">1</span>,<span class="st">"] =</span><span class="ch">\n</span><span class="st">"</span>, layers[l<span class="op">+</span><span class="dv">1</span>], sep<span class="op">=</span><span class="st">""</span>)  <span class="co"># sep="" just omits spaces</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>layers [1] =
[[-0.58431094  0.24256333  0.85640655  0.25641678]
 [-0.45815989  0.80598459  1.63132867 -0.10938617]
 [-0.38151492  0.81129189  0.8919347   0.03921142]
 [-0.25536386  1.37471315  1.66685682 -0.32659153]]
layers [2] =
[[-0.97388449]
 [-2.37430685]
 [-1.56570389]
 [-2.96612624]]</code></pre>
</div>
</div>
<p>Generalizing this so it will do the full feed-forward will take a bit more code. We’ll leave a placeholder routine for backpropagation for now.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Activation choices</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x, deriv<span class="op">=</span><span class="va">False</span>): </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  f <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> f<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>f) <span class="cf">if</span> deriv <span class="cf">else</span> f</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x, deriv<span class="op">=</span><span class="va">False</span>):   </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>) <span class="cf">if</span> deriv <span class="cf">else</span> x<span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Placeholder routine to perform backprop.  Will fill in later</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_weights(weights, layers, Y, alpha, activ):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> weights                   <span class="co"># for now, it's a no-op</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(layers, Y, activ<span class="op">=</span>[sigmoid]<span class="op">*</span><span class="dv">2</span>, use_bias<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, maxiter<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">  Routine for training using a multi-layer network</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">    layers:    list of layer values, i.e. layers =  [X, H, Y_tilde]</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Y:         target output</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co">    activ:     list of activation functions. default = list of 2 sigmoids</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">    use_bias:  Whether to include a constant offset in weighted sums</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co">    alpha:     learning rate</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co">    maxiter:   number of iterations to run</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>  lmax <span class="op">=</span> <span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>             <span class="co"># max index of layers, also = # of weights</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> use_bias:           <span class="co"># add a column of 1's to every layer except the last</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        new_col <span class="op">=</span> np.ones((layers[el].shape[<span class="dv">0</span>],<span class="dv">1</span>)) </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        layers[el] <span class="op">=</span> np.hstack((new_col, layers[el])) </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Define weights</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>  np.random.seed(<span class="dv">1</span>)                <span class="co"># for reproducibility</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>  weights <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span>lmax            <span class="co"># allocate slots in a blank list</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):           <span class="co"># "el" because "l" and "1" may look similar</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    weights[el] <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((layers[el].shape[<span class="dv">1</span>], layers[el<span class="op">+</span><span class="dv">1</span>].shape[<span class="dv">1</span>])) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>  loss_hist <span class="op">=</span> []                   <span class="co"># start with an empty list</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(maxiter):</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feed-forward pass</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>      layers[el<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> activ[el](np.dot(layers[el], weights[el]))</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss monitoring</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> layers[lmax] <span class="op">-</span> Y</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    loss_hist.append( (diff<span class="op">**</span><span class="dv">2</span>).mean()  )    <span class="co"># use MSE loss for monitoring</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop code will go here</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> update_weights(weights, layers, Y, alpha, activ)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> weights, layers[lmax], loss_hist        </span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Test this just to make sure it runs</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X,H,Y_tilde]</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist <span class="op">=</span> fit(layers, Y, maxiter<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"weights["</span>,el,<span class="st">"] = </span><span class="ch">\n</span><span class="st">"</span>,weights[el], sep<span class="op">=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weights[0] = 
[[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822]
 [-0.81532281 -0.62747958 -0.30887855 -0.20646505  0.07763347]
 [-0.16161097  0.370439   -0.5910955   0.75623487 -0.94522481]
 [ 0.34093502 -0.1653904   0.11737966 -0.71922612 -0.60379702]]
weights[1] = 
[[ 0.60148914]
 [ 0.93652315]
 [-0.37315164]
 [ 0.38464523]
 [ 0.7527783 ]]</code></pre>
</div>
</div>
<p>…Now that we’ve achieved feed-foward operation of the network, in order to make it ‘learn’ or ‘train’, we need to compare the output value <span class="math inline">\(\tilde{Y}\)</span> (which is the same as <code>layers[2]</code> by the way) to the target value, compute the gradients of the loss function, and then backpropagate in order to update all the weights!</p>
</section>
<section id="backpropagating-theory" class="level2">
<h2 class="anchored" data-anchor-id="backpropagating-theory">Backpropagating: Theory</h2>
<p><strong>TL/DR: You can skip down to the last boxed equation of this section if math scares you. You will not be required to reproduce this. I <em>do</em> want to show you “where this stuff comes from”, but if you find the derivation too intimidating, you can still progress in the course just fine.</strong></p>
<p>Let’s review how we got the gradients for <span class="math inline">\(w^{(1)}\)</span> in Part 2, denoting weighted sums by “<span class="math inline">\(S\)</span>”, e.g.&nbsp;<span class="math inline">\(S^l = \mathcal{L}^l\cdot w^l\)</span>, we just used the Chain Rule: <span class="math display">\[
{\partial L\over\partial w^{(1)}} =
\color{blue}
{\partial L \over\partial \mathcal{L}^{(2)}}
\color{green}
{\partial \mathcal{L}^{(2)} \over\partial S^{(1)}}
\color{red}
{\partial S^{(1)} \over\partial w^{(1)}}
\]</span> We’ll define the first partial derivative to be <span class="math inline">\(\delta^{(2)}\)</span>, which works out (given our choice of <span class="math inline">\(L\)</span> from Part 2) to be <span class="math display">\[\color{blue}{
{\partial L \over\partial \mathcal{L}^{(2)}}
=\delta^{(2)} = \tilde{Y}-Y},\]</span> i.e., it is the error in the final ouput. The next partial derivative (in green) is just the derivative of the activation function <span class="math inline">\(f\)</span>, and the last partial derivative is just <span class="math inline">\(\color{red}{\mathcal{L}^{(1)}}\)</span>, so as we saw in the previous lesson, we can write this ‘schematically’ (i.e.&nbsp;not quite as a properly-set-up matix equation yet) as <span class="math display">\[
{\partial L\over\partial w^{(1)}} =
\color{blue}{\delta^{(2)}}
\color{green} {f^{(1)\prime} }
\color{red}{\mathcal{L}^{(1)}}
\]</span> whereas in proper form it will take on this ordering as a matrix equation: <span class="math display">\[
\boxed{
{\partial L\over\partial w^{(1)}} =
{\mathcal{L}^{(1)}}^T \cdot
{\delta^{(2)}}
{f^{(1)\prime}}
}.
\]</span></p>
<p>To get the gradients for <span class="math inline">\(w^{(0)}\)</span>, we can make use of a similar “<span class="math inline">\(\delta\)</span>” notation if we’re careful in how we define a new <span class="math inline">\(\delta^{(1)}\)</span>. Let’s write out the chain rule, and put parentheses around a particular group of terms for later:</p>
<p><span class="math display">\[
{\partial L\over\partial w^{(0)}} =
\color{blue}{
\left(
{\partial L \over\partial \mathcal{L}^{(2)}}
{\partial \mathcal{L}^{(2)} \over\partial S^{(1)}}
{\partial S^{(1)} \over\partial \mathcal{L}^{(1)}}
\right)}
\color{green}
{\partial \mathcal{L}^{(1)} \over\partial S^{(0)}}
\color{red}
{\partial S^{(0)} \over\partial w^{(0)}}
\]</span> In a manner similar to what we did above, this can be written as <span class="math display">\[
{\partial L\over\partial w^{(0)}} =
\color{blue}{
\left(\delta^{(2)}f^{(1)\prime}w^{(1)}\right)}
\color{green}{f^{(0)\prime}}
\color{red}{\mathcal{L^{(0)}}}
\]</span> We now <em>define</em> the terms in parentheses as <span class="math inline">\(\delta^{(1)}\)</span></p>
<p><span class="math display">\[
\color{blue}{
\delta^{(1)} =  \delta^{(2)}f^{(1)\prime}w^{(1)}
},
\]</span> …which is <em>kind of</em> like “the error in the hidden layer,” or like the final solution error projected backward into the hidden layers via our (momentarily fixed) weights <span class="math inline">\(w^{(1)}\)</span>.</p>
<p>Then our gradients for <span class="math inline">\(w^{(0)}\)</span> take on a similar form as the gradients for <span class="math inline">\(w^{(1)}\)</span>. ‘Schematically’ this looks like <span class="math display">\[
{\partial L\over\partial w^{(0)}} =
\color{blue}{\delta^{(1)}}
\color{green}{f^{(0)\prime}}
\color{red}{\mathcal{L^{(0)}}}
\]</span> and in proper matrix form this is <span class="math display">\[
\boxed{
{\partial L\over\partial w^{(0)}} =
{\mathcal{L}^{(0)}}^T \cdot
{\delta^{(1)}}
{f^{(0)\prime}}
},
\]</span> i.e., the <em>same form</em> as the preceding layer, just “back” one layer. We are backpropagating the errors <span class="math inline">\(\delta^{(l)}\)</span> from one layer to another in order to update the weights.</p>
<p>The weights are then updated as before, except now we will write this ‘generically’ for all weights and layers using the index <span class="math inline">\(l\)</span>: <span class="math display">\[
\boxed{
w^{(l)} := w^{(l)} - \alpha {\mathcal{L}^{(l)}}^T \cdot
{\delta^{(l+1)}}
{f^{(l)\prime}}
},
\]</span> where <span class="math display">\[
\delta^{(l+1)} = \left\{ \begin{array}{l}
\tilde{Y}-Y,\ \ \  &amp;\ &amp;l+1=l_{max} \ \ \ \ ({\rm e.g.} \ l_{max}=2)\\
\delta^{(l+2)}f^{(l+1)\prime}\cdot {w^{(l+1)}}^T, &amp;\ &amp;l+1 &lt; l_{max}
\end{array}\right.
\]</span></p>
</section>
<section id="writing-the-backprop-code" class="level2">
<h2 class="anchored" data-anchor-id="writing-the-backprop-code">Writing the Backprop Code</h2>
<p>Now we’ll use the above analysis to replace the <code>update_weights()</code> function from earlier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_weights(weights, layers, Y, alpha, activ):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Backprop routine, for arbitrary numbers of layers, assuming weights &amp; </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">  activations are defined</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">  </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">  Inputs:</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    weights: list of arrays of weights between each layer</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">    layers:  list of arrays of layer values (post-activation function)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Y:       target output</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">    alpha:   learning rate</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    activ:   list of activation functions for each (non-input) layer</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">  Outputs:</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">    weights (updated)</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  lmax <span class="op">=</span> <span class="bu">len</span>(layers) <span class="op">-</span> <span class="dv">1</span>                        <span class="co"># a useful variable</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> <span class="bu">len</span>(weights)<span class="op">==</span>lmax                     <span class="co"># make sure number of weights match up</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> <span class="bu">len</span>(activ) <span class="op">&gt;=</span> lmax                     <span class="co"># make sure we defined enough activations for the layers</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  delta <span class="op">=</span> layers[lmax] <span class="op">-</span> Y                      <span class="co"># error between output and target</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):              <span class="co"># Count backwards to layer zero</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    fprime <span class="op">=</span> activ[el](np.dot(layers[el], weights[el]), deriv<span class="op">=</span><span class="va">True</span>)   <span class="co"># deriv of activation</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    weights[el] <span class="op">-=</span> alpha <span class="op">*</span> np.dot( layers[el].T, delta<span class="op">*</span>fprime )       <span class="co"># gradient descent step</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> np.dot(delta<span class="op">*</span>fprime, weights[el].T) <span class="cf">if</span> (el<span class="op">&gt;</span><span class="dv">0</span>) <span class="cf">else</span> <span class="va">None</span>   <span class="co"># setup delta for next pass in loop</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> weights           </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's run it!</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X, H, Y_tilde]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>maxiter<span class="op">=</span><span class="dv">5000</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist_2weights <span class="op">=</span> fit(layers, Y, alpha<span class="op">=</span>alpha, maxiter<span class="op">=</span>maxiter)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># compare against a 1-weight (no hidden layer) network:</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X, Y_tilde]</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist_1weight <span class="op">=</span> fit(layers, Y, alpha<span class="op">=</span>alpha, maxiter<span class="op">=</span>maxiter)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the loss history</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>plt.semilogy(loss_hist_1weight, label<span class="op">=</span><span class="st">"No hidden layers"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>plt.semilogy(loss_hist_2weights, label<span class="op">=</span><span class="st">"Hidden layer"</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-08-My-1st-NN-Part-3-Multi-Layer-and-Backprop_files/figure-html/cell-4-output-1.png" width="599" height="429"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's add more hidden neurons</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="dv">50</span>                     <span class="co"># number of hidden neurons, i.e. "size of hidden layer"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.zeros((N,Q))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X, H, Y_tilde]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist_many <span class="op">=</span> fit(layers, Y, alpha<span class="op">=</span>alpha, maxiter<span class="op">=</span>maxiter)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># try a relu activation for the hidden layer (leave output activ as sigmoid!)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, lhm_relu <span class="op">=</span> fit(layers, Y, alpha<span class="op">=</span>alpha, activ<span class="op">=</span>[relu,sigmoid], maxiter<span class="op">=</span>maxiter)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>plt.semilogy(loss_hist_1weight, label<span class="op">=</span><span class="st">"0 hidden neurons"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.semilogy(loss_hist_2weights, label<span class="op">=</span><span class="st">"4 hidden neurons"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.semilogy(loss_hist_many, label<span class="op">=</span><span class="st">"many hidden neurons"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.semilogy(lhm_relu, label<span class="op">=</span><span class="st">"many, relu on hidden"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0, 0.5, 'Loss')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-08-My-1st-NN-Part-3-Multi-Layer-and-Backprop_files/figure-html/cell-5-output-2.png" width="599" height="429"></p>
</div>
</div>
</section>
<section id="solving-xor" class="level2">
<h2 class="anchored" data-anchor-id="solving-xor">Solving XOR</h2>
<p>Now let’s revisit the “XOR” problem that a single neuron couldn’t handle.</p>
<p><span class="math display">\[ \overbrace{
\left[ {\begin{array}{cc}
    0 &amp; 0 \\
    0 &amp; 1 \\
    1 &amp; 0 \\
    1 &amp; 1 \\
  \end{array} } \right]
}^{X} \rightarrow
\overbrace{
\left[ {\begin{array}{c}
   0   \\
   1  \\
   1  \\
   0 \\
  \end{array} } \right]
  }^Y.
\]</span> With our multi-layer network, we can solve this. Note that while an exact solution to the XOR problem exists using only 2 hidden neurons and linear activations, a program can still have a hard time <em>finding</em> a good approximation via gradient descent, and we use 20 hidden neurons to assist, as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>]]).T</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>Y_tilde <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>Y                         <span class="co"># Just allocate some storage</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.zeros((N,<span class="dv">20</span>))                  </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist_xor <span class="op">=</span> fit([X,H,Y_tilde], Y, activ<span class="op">=</span>[relu,sigmoid], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction Y_tilde ="</span>,Y_tilde.T)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target Y (correct answer)  ="</span>,Y.T)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.loglog(loss_hist_xor)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">#print("weights = ",weights)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction Y_tilde = [[0.00480128 0.9962425  0.9963109  0.00374649]]
Target Y (correct answer)  = [[0 1 1 0]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-08-My-1st-NN-Part-3-Multi-Layer-and-Backprop_files/figure-html/cell-6-output-2.png" width="599" height="431"></p>
</div>
</div>
</section>
<section id="same-thing-using-neural-network-libraries-keras-pytorch." class="level1">
<h1>Same thing using neural network libraries Keras &amp; PyTorch.</h1>
<p>Since most of the time we won’t be writing neural network systems “from scratch, by hand” in numpy, let’s take a look at similar operations using libraries such as Keras or PyTorch.</p>
<section id="keras-version" class="level2">
<h2 class="anchored" data-anchor-id="keras-version">Keras version</h2>
<p><a href="https://keras.io/">Keras</a> is so simple to set up, it’s easy to get started. This is what the previous example for XOR looks like “in Keras”:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Activation</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># training data</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>]]).T</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># specify model</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    Dense(n_hidden, input_shape<span class="op">=</span>(X.shape[<span class="dv">1</span>],), activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)])</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># choices for loss and optimization method</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> Adam(lr<span class="op">=</span>alpha)   <span class="co"># We'll talk about optimizer choices later</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>opt, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,metrics<span class="op">=</span>[<span class="st">'binary_accuracy'</span>])</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># training iterations</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>model.fit(X, Y, epochs<span class="op">=</span>maxiter, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Y_tilde = </span><span class="ch">\n</span><span class="st">"</span>, model.predict(X) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 28ms/step

Y_tilde = 
 [[2.1021294e-03]
 [9.9999869e-01]
 [9.9999863e-01]
 [2.0069590e-06]]</code></pre>
</div>
</div>
<p>Keras can get a better appoximation than we did because of the choice of optimizer algorithm. We’ll talk about optimization algorithms (refinements to gradient descent) another time.</p>
</section>
<section id="pytorch-version" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-version">PyTorch version</h2>
<p>Unlike Keras, <a href="https://pytorch.org/">PyTorch</a> does not have any “training wheels.” You have to specify a number of the operations yourself. It’s helpful to have a template to start from, such as the following example for our XOR problem.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch                  <span class="co"># it's 'PyTorch' but the package is 'torch'</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cpu'</span>)  <span class="co"># handy for changing to 'cuda' in GPU runtimes later!</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># training data</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>]],dtype<span class="op">=</span>np.float32)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>]],dtype<span class="op">=</span>np.float32).T</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># re-cast data as PyTorch variables, on the device (CPU or GPU) were calc's are performed</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> torch.tensor(X).to(device), torch.tensor(Y).to(device)   </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># specify model (similar to Keras but not quite)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">20</span>                           <span class="co"># number of hidden neurons</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>          torch.nn.Linear(X.shape[<span class="dv">1</span>], n_hidden),</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>          torch.nn.ReLU(),</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>          torch.nn.Linear(n_hidden, <span class="dv">1</span>),</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>          torch.nn.Sigmoid()</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        ).to(device)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># choices for loss and optimization method</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.BCELoss()      <span class="co"># binary cross-entropy loss</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam([{<span class="st">'params'</span>: model.parameters()}], lr<span class="op">=</span>alpha)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co"># training iterations</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>loss_hist_pytorch <span class="op">=</span> []</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(maxiter):</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>  optimizer.zero_grad()                  <span class="co"># set gradients=0 before calculating more</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>  y_tilde <span class="op">=</span> model(x)                     <span class="co"># feed-forward step</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> loss_fn(y_tilde, y)             <span class="co"># compute the loss</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>  loss_hist_pytorch.append(loss.item())  <span class="co"># save loss for plotting later</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>  loss.backward()                        <span class="co"># compute gradients via backprop</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>  optimizer.step()                       <span class="co"># actually update the weights</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co"># print and plot our results</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Y_tilde = </span><span class="ch">\n</span><span class="st">"</span>, y_tilde.cpu().data.numpy() )</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>plt.loglog(loss_hist_pytorch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Y_tilde = 
 [[2.9900986e-07]
 [9.9999976e-01]
 [9.9999964e-01]
 [3.4445571e-07]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-08-My-1st-NN-Part-3-Multi-Layer-and-Backprop_files/figure-html/cell-8-output-2.png" width="599" height="431"></p>
</div>
</div>
</section>
</section>
<section id="exercise-exploring-hidden-layers." class="level1">
<h1>Exercise: Exploring Hidden Layers.</h1>
<section id="more-with-the-7-segment-display" class="level2">
<h2 class="anchored" data-anchor-id="more-with-the-7-segment-display">More with the 7-segment display</h2>
<p>Using the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> arrays for from previous exercices with the 7-segment display, we’ll explore the effects of adding hidden neurons and different activation functions. Using the code template that follows below,…</p>
<section id="a.-explore-hidden-layer-sizes-activations" class="level3">
<h3 class="anchored" data-anchor-id="a.-explore-hidden-layer-sizes-activations">A. Explore hidden layer sizes &amp; activations</h3>
<ol type="1">
<li>Write code for a <em>new</em> activation function: <span class="math inline">\(\tanh(x)\)</span> and its derivative. <strong>Note: there is already placeholder code for this in the template below</strong></li>
<li>Set training data to be that of the 7-segment display.</li>
<li>Choose (for yourself) a single learning rate (e.g.&nbsp;$=$0.5), and a standard number of iterations (e.g.&nbsp;10000).</li>
</ol>
<p>Then compare results for multiple networks (all with <del>softmax</del>sigmoid activation on the end): 1. A single hidden layer with 20 neurons and (for the hidden layer)… - sigmoid activation - relu activation - tanh activation</p>
<ol start="2" type="1">
<li>A single hidden layer with 100 neurons and (for the hidden layer)…</li>
</ol>
<ul>
<li>sigmoid activation</li>
<li>relu activation</li>
<li>tanh activation</li>
</ul>
</section>
<section id="b.-explore-multiple-hidden-layers" class="level3">
<h3 class="anchored" data-anchor-id="b.-explore-multiple-hidden-layers">B. Explore multiple hidden layers</h3>
<ol start="3" type="1">
<li>Now use two hidden layers, H and H2 with 10 neurons each, and experiment to <em>find the best combination</em> of activations, and best choice of learning rate that gives you the lowest loss at the end of your chosen number of iterations. Check that your predicted output is as you expect.</li>
</ol>
</section>
<section id="assignment" class="level3">
<h3 class="anchored" data-anchor-id="assignment">Assignment:</h3>
<p>Upload a text file of the code for your “winning” entry for #3 to Blackboard. Use the code below as a template.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">## TEMPLATE CODE. Scroll down to "MAKE YOUR CHANGES BELOW", below</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">### LEAVE THIS UNCHANGED</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># First, let's repeat the sigmoid(), relu(), update_weights() and fit() routines</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># already defined, so we have a'standalone' code and can easily make changes</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Activation choices</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x, deriv<span class="op">=</span><span class="va">False</span>): </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  f <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> f<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>f) <span class="cf">if</span> deriv <span class="cf">else</span> f</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x, deriv<span class="op">=</span><span class="va">False</span>):   </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>) <span class="cf">if</span> deriv <span class="cf">else</span> x<span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Backpropagation routine</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_weights(weights, layers, Y, alpha, activ):</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  lmax <span class="op">=</span> <span class="bu">len</span>(layers) <span class="op">-</span> <span class="dv">1</span>                        <span class="co"># a useful variable</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> <span class="bu">len</span>(weights)<span class="op">==</span>lmax                     <span class="co"># make sure number of weights match up</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> <span class="bu">len</span>(activ) <span class="op">&gt;=</span> lmax                     <span class="co"># make sure we defined enough activations for the layers</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  delta <span class="op">=</span> layers[lmax] <span class="op">-</span> Y                      <span class="co"># error between output and target</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):              <span class="co"># Count backwards to layer zero</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    fprime <span class="op">=</span> activ[el](np.dot(layers[el], weights[el]), deriv<span class="op">=</span><span class="va">True</span>)   <span class="co"># deriv of activation</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    weights[el] <span class="op">-=</span> alpha <span class="op">*</span> np.dot( layers[el].T, delta<span class="op">*</span>fprime )       <span class="co"># gradient descent step</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> np.dot(delta<span class="op">*</span>fprime, weights[el].T) <span class="cf">if</span> (el<span class="op">&gt;</span><span class="dv">0</span>) <span class="cf">else</span> <span class="va">None</span>   <span class="co"># setup delta for next pass in loop</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> weights           </span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Routine for training via gradient descent</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(layers, Y, activ<span class="op">=</span>[sigmoid]<span class="op">*</span><span class="dv">2</span>, use_bias<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, maxiter<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>  lmax <span class="op">=</span> <span class="bu">len</span>(layers)<span class="op">-</span><span class="dv">1</span>             <span class="co"># max index of layers, also = # of weights</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> use_bias:           <span class="co"># add a column of 1's to every layer except the last</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        new_col <span class="op">=</span> np.ones((layers[el].shape[<span class="dv">0</span>],<span class="dv">1</span>)) </span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        layers[el] <span class="op">=</span> np.hstack((new_col, layers[el])) </span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Define weights</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>  np.random.seed(<span class="dv">1</span>)                <span class="co"># for reproducibility</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>  weights <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span>lmax            <span class="co"># allocate slots in a blank list</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):           <span class="co"># "el" because "l" and "1" may look similar</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    weights[el] <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((layers[el].shape[<span class="dv">1</span>], layers[el<span class="op">+</span><span class="dv">1</span>].shape[<span class="dv">1</span>]))<span class="op">-</span><span class="dv">1</span> </span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>  loss_hist <span class="op">=</span> []                   <span class="co"># start with an empty list</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(maxiter):</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feed-forward pass</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> el <span class="kw">in</span> <span class="bu">range</span>(lmax):</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>      layers[el<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> activ[el](np.dot(layers[el], weights[el]))</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>    Y_tilde <span class="op">=</span> layers[lmax]</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss monitoring</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> Y_tilde <span class="op">-</span> Y</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>    loss_hist.append( (diff<span class="op">**</span><span class="dv">2</span>).mean()  )    <span class="co"># use MSE loss for monitoring</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop code will go here</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> update_weights(weights, layers, Y, alpha, activ)</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> weights, Y_tilde, loss_hist </span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="co">##### </span><span class="re">END</span><span class="co"> OF PART TO LEAVE UNCHANGED</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="co">#####---------------  MAKE YOUR CHANGES BELOW ------------##############</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a><span class="co"># define the tanh activation function</span></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x, deriv<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> deriv:</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span> <span class="co"># *** Students: replace 'pass' with what the derivative should be</span></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.tanh(x)</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="co">## Students: replace X, Y with 7-segment data instead</span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">1</span>,<span class="dv">1</span>]],dtype<span class="op">=</span>np.float32)</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>]],dtype<span class="op">=</span>np.float32).T</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a>Y_tilde <span class="op">=</span> np.copy(Y)                     <span class="co"># Just allocates some storage for Y_tilde</span></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="co">##  Hidden layers: Students: Change Q, the number of hidden neurons, as needed</span></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X.shape[<span class="dv">0</span>]                           <span class="co"># this just grabs the number of rows in X</span></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> np.zeros((N,Q))                  </span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a>H2 <span class="op">=</span> np.zeros((N,Q))                     <span class="co"># extra hidden layer, might not be used</span></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a><span class="co">## Students: change this as instructed</span></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>layers <span class="op">=</span> [X, H, Y_tilde]              <span class="co"># later, add another layer H2 when instructed</span></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a>activ <span class="op">=</span> [sigmoid, sigmoid, sigmoid]   <span class="co"># change the first (2) activation(s) as instructed</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span>                           <span class="co"># play around with this</span></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a><span class="co">## LEAVE THIS PART UNCHANGED</span></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>weights, Y_tilde, loss_hist <span class="op">=</span> fit(layers, Y, activ<span class="op">=</span>activ, alpha<span class="op">=</span>alpha)</span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(formatter<span class="op">=</span>{<span class="st">'float'</span>: <span class="kw">lambda</span> x: <span class="st">"</span><span class="sc">{0:0.2f}</span><span class="st">"</span>.<span class="bu">format</span>(x)}) <span class="co"># 2 sig figs</span></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction Y_tilde =</span><span class="ch">\n</span><span class="st">"</span>,Y_tilde.T)</span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target Y (correct answer)  =</span><span class="ch">\n</span><span class="st">"</span>,Y.T)</span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a>plt.loglog(loss_hist)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction Y_tilde =
 [[0.01 0.99 0.99 0.01]]
Target Y (correct answer)  =
 [[0.00 1.00 1.00 0.00]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-08-My-1st-NN-Part-3-Multi-Layer-and-Backprop_files/figure-html/cell-9-output-2.png" width="599" height="431"></p>
</div>
</div>
</section>
</section>
<section id="preview-of-next-lesson-mnist" class="level2">
<h2 class="anchored" data-anchor-id="preview-of-next-lesson-mnist">Preview of next lesson: MNIST</h2>
<p>Now that you’ve built up some experience with reading digits, let’s move to handwritten digits! This is a problem usually solved with an architecture called a Convolutional Neural Network, but our ordinary feed-forward network can do it too.</p>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST database of handwritten digits</a> is a classic dataset that every ML student works on. It consists of a large number images of handwritten digits only 28x28 pixels in size. We will “flatten” these into a row of 784 columns, and output a <span class="math inline">\(\tilde{Y}\)</span> of one-hot-encoded vectore just like we did for the output of the 7-segment display (same digits, 0 to 9!).</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>