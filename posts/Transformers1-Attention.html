<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Scott H. Hawley">
<meta name="dcterms.date" content="2023-08-21">
<meta name="description" content="If you can get this one thing, the rest will make sense.">

<title>blog - To Understand Transfomers, Focus on Attention</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - To Understand Transfomers, Focus on Attention">
<meta name="twitter:description" content="If you can get this one thing, the rest will make sense.">
<meta name="twitter:image" content="https://drscotthawley.github.io/blog/posts/images/TransformerOvalInOut.gif">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">To Understand Transfomers, Focus on Attention</h1>
                  <div>
        <div class="description">
          If you can get this one thing, the rest will make sense.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">architectures</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Scott H. Hawley </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 21, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface"><span class="header-section-number">1</span> Preface</a></li>
  <li><a href="#building-intuition" id="toc-building-intuition" class="nav-link" data-scroll-target="#building-intuition"><span class="header-section-number">2</span> Building Intuition</a>
  <ul class="collapse">
  <li><a href="#weighted-averages" id="toc-weighted-averages" class="nav-link" data-scroll-target="#weighted-averages"><span class="header-section-number">2.1</span> Weighted Averages</a></li>
  </ul></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context"><span class="header-section-number">3</span> Historical “Context”</a></li>
  <li><a href="#making-attention-mathematical" id="toc-making-attention-mathematical" class="nav-link" data-scroll-target="#making-attention-mathematical"><span class="header-section-number">4</span> Making Attention Mathematical</a>
  <ul class="collapse">
  <li><a href="#what-is-softmax" id="toc-what-is-softmax" class="nav-link" data-scroll-target="#what-is-softmax"><span class="header-section-number">4.0.1</span> What is Softmax?</a></li>
  <li><a href="#what-are-q-k-v" id="toc-what-are-q-k-v" class="nav-link" data-scroll-target="#what-are-q-k-v"><span class="header-section-number">4.1</span> What are Q, K, &amp; V?</a></li>
  <li><a href="#similarity-and-its-measures" id="toc-similarity-and-its-measures" class="nav-link" data-scroll-target="#similarity-and-its-measures"><span class="header-section-number">4.2</span> Similarity and its Measures</a>
  <ul class="collapse">
  <li><a href="#euclidean-distance-delta" id="toc-euclidean-distance-delta" class="nav-link" data-scroll-target="#euclidean-distance-delta"><span class="header-section-number">4.2.1</span> <span style="color:red;">Euclidean Distance: <span class="math inline">\(\Delta\)</span></span></a></li>
  <li><a href="#cosine-similarity-costheta" id="toc-cosine-similarity-costheta" class="nav-link" data-scroll-target="#cosine-similarity-costheta"><span class="header-section-number">4.2.2</span> <span style="color:yellow;">Cosine Similarity: <span class="math inline">\(\cos\theta\)</span></span></a></li>
  <li><a href="#scaled-dot-product-qkt-sqrtd_k" id="toc-scaled-dot-product-qkt-sqrtd_k" class="nav-link" data-scroll-target="#scaled-dot-product-qkt-sqrtd_k"><span class="header-section-number">4.2.3</span> <span style="color:#54B98F;"> 🌟 Scaled Dot Product: <span class="math inline">\(QK^T / \sqrt{d_k}\)</span></span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#extensions-multi-headedness-masking" id="toc-extensions-multi-headedness-masking" class="nav-link" data-scroll-target="#extensions-multi-headedness-masking"><span class="header-section-number">5</span> Extensions: Multi-Headedness &amp; Masking</a>
  <ul class="collapse">
  <li><a href="#multi-headed-attention" id="toc-multi-headed-attention" class="nav-link" data-scroll-target="#multi-headed-attention"><span class="header-section-number">5.1</span> Multi-Headed Attention</a></li>
  <li><a href="#masked-attention" id="toc-masked-attention" class="nav-link" data-scroll-target="#masked-attention"><span class="header-section-number">5.2</span> Masked Attention</a></li>
  </ul></li>
  <li><a href="#sec-nextup" id="toc-sec-nextup" class="nav-link" data-scroll-target="#sec-nextup"><span class="header-section-number">6</span> Summary and Further Topics</a></li>
  <li><a href="#afterward-i-wrote-a-song" id="toc-afterward-i-wrote-a-song" class="nav-link" data-scroll-target="#afterward-i-wrote-a-song"><span class="header-section-number">7</span> Afterward: I Wrote A Song</a></li>
  <li><a href="#sec-references" id="toc-sec-references" class="nav-link" data-scroll-target="#sec-references"><span class="header-section-number">8</span> References</a></li>
  <li><a href="#sec-softmax-fun" id="toc-sec-softmax-fun" class="nav-link" data-scroll-target="#sec-softmax-fun"><span class="header-section-number">9</span> Appendix: Fun with Softmax - Interactive!</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/TransformerOvalInOut.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">(GIF animation, zooming in on Attention part of Transformer model diagram)</figcaption>
</figure>
</div>
<section id="preface" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preface</h1>
<p>“Attention is All You Need” (hereafter AIAYN) is the name of the 2017 paper <span class="citation" data-cites="aiayn"><a href="#ref-aiayn" role="doc-biblioref">[1]</a></span> that introduced Transformer models, a neural network architecture that eventually “took over” as the preferred architecture for many machine learning tasks, with models such a BERT and the GPT series (“#ChatGPT” - for this blog’s SEO) becoming “foundation” models upon which further systems continue to be built.</p>
<p>Yet I found the multiplicity of Transformers’ “moving parts” to be daunting, and this hindered my numerous, ardent attempts at studying them – see the References in <a href="#sec-references">Section&nbsp;8</a> for a list of all the papers, tutorials, and videos I consumed over the years. Many of these treatments do little to reduce the complexity, and thus the simplicity of the original paper’s title is often lost.</p>
<p>This is partly because Attention <em>isn’t really</em> “all you need,” and it’s a bit of a joke to pair the title with such a complex model diagram like the one shown in the GIF image above. However, I’ve come to see the Attention mechanism as the <em>key piece</em>, the <em>central mechanism</em>, for which everything else is (pretty much) just an add-on.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
An Analogy
</div>
</div>
<div class="callout-body-container callout-body">
<p>I liken this to rocketry: whether you’re talking about model rockets or Saturn Vs, the essential part is the engine which uses a chemical reaction to produce thrust, yet you still need to add on stabilizing fins and a cone for aerodynamics, etc. And if you <em>really</em> want to lift heavy payloads then you’ll need all kinds of auxiliary systems to regulate fuel flow and control systems for stability. But if you want to understand a Saturn V (I don’t, but I’d imagine…) you wouldn’t start by listing all the different systems and go over them one by one, rather you’d start from the basic idea and then go into why these other pieces were added on. Such is how I’m recommending that we approach Transformer models.</p>
</div>
</div>
<p>Things finally “clicked” for me this summer – years later! What finally helped was watching Andrej Karpathy’s excellent YouTube tutorial <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span> for the <em>second time</em>, when I noticed he homed in on the Attention part of the model and explained it as being a <em>weighted average of words.</em> I believe that if you grasp that central part, then all the other parts of the Transformer can be regarded as “bells and whistles” that help it work better.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Throughout this tutorial I will highlight key ideas with the phrase “Attention is just…” to share different perspectives that have helped me understand.</p>
<p>In most places below, the code is hidden by default (behind “&gt; Show the code” expanders) because I don’t want you getting distracted by how I made certain figures; the point is the figure, not the code. This lesson is intended to be “coding-free”!</p>
</div>
</div>
</section>
<section id="building-intuition" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Building Intuition</h1>
<p>Certain parts of an input hold greater significance than others. Take, for instance, when you observe an image, your attention is likely drawn to objects in the foreground rather than the background. In the following picture, the dog, for instance, captures the attention of most people (and AI systems!):</p>
<div id="fig-doggie" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/doggie_vit_attn_map.png" width="96%" align="left" class="figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Example of attention being given to a foreground object, like this cute doggy! In the image on the right, the background is darkened compared to the foreground, indicating that the foreground is receiving more attention than the background. (Source: Eunkwang Jeon, <a href="https://github.com/jeonsworld/ViT-pytorch">ViT-pytorch</a>)</figcaption>
</figure>
</div>
<p>Alternatively, when you’re reading or listening to someone speaking, certain words carry more weight than others and contribute differently to the meaning formed in your mind. Consider the following sentence:</p>
<blockquote class="blockquote">
<p>“Please go to the store and get some milk.”</p>
</blockquote>
<p>If you’re trying understand the command being given, the words “go”, “store”, “get”, and “milk” probably matter the most to you. If what you’re interested in is the <em>tone</em> (e.g.&nbsp;“is the speaker being polite?”) then probably the “Please” would matter most.</p>
<p>Since the topic of Attention often comes up in the context of Natural Language Processing (NLP), we’ll stick to text for this lesson, but know that “<a href="https://paperswithcode.com/method/vision-transformer">Vision Transformers</a>” for image processing are definitely a thing <span class="citation" data-cites="vit_paper"><a href="#ref-vit_paper" role="doc-biblioref">[3]</a></span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a way to give certain parts of an input more weight than others.</p>
</div>
</div>
<section id="weighted-averages" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="weighted-averages"><span class="header-section-number">2.1</span> Weighted Averages</h2>
<p>On computers, the way we often emphasize certain parts of an array more than others is to multiply them by another array that has the “weights” to be assigned. In some other contexts, the “weights” array may be also called a “mask”, such as a “hard mask” of 1’s and 0’s to turn on or off parts of the array, or a “soft mask” with floating-point values to emphasize certain elements.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a soft mask.</p>
</div>
</div>
<p>Let’s use the sample sentence above, and <em>make up</em> some mask weights to stress the relative importants of different words. We’ll display the weight values with a colored background to visualize their magnitude (darker = more weight):</p>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML, display</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> <span class="st">"Please go to the store and get some milk"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">input</span>.split()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.array([<span class="fl">0.5</span>,<span class="fl">.9</span>,<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">.7</span>, <span class="fl">0.1</span>,<span class="fl">.98</span>,<span class="fl">0.1</span>,<span class="fl">.99</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>:words,<span class="st">"weight"</span>:mask})</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>display(df.set_index(<span class="st">'word'</span>).T.style.background_gradient(vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of mask weights = </span><span class="sc">{</span>mask<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="word-weights-0" class="cell-output cell-output-display">
<style type="text/css">
#T_e5ca5_row0_col0 {
  background-color: #73a9cf;
  color: #f1f1f1;
}
#T_e5ca5_row0_col1 {
  background-color: #045382;
  color: #f1f1f1;
}
#T_e5ca5_row0_col2, #T_e5ca5_row0_col5, #T_e5ca5_row0_col7 {
  background-color: #f0eaf4;
  color: #000000;
}
#T_e5ca5_row0_col3 {
  background-color: #fef6fa;
  color: #000000;
}
#T_e5ca5_row0_col4 {
  background-color: #187cb6;
  color: #f1f1f1;
}
#T_e5ca5_row0_col6 {
  background-color: #023d60;
  color: #f1f1f1;
}
#T_e5ca5_row0_col8 {
  background-color: #023a5b;
  color: #f1f1f1;
}
</style>

<table id="T_e5ca5" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_e5ca5_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_e5ca5_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_e5ca5_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_e5ca5_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_e5ca5_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_e5ca5_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_e5ca5_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_e5ca5_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_e5ca5_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_e5ca5_row0_col0" class="data row0 col0">0.500</td>
<td id="T_e5ca5_row0_col1" class="data row0 col1">0.900</td>
<td id="T_e5ca5_row0_col2" class="data row0 col2">0.100</td>
<td id="T_e5ca5_row0_col3" class="data row0 col3">0.010</td>
<td id="T_e5ca5_row0_col4" class="data row0 col4">0.700</td>
<td id="T_e5ca5_row0_col5" class="data row0 col5">0.100</td>
<td id="T_e5ca5_row0_col6" class="data row0 col6">0.980</td>
<td id="T_e5ca5_row0_col7" class="data row0 col7">0.100</td>
<td id="T_e5ca5_row0_col8" class="data row0 col8">0.990</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sum of mask weights = 4.38</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above example, I made up a weight of nearly zero for “the” because it really doesn’t matter. The meaning of the sentence would still be unambiguous without “the”. We’ll use this trick of setting some weights to zero later on when we talk about “Masked Attention” for the Decoder part of the Transformer.</p>
</div>
</div>
<p>We’ll make it so that the weights all add up to one. If all <span class="math inline">\(N\)</span> words were weighted equally, the weight values would all be <span class="math inline">\(1/N\)</span>. For other weighting schemes, we divide the mask weights by their sum to get our attention weights. For the example sentence above, dividing by the sum gives us:</p>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> mask<span class="op">/</span>mask.<span class="bu">sum</span>()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>:words,<span class="st">"weight"</span>:attention})</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>display(df.set_index(<span class="st">'word'</span>).T.style.background_gradient(vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="fl">.22</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of attention weights = </span><span class="sc">{</span>attention<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="word-weights-1" class="cell-output cell-output-display">
<style type="text/css">
#T_9ed2e_row0_col0 {
  background-color: #6ba5cd;
  color: #f1f1f1;
}
#T_9ed2e_row0_col1 {
  background-color: #034973;
  color: #f1f1f1;
}
#T_9ed2e_row0_col2, #T_9ed2e_row0_col5, #T_9ed2e_row0_col7 {
  background-color: #f0eaf4;
  color: #000000;
}
#T_9ed2e_row0_col3 {
  background-color: #fef6fa;
  color: #000000;
}
#T_9ed2e_row0_col4 {
  background-color: #0f76b3;
  color: #f1f1f1;
}
#T_9ed2e_row0_col6, #T_9ed2e_row0_col8 {
  background-color: #023858;
  color: #f1f1f1;
}
</style>

<table id="T_9ed2e" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_9ed2e_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_9ed2e_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_9ed2e_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_9ed2e_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_9ed2e_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_9ed2e_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_9ed2e_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_9ed2e_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_9ed2e_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_9ed2e_row0_col0" class="data row0 col0">0.114</td>
<td id="T_9ed2e_row0_col1" class="data row0 col1">0.205</td>
<td id="T_9ed2e_row0_col2" class="data row0 col2">0.023</td>
<td id="T_9ed2e_row0_col3" class="data row0 col3">0.002</td>
<td id="T_9ed2e_row0_col4" class="data row0 col4">0.160</td>
<td id="T_9ed2e_row0_col5" class="data row0 col5">0.023</td>
<td id="T_9ed2e_row0_col6" class="data row0 col6">0.224</td>
<td id="T_9ed2e_row0_col7" class="data row0 col7">0.023</td>
<td id="T_9ed2e_row0_col8" class="data row0 col8">0.226</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sum of attention weights = 1</code></pre>
</div>
</div>
<p>Now, the words themselves will <em>each</em> be represented by an array of numbers we call a “word vector”. Usually, word vectors are viewed as living in some high-dimensional space given by 256, 512, or even 1024 numbers. When we combine the word vectors we get a “context vector”, and the contributions of the word vectors will be weighted appropriately. These weights are <em>literally</em> the attention. To put it differently:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a weighted average of inputs.</p>
</div>
</div>
<p>So how should we actually <em>get</em> those weights – how should they be determined? That’s what we’ll cover for the rest of this lesson. Firstly, it helps if we understand the context of something called “context vectors.”</p>
</section>
</section>
<section id="historical-context" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Historical “Context”</h1>
<p>There are a few different attention-like schemes that have arisen over the years. The cleanest starting point for our lesson is the work of Bahdanau et al <span class="citation" data-cites="bahdanau_attn"><a href="#ref-bahdanau_attn" role="doc-biblioref">[4]</a></span> who were making improvements to automated language translation models that at the time were using an architecture called a Recurrent Neural Network (RNN, which you don’t need to understand for this lesson, or the fact that specific flavor of RNN was called an LSTM which stood for Long Short-Term Memory).</p>
<p>At issue was traditional models’ management of the “context vector” of numbers that would store the “meaning” of the text as it was being processed. You can think of the context vector as being like the “state” of the machine as it works through the text.</p>
<p>The simplest context vector is just the average of the word vectors making up the sentence. This method is called “<strong>Bag of Words</strong>”. There’s a <em>lot</em> you can do with a Bag of Words model. If we were to weight certain words more than others – say using Attention – then we’d have an even better model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a weighted Bag of Words.</p>
</div>
</div>
<p>Note that a Bag of Words doesn’t take into account word ordering. For example, on the outdoor tables at the coffee shop where I’m working, <a href="https://www.bongojava.com/pages/bongo-java">Bongo Java in Nashville, TN</a>, there are signs that would mean something very different if the word “outside” were moved to the end of the sentence:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/bongo_outside.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Bongo Java image: “Please no outside food or drink” != “Please no food or drink outside.” This is on an outdoor table where the owners clearly expect you to be eating, so only the first sentence makes sense in this case. A Bag of Words model would incorrectly treat those two sentences equivalently.</figcaption>
</figure>
</div>
<p>To get a sense of word ordering, we’ll need another part of the Transfomer model called Positional Encoding, which we mention in <a href="#sec-nextup">Section&nbsp;6</a>.</p>
</section>
<section id="making-attention-mathematical" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Making Attention Mathematical</h1>
<p>We’ll start by showing the full formula for the Attention mechanism in Transformer models, and then we’ll unpack that to explain what its parts are and <em>why</em> they are there.</p>
<p>The set of attention weights <span class="math inline">\(A\)</span> – the ones used in the weighted average – are given by</p>
<p><span id="eq-atnn-mine"><span class="math display">\[ A = {\rm softmax}\left( Q K^T \over \sqrt{d_k} \right). \tag{1}\]</span></span> We then multiply these by the input values <span class="math inline">\(V\)</span> to get the context <span class="math inline">\(C = A V\)</span>. Or, putting these together in the form that appears in the original AIAYN paper,</p>
<p><span id="eq-atnn-full"><span class="math display">\[ {\rm Attention}(Q,K,V) = {\rm softmax}\left( Q K^T \over \sqrt{d_k} \right)V \tag{2}\]</span></span></p>
<style>
.img3 {
    /*transform-origin: top left;*/
    width:140px;
    transition:transform 0.4s ease;
    position: relative;
    z-index: 0;
}
.img3:hover {
    -webkit-transform:scale(2.07);
    transform:scale(2.07);
    z-index:1;
}
</style>
<p>Here’s the diagram from the AIAYN paper illustrating this process :</p>
<div id="fig-qkv" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/qkv.png" class="img3 img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> (Mouse-over or press to expand.) Attention computation diagram from AIAYN paper <span class="citation" data-cites="aiayn"><a href="#ref-aiayn" role="doc-biblioref">[1]</a></span>. “MatMul” = “matrix multiplication” or what we’ll also call a “dot product”, which includes summation (which given our normalized weights means we get a weighted average).</figcaption>
</figure>
</div>
<p>So then, what is softmax, what are Q and K, and what’s that square root doing in the denominator? In what follows, we’re going to emphasize the intuitive understanding of what’s going on and why. (If you’re satisfied to know the “is” without the “why,” then this tutorial is probably not for you.)</p>
<section id="what-is-softmax" class="level3" data-number="4.0.1">
<h3 data-number="4.0.1" class="anchored" data-anchor-id="what-is-softmax"><span class="header-section-number">4.0.1</span> What is Softmax?</h3>
<p><a href="https://www.youtube.com/watch?v=ytbYRIN0N4g">Softmax</a> is a normalized element-wise exponential function that operates on a vector (i.e.&nbsp;an array) of data <span class="math inline">\(\vec{x}\)</span> by the following equation:</p>
<p><span class="math display">\[ {\rm softmax}(\vec{x})_i = {e^{x_i}  \over \sum_i e^{x_i}} \]</span> where <span class="math inline">\(x_i\)</span> are the individual elements (i.e., components) of <span class="math inline">\(\vec{x}\)</span>. Usually one sees softmax used as the final output of a classifier or recommendation system, and indeed the final stage of the Transfomer’s <em>Decoder</em> is a softmax function.</p>
<p><em>Why</em> is softmax appearing in the Attention calculation? Well, we <em>could</em> use something else, but softmax a great choice! ;-) What we need for our attention weights is a set of positive numbers that add up to 1 (what I call “parts of a whole” and what mathematicians call a “[probability] distribution”). Usually the <span class="math inline">\(x_i\)</span> values can be positive or negative, so we need <em>some</em> function to map from sets of arbitrary real numbers to our weights that are “parts of a whole.” (Mathematicians call such functions “distribution-generating functions.”) We could choose some other function instead of softmax to do the job we need – say, something monotonic and positive like a sigmoid function – but softmax is arguably the best choice for a distribution generator because…</p>
<ul>
<li>other functions may be slower computationally</li>
<li>people are already accustomed to using softmax for other things</li>
<li>it’s got some sweet properties in terms of both probability theory <em>and</em> calculus</li>
<li>and it’s hella versatile – see the interactive Appendix “Fun With Softmax” aka <a href="#sec-softmax-fun">Section&nbsp;9</a>.</li>
</ul>
</section>
<section id="what-are-q-k-v" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="what-are-q-k-v"><span class="header-section-number">4.1</span> What are Q, K, &amp; V?</h2>
<p>Literally, these stand for “Query,” “Key,” and “Value,” respectively. We already mentioned the value <span class="math inline">\(V\)</span> as the “inputs” that are being attended. Note: these “inputs” could be actual data points, or just the outputs from a previous layer in the (stacked) Transformer architecture.</p>
<p>“Query” and “Key” are typically described in terms of search algorithms, with <span class="math inline">\(Q\)</span> being “what am I looking for?” and <span class="math inline">\(K\)</span> being, say, the keywords or metadata associated with the “actual” value <span class="math inline">\(V\)</span>. For example, if you’re searching on YouTube for videos, you’re not searching the <em>videos themselves</em>, you’re searching over metadata/“keys” <em>about</em> those videos, and then the <em>values</em> of the actual, corresponding videos are served up.</p>
<p><em>Buuuuut</em>…in the context of Transformer models, I tend to find the “search” analogy to be lacking…or at least it can be an impediment to my understanding. So I’ll say it this way: <span class="math inline">\(Q\)</span> is usually the “current state of the machine,” such as the context vector describing the preceding words so that we can predict the <em>next</em> word.</p>
<p>In the simplest implementations <span class="math inline">\(K\)</span> is exactly the same as <span class="math inline">\(V\)</span>. Like, literally <span class="math inline">\(K\)</span>=<span class="math inline">\(V\)</span> can work in many cases. We’re not going to do that, but just to say, it can be done. Instead we’ll say that <span class="math inline">\(K\)</span> is an “alternate reduced-dimensional embedding of the embedding of <span class="math inline">\(V\)</span>,” meaning that typically <span class="math inline">\(Q, K,\)</span> and <span class="math inline">\(V\)</span> are usually not the vectors <em>themselves</em> but rather the result of a remapping by a (typically) single-layer neural network which adds a little extra expressiveness to the model and which could also be used to project these vectors into some new space with a different (reduced) number of dimensions which is called <span class="math inline">\(d_k\)</span>.</p>
<p>One more thing: we’ve been saying “vectors” but technically <span class="math inline">\(Q, K,\)</span> and <span class="math inline">\(V\)</span> are <em>matrices</em> consisting of multiple vectors stacked up in rows to form a “batch” that allows us to take advantage of the parallel-processing capabilities of GPUs. In our discussion, the matrix nature doesn’t really affect anything. (If you like, you might think of the vector version as a matrix with a batch size of 1.)</p>
</section>
<section id="similarity-and-its-measures" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="similarity-and-its-measures"><span class="header-section-number">4.2</span> Similarity and its Measures</h2>
<p>The thing that goes inside the parentheses of the softmax function is something that measures the <em>similarity</em> between the query <span class="math inline">\(Q\)</span> and the key <span class="math inline">\(K\)</span>. Treating these as vectors, we have a few choices that I’ll show on a diagram:</p>
<div id="fig-sim-qkt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/similarity_qkt_vectors.png" class="img-fluid figure-img" width="350"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Diagram illustrating a few similarity measures for vectors <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span>.</figcaption>
</figure>
</div>
<p>Our options for measuring similarity include the following 3, the last of which is what the Transformer’s Attention uses:</p>
<section id="euclidean-distance-delta" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="euclidean-distance-delta"><span class="header-section-number">4.2.1</span> <span style="color:red;">Euclidean Distance: <span class="math inline">\(\Delta\)</span></span></h3>
<p>This is just what you think: it’s just the magnitude of the difference <span class="math inline">\(\Delta = Q-K\)</span>, i.e.&nbsp;square the differences of all the coordinates and add up the squares. (The true Euclidean distance would be the square root of that sum, but usually we’d skip that.) Another way of saying is that we take the “dot product” of <span class="math inline">\(\Delta\)</span> with itself, <span class="math inline">\(\vec{\Delta} \cdot \vec{\Delta}\)</span> which in matrix form would be <span class="math inline">\(\Delta\)</span> times its transpose, ie. <span class="math inline">\(\Delta\,\Delta^T\)</span>.</p>
<p>The Euclidean distance is “okay,” and there’s nothing inherently wrong with this measure, but for very high-dimensional spaces it can be tricky to make work well (e.g., it can require extra tweaking to get the scaling right) since most points end up both super-far away from each other and yet (paradoxically?) also distributed around near the surface of a hypersphere. 🤷‍♂️ If what I just wrote didn’t resonate with you, just shrug like that emoji-man and read on.</p>
</section>
<section id="cosine-similarity-costheta" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="cosine-similarity-costheta"><span class="header-section-number">4.2.2</span> <span style="color:yellow;">Cosine Similarity: <span class="math inline">\(\cos\theta\)</span></span></h3>
<p>This measure is <em>extremely</em> common in the field of NLP: We just look at the angle between <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span>, and this maps really nicely if you use the cosine of the angle. So, similar vectors will point in nearly the same direction and have a cosine close to 1, whereas vecctors that are pointing in opposite directions will have a cosine of -1. Notice that this only considers the <em>directions</em> of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> and ignores their magnitudes <span class="math inline">\(|Q|\)</span> and <span class="math inline">\(|K|\)</span>. It’s as if we’re treating <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> like they were <em>unit</em> vectors (which is often fine or at least “good enough”). In fact, the cosine is just the dot product bewteen the unit-vector versions of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span>:</p>
<p><span class="math display">\[\cos\theta = {Q\over |Q|}\cdot{K \over |K|} = {Q K^T \over |Q||K|}\]</span></p>
<p>As we said, this is really handy, and works fine in many situations. But…what if we <em>didn’t</em> throw away the magnitude information? Might that work even better? Also, consider the fact it takes extra effort to compute the denominator <span class="math inline">\(|Q||K|\)</span> every time. What if we just computed <span class="math inline">\(Q K^T\)</span> and went with that?</p>
</section>
<section id="scaled-dot-product-qkt-sqrtd_k" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="scaled-dot-product-qkt-sqrtd_k"><span class="header-section-number">4.2.3</span> <span style="color:#54B98F;"> 🌟 Scaled Dot Product: <span class="math inline">\(QK^T / \sqrt{d_k}\)</span></span></h3>
<p>The quantity <span class="math inline">\(Q K^T\)</span> is a “dot product” that yields a distance corresponding to the length of <span class="math inline">\(Q\)</span> projected along <span class="math inline">\(K\)</span> (or vice versa), as shown in <a href="#fig-sim-qkt">Figure&nbsp;3</a> above. This works like the cosine similarity but includes the magnitude, and allows us to consider the releative length of the two vectors. It also can map to any real number instead of just the interval <span class="math inline">\([-1, 1]\)</span> like the cosine. But if we just use <span class="math inline">\(Q K^T\)</span>, we run into a problem in high dimensions similar to what we see for the Euclidean distance. We’ll look at the fix for that in the next sub-section, which is where the <span class="math inline">\(1\sqrt{d_k}\)</span> comes in…</p>
<section id="why-the-1sqrtd_k" class="level4" data-number="4.2.3.1">
<h4 data-number="4.2.3.1" class="anchored" data-anchor-id="why-the-1sqrtd_k"><span class="header-section-number">4.2.3.1</span> Why the <span class="math inline">\(1/\sqrt{d_k}\,\)</span> ?</h4>
<p>Simply put, it keeps the variance of our resulting data from exploding as the number of dimensions increases. Take a look at <a href="#fig-sqrtd">Figure&nbsp;4</a> (a) and (b).</p>
<style>
.img1 {
    transition:max-width 0.4s ease;
    position: relative;
    z-index:0;
}
.img1:hover {
    max-width: 210%;
    z-index:1;
}
.img2 {
    transform-origin: top right;
    transition:transform 0.4s ease;
    position: relative;
    z-index: 0;
}
.img2:hover {
    -webkit-transform:scale(2.07);
    transform:scale(2.07);
    z-index:1;
}
</style>
<div id="fig-sqrtd" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drscotthawley.github.io/blog/posts/2022-01-24-MultiDim-DotProducts_files/figure-html/cell-5-output-1.png" class="img1 img-fluid figure-img"></p>
<figcaption class="figure-caption">a) No <span class="math inline">\(1/\sqrt{d_k}\)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drscotthawley.github.io/blog/posts/2022-01-24-MultiDim-DotProducts_files/figure-html/cell-6-output-1.png" class="img2 img-fluid figure-img"></p>
<figcaption class="figure-caption">b) With <span class="math inline">\(1/\sqrt{d_k}\)</span></figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> (Mouse-over or press to expand.) Histograms of dot-products between random vectors in increasingly high-dimensional spaces, both without (a) and with (b) the <span class="math inline">\(1/\sqrt{d_k}\,\)</span> factor. Note how, in (a), the horizonal scale and standard deviation <span class="math inline">\(\sigma\)</span> both grow with the number of dimensions (written as <span class="math inline">\(D\)</span> instead of <span class="math inline">\(d_k\)</span>), whereas in (b) these remain constant. Source: Another blog post of mine <span class="citation" data-cites="blog-vec-orthog"><a href="#ref-blog-vec-orthog" role="doc-biblioref">[5]</a></span>.</figcaption><p></p>
</figure>
</div>
<p>The first figure (a) shows histograms of <span class="math inline">\(QK^T\)</span> for a bunch of random vectors as we increase the number of dimensions: what happens is that the “spread” of these values grows arbitrarily big. And generally in machine learning this can be a bad thing – we like to keep our values “normalized” within a certain range. It turns out that if you just divide by the <em>square root</em> of the number of dimensions…well, check out what happens in (b)! The data stays nicely distributed in the same range regardless of how many dimensions you want to use. 👍</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inputs will get high attention-weight whenever their key <span class="math inline">\(K\)</span> has high similarity with the query <span class="math inline">\(Q\)</span>.</p>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="extensions-multi-headedness-masking" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Extensions: Multi-Headedness &amp; Masking</h1>
<p>Two “extensions” of Attention that show up in the Transformer model are worth including. But maybe some readers are starting to hope we’re nearly finished, so I’ll discuss these extensions in the following expandable “Details” section for those who want to read about them:</p>
<details>
<section id="multi-headed-attention" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="multi-headed-attention"><span class="header-section-number">5.1</span> Multi-Headed Attention</h2>
<p>If we extend the dimensionality of our attention appararatus, we can look at different “senses” of the sentence. These extra dimensions are referred to as “heads”. One head might focus on the sheer “facts” or instructions, such as “go get milk”. Another head might sense the tone of the text, such as the use of “Please” making it polite. By allowing for different “attention heads”, this can be handled automatically, using the broadcasting capability of Python.</p>
</section>
<section id="masked-attention" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="masked-attention"><span class="header-section-number">5.2</span> Masked Attention</h2>
<p>We’ve already likened Attention to a mask function. When saying “Masked Attention,” the authors mean that some values will be given zero weight, effectively removing them from the sequence. This kind of masked attention is found in the Decoder part of the Transfomer when we are trying to (“auto-regressively”) generate new words without “looking into the future” of the training data. Karpathy’s video <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span> does a great job of explaining this.</p>
<p>Say we were training a model to generate the phrase “Please go to the store and get some milk” one word at a time starting with the word “Please.” We’d start by giving weights of zero to all the later words.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Transformers do not handle variable-length sequences; all sequences have the same length in Transformer models – this is what makes them parallelizable and therefore fast. So to vary the length of a sequence, we just set all the unused parts to some token values such as zeros.</p>
</div>
</div>
<p>To get a value of 0 in the output of a softmax function, we supply <code>-inf</code> (i.e.&nbsp;<span class="math inline">\(-\infty\)</span>) to the logits (i.e.&nbsp;the arguments of softmax), since <span class="math inline">\(\lim_{x\rightarrow-\infty}e^x = 0\)</span>. So to mask out future words in Masked Attention, we supply the value <code>-inf</code> for those words.</p>
<p>The net result of this will be a sequence of logits with progressively fewer <code>-inf</code>’s in them, or accordingly a set of attention wights with progressively fewer 0’s. As shown below (white=very little weight, dark=most weight):</p>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    sums <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(x), axis<span class="op">=-</span><span class="dv">1</span>)[:,np.newaxis]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(x)<span class="op">/</span>sums</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> <span class="st">"Please go to the store and get some milk"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">input</span>.split()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(words)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> np.array([<span class="fl">0.114</span>, <span class="fl">0.205</span>, <span class="fl">0.023</span>, <span class="fl">0.002</span>, <span class="fl">0.160</span>, <span class="fl">0.023</span>, <span class="fl">0.224</span>, <span class="fl">0.023</span>, <span class="fl">0.226</span>])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>logits0 <span class="op">=</span> np.log(attn_weights<span class="op">+</span><span class="fl">1.0e-8</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.full((n,n), <span class="op">-</span>np.inf)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n): </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    logits[i,<span class="dv">0</span>:i<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> logits0[<span class="dv">0</span>:i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> softmax(logits)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"logits:"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(logits, columns<span class="op">=</span>words)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>display(df.style.background_gradient(vmin<span class="op">=-</span><span class="dv">10</span>, vmax<span class="op">=-</span><span class="fl">1.4</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>logits:</code></pre>
</div>
<div id="masked-logits" class="cell-output cell-output-display">
<style type="text/css">
#T_0957d_row0_col0, #T_0957d_row1_col0, #T_0957d_row2_col0, #T_0957d_row3_col0, #T_0957d_row4_col0, #T_0957d_row5_col0, #T_0957d_row6_col0, #T_0957d_row7_col0, #T_0957d_row8_col0 {
  background-color: #034f7d;
  color: #f1f1f1;
}
#T_0957d_row0_col1, #T_0957d_row0_col2, #T_0957d_row0_col3, #T_0957d_row0_col4, #T_0957d_row0_col5, #T_0957d_row0_col6, #T_0957d_row0_col7, #T_0957d_row0_col8, #T_0957d_row1_col2, #T_0957d_row1_col3, #T_0957d_row1_col4, #T_0957d_row1_col5, #T_0957d_row1_col6, #T_0957d_row1_col7, #T_0957d_row1_col8, #T_0957d_row2_col3, #T_0957d_row2_col4, #T_0957d_row2_col5, #T_0957d_row2_col6, #T_0957d_row2_col7, #T_0957d_row2_col8, #T_0957d_row3_col4, #T_0957d_row3_col5, #T_0957d_row3_col6, #T_0957d_row3_col7, #T_0957d_row3_col8, #T_0957d_row4_col5, #T_0957d_row4_col6, #T_0957d_row4_col7, #T_0957d_row4_col8, #T_0957d_row5_col6, #T_0957d_row5_col7, #T_0957d_row5_col8, #T_0957d_row6_col7, #T_0957d_row6_col8, #T_0957d_row7_col8 {
  background-color: #fff7fb;
  color: #000000;
}
#T_0957d_row1_col1, #T_0957d_row2_col1, #T_0957d_row3_col1, #T_0957d_row4_col1, #T_0957d_row5_col1, #T_0957d_row6_col1, #T_0957d_row7_col1, #T_0957d_row8_col1 {
  background-color: #023d60;
  color: #f1f1f1;
}
#T_0957d_row2_col2, #T_0957d_row3_col2, #T_0957d_row4_col2, #T_0957d_row5_col2, #T_0957d_row5_col5, #T_0957d_row6_col2, #T_0957d_row6_col5, #T_0957d_row7_col2, #T_0957d_row7_col5, #T_0957d_row7_col7, #T_0957d_row8_col2, #T_0957d_row8_col5, #T_0957d_row8_col7 {
  background-color: #0f76b3;
  color: #f1f1f1;
}
#T_0957d_row3_col3, #T_0957d_row4_col3, #T_0957d_row5_col3, #T_0957d_row6_col3, #T_0957d_row7_col3, #T_0957d_row8_col3 {
  background-color: #8cb3d5;
  color: #000000;
}
#T_0957d_row4_col4, #T_0957d_row5_col4, #T_0957d_row6_col4, #T_0957d_row7_col4, #T_0957d_row8_col4 {
  background-color: #03456c;
  color: #f1f1f1;
}
#T_0957d_row6_col6, #T_0957d_row7_col6, #T_0957d_row8_col6, #T_0957d_row8_col8 {
  background-color: #023a5b;
  color: #f1f1f1;
}
</style>

<table id="T_0957d" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_0957d_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_0957d_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_0957d_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_0957d_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_0957d_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_0957d_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_0957d_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_0957d_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_0957d_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_0957d_row0_col0" class="data row0 col0">-2.172</td>
<td id="T_0957d_row0_col1" class="data row0 col1">-inf</td>
<td id="T_0957d_row0_col2" class="data row0 col2">-inf</td>
<td id="T_0957d_row0_col3" class="data row0 col3">-inf</td>
<td id="T_0957d_row0_col4" class="data row0 col4">-inf</td>
<td id="T_0957d_row0_col5" class="data row0 col5">-inf</td>
<td id="T_0957d_row0_col6" class="data row0 col6">-inf</td>
<td id="T_0957d_row0_col7" class="data row0 col7">-inf</td>
<td id="T_0957d_row0_col8" class="data row0 col8">-inf</td>
</tr>
<tr class="even">
<td id="T_0957d_row1_col0" class="data row1 col0">-2.172</td>
<td id="T_0957d_row1_col1" class="data row1 col1">-1.585</td>
<td id="T_0957d_row1_col2" class="data row1 col2">-inf</td>
<td id="T_0957d_row1_col3" class="data row1 col3">-inf</td>
<td id="T_0957d_row1_col4" class="data row1 col4">-inf</td>
<td id="T_0957d_row1_col5" class="data row1 col5">-inf</td>
<td id="T_0957d_row1_col6" class="data row1 col6">-inf</td>
<td id="T_0957d_row1_col7" class="data row1 col7">-inf</td>
<td id="T_0957d_row1_col8" class="data row1 col8">-inf</td>
</tr>
<tr class="odd">
<td id="T_0957d_row2_col0" class="data row2 col0">-2.172</td>
<td id="T_0957d_row2_col1" class="data row2 col1">-1.585</td>
<td id="T_0957d_row2_col2" class="data row2 col2">-3.772</td>
<td id="T_0957d_row2_col3" class="data row2 col3">-inf</td>
<td id="T_0957d_row2_col4" class="data row2 col4">-inf</td>
<td id="T_0957d_row2_col5" class="data row2 col5">-inf</td>
<td id="T_0957d_row2_col6" class="data row2 col6">-inf</td>
<td id="T_0957d_row2_col7" class="data row2 col7">-inf</td>
<td id="T_0957d_row2_col8" class="data row2 col8">-inf</td>
</tr>
<tr class="even">
<td id="T_0957d_row3_col0" class="data row3 col0">-2.172</td>
<td id="T_0957d_row3_col1" class="data row3 col1">-1.585</td>
<td id="T_0957d_row3_col2" class="data row3 col2">-3.772</td>
<td id="T_0957d_row3_col3" class="data row3 col3">-6.215</td>
<td id="T_0957d_row3_col4" class="data row3 col4">-inf</td>
<td id="T_0957d_row3_col5" class="data row3 col5">-inf</td>
<td id="T_0957d_row3_col6" class="data row3 col6">-inf</td>
<td id="T_0957d_row3_col7" class="data row3 col7">-inf</td>
<td id="T_0957d_row3_col8" class="data row3 col8">-inf</td>
</tr>
<tr class="odd">
<td id="T_0957d_row4_col0" class="data row4 col0">-2.172</td>
<td id="T_0957d_row4_col1" class="data row4 col1">-1.585</td>
<td id="T_0957d_row4_col2" class="data row4 col2">-3.772</td>
<td id="T_0957d_row4_col3" class="data row4 col3">-6.215</td>
<td id="T_0957d_row4_col4" class="data row4 col4">-1.833</td>
<td id="T_0957d_row4_col5" class="data row4 col5">-inf</td>
<td id="T_0957d_row4_col6" class="data row4 col6">-inf</td>
<td id="T_0957d_row4_col7" class="data row4 col7">-inf</td>
<td id="T_0957d_row4_col8" class="data row4 col8">-inf</td>
</tr>
<tr class="even">
<td id="T_0957d_row5_col0" class="data row5 col0">-2.172</td>
<td id="T_0957d_row5_col1" class="data row5 col1">-1.585</td>
<td id="T_0957d_row5_col2" class="data row5 col2">-3.772</td>
<td id="T_0957d_row5_col3" class="data row5 col3">-6.215</td>
<td id="T_0957d_row5_col4" class="data row5 col4">-1.833</td>
<td id="T_0957d_row5_col5" class="data row5 col5">-3.772</td>
<td id="T_0957d_row5_col6" class="data row5 col6">-inf</td>
<td id="T_0957d_row5_col7" class="data row5 col7">-inf</td>
<td id="T_0957d_row5_col8" class="data row5 col8">-inf</td>
</tr>
<tr class="odd">
<td id="T_0957d_row6_col0" class="data row6 col0">-2.172</td>
<td id="T_0957d_row6_col1" class="data row6 col1">-1.585</td>
<td id="T_0957d_row6_col2" class="data row6 col2">-3.772</td>
<td id="T_0957d_row6_col3" class="data row6 col3">-6.215</td>
<td id="T_0957d_row6_col4" class="data row6 col4">-1.833</td>
<td id="T_0957d_row6_col5" class="data row6 col5">-3.772</td>
<td id="T_0957d_row6_col6" class="data row6 col6">-1.496</td>
<td id="T_0957d_row6_col7" class="data row6 col7">-inf</td>
<td id="T_0957d_row6_col8" class="data row6 col8">-inf</td>
</tr>
<tr class="even">
<td id="T_0957d_row7_col0" class="data row7 col0">-2.172</td>
<td id="T_0957d_row7_col1" class="data row7 col1">-1.585</td>
<td id="T_0957d_row7_col2" class="data row7 col2">-3.772</td>
<td id="T_0957d_row7_col3" class="data row7 col3">-6.215</td>
<td id="T_0957d_row7_col4" class="data row7 col4">-1.833</td>
<td id="T_0957d_row7_col5" class="data row7 col5">-3.772</td>
<td id="T_0957d_row7_col6" class="data row7 col6">-1.496</td>
<td id="T_0957d_row7_col7" class="data row7 col7">-3.772</td>
<td id="T_0957d_row7_col8" class="data row7 col8">-inf</td>
</tr>
<tr class="odd">
<td id="T_0957d_row8_col0" class="data row8 col0">-2.172</td>
<td id="T_0957d_row8_col1" class="data row8 col1">-1.585</td>
<td id="T_0957d_row8_col2" class="data row8 col2">-3.772</td>
<td id="T_0957d_row8_col3" class="data row8 col3">-6.215</td>
<td id="T_0957d_row8_col4" class="data row8 col4">-1.833</td>
<td id="T_0957d_row8_col5" class="data row8 col5">-3.772</td>
<td id="T_0957d_row8_col6" class="data row8 col6">-1.496</td>
<td id="T_0957d_row8_col7" class="data row8 col7">-3.772</td>
<td id="T_0957d_row8_col8" class="data row8 col8">-1.487</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">attention weights:  ( = softmax(logits), row-wise )"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(attention, columns<span class="op">=</span>words)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>display(df.style.background_gradient(vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="fl">0.5</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
attention weights:  ( = softmax(logits), row-wise )</code></pre>
</div>
<div id="masked-attn" class="cell-output cell-output-display">
<style type="text/css">
#T_9fcca_row0_col0, #T_9fcca_row1_col1, #T_9fcca_row2_col1, #T_9fcca_row3_col1 {
  background-color: #023858;
  color: #f1f1f1;
}
#T_9fcca_row0_col1, #T_9fcca_row0_col2, #T_9fcca_row0_col3, #T_9fcca_row0_col4, #T_9fcca_row0_col5, #T_9fcca_row0_col6, #T_9fcca_row0_col7, #T_9fcca_row0_col8, #T_9fcca_row1_col2, #T_9fcca_row1_col3, #T_9fcca_row1_col4, #T_9fcca_row1_col5, #T_9fcca_row1_col6, #T_9fcca_row1_col7, #T_9fcca_row1_col8, #T_9fcca_row2_col3, #T_9fcca_row2_col4, #T_9fcca_row2_col5, #T_9fcca_row2_col6, #T_9fcca_row2_col7, #T_9fcca_row2_col8, #T_9fcca_row3_col4, #T_9fcca_row3_col5, #T_9fcca_row3_col6, #T_9fcca_row3_col7, #T_9fcca_row3_col8, #T_9fcca_row4_col5, #T_9fcca_row4_col6, #T_9fcca_row4_col7, #T_9fcca_row4_col8, #T_9fcca_row5_col6, #T_9fcca_row5_col7, #T_9fcca_row5_col8, #T_9fcca_row6_col7, #T_9fcca_row6_col8, #T_9fcca_row7_col8 {
  background-color: #fff7fb;
  color: #000000;
}
#T_9fcca_row1_col0 {
  background-color: #1379b5;
  color: #f1f1f1;
}
#T_9fcca_row2_col0 {
  background-color: #2685bb;
  color: #f1f1f1;
}
#T_9fcca_row2_col2, #T_9fcca_row3_col2 {
  background-color: #eae6f1;
  color: #000000;
}
#T_9fcca_row3_col0 {
  background-color: #2786bb;
  color: #f1f1f1;
}
#T_9fcca_row3_col3, #T_9fcca_row4_col3 {
  background-color: #fef6fa;
  color: #000000;
}
#T_9fcca_row4_col0, #T_9fcca_row8_col8 {
  background-color: #88b1d4;
  color: #000000;
}
#T_9fcca_row4_col1 {
  background-color: #04649e;
  color: #f1f1f1;
}
#T_9fcca_row4_col2 {
  background-color: #f1ebf5;
  color: #000000;
}
#T_9fcca_row4_col4 {
  background-color: #328dbf;
  color: #f1f1f1;
}
#T_9fcca_row5_col0 {
  background-color: #8fb4d6;
  color: #000000;
}
#T_9fcca_row5_col1 {
  background-color: #056ba7;
  color: #f1f1f1;
}
#T_9fcca_row5_col2, #T_9fcca_row5_col5 {
  background-color: #f2ecf5;
  color: #000000;
}
#T_9fcca_row5_col3, #T_9fcca_row6_col3, #T_9fcca_row7_col3, #T_9fcca_row8_col3 {
  background-color: #fef6fb;
  color: #000000;
}
#T_9fcca_row5_col4 {
  background-color: #3f93c2;
  color: #f1f1f1;
}
#T_9fcca_row6_col0 {
  background-color: #bfc9e1;
  color: #000000;
}
#T_9fcca_row6_col1 {
  background-color: #5ea0ca;
  color: #f1f1f1;
}
#T_9fcca_row6_col2, #T_9fcca_row6_col5, #T_9fcca_row7_col2, #T_9fcca_row7_col5, #T_9fcca_row7_col7 {
  background-color: #f6eff7;
  color: #000000;
}
#T_9fcca_row6_col4 {
  background-color: #91b5d6;
  color: #000000;
}
#T_9fcca_row6_col6 {
  background-color: #4496c3;
  color: #f1f1f1;
}
#T_9fcca_row7_col0 {
  background-color: #c1cae2;
  color: #000000;
}
#T_9fcca_row7_col1 {
  background-color: #65a3cb;
  color: #f1f1f1;
}
#T_9fcca_row7_col4 {
  background-color: #97b7d7;
  color: #000000;
}
#T_9fcca_row7_col6 {
  background-color: #4c99c5;
  color: #f1f1f1;
}
#T_9fcca_row8_col0 {
  background-color: #d5d5e8;
  color: #000000;
}
#T_9fcca_row8_col1 {
  background-color: #99b8d8;
  color: #000000;
}
#T_9fcca_row8_col2, #T_9fcca_row8_col5, #T_9fcca_row8_col7 {
  background-color: #f8f1f8;
  color: #000000;
}
#T_9fcca_row8_col4 {
  background-color: #b9c6e0;
  color: #000000;
}
#T_9fcca_row8_col6 {
  background-color: #89b1d4;
  color: #000000;
}
</style>

<table id="T_9fcca" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_9fcca_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_9fcca_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_9fcca_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_9fcca_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_9fcca_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_9fcca_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_9fcca_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_9fcca_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_9fcca_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_9fcca_row0_col0" class="data row0 col0">1.000</td>
<td id="T_9fcca_row0_col1" class="data row0 col1">0.000</td>
<td id="T_9fcca_row0_col2" class="data row0 col2">0.000</td>
<td id="T_9fcca_row0_col3" class="data row0 col3">0.000</td>
<td id="T_9fcca_row0_col4" class="data row0 col4">0.000</td>
<td id="T_9fcca_row0_col5" class="data row0 col5">0.000</td>
<td id="T_9fcca_row0_col6" class="data row0 col6">0.000</td>
<td id="T_9fcca_row0_col7" class="data row0 col7">0.000</td>
<td id="T_9fcca_row0_col8" class="data row0 col8">0.000</td>
</tr>
<tr class="even">
<td id="T_9fcca_row1_col0" class="data row1 col0">0.357</td>
<td id="T_9fcca_row1_col1" class="data row1 col1">0.643</td>
<td id="T_9fcca_row1_col2" class="data row1 col2">0.000</td>
<td id="T_9fcca_row1_col3" class="data row1 col3">0.000</td>
<td id="T_9fcca_row1_col4" class="data row1 col4">0.000</td>
<td id="T_9fcca_row1_col5" class="data row1 col5">0.000</td>
<td id="T_9fcca_row1_col6" class="data row1 col6">0.000</td>
<td id="T_9fcca_row1_col7" class="data row1 col7">0.000</td>
<td id="T_9fcca_row1_col8" class="data row1 col8">0.000</td>
</tr>
<tr class="odd">
<td id="T_9fcca_row2_col0" class="data row2 col0">0.333</td>
<td id="T_9fcca_row2_col1" class="data row2 col1">0.599</td>
<td id="T_9fcca_row2_col2" class="data row2 col2">0.067</td>
<td id="T_9fcca_row2_col3" class="data row2 col3">0.000</td>
<td id="T_9fcca_row2_col4" class="data row2 col4">0.000</td>
<td id="T_9fcca_row2_col5" class="data row2 col5">0.000</td>
<td id="T_9fcca_row2_col6" class="data row2 col6">0.000</td>
<td id="T_9fcca_row2_col7" class="data row2 col7">0.000</td>
<td id="T_9fcca_row2_col8" class="data row2 col8">0.000</td>
</tr>
<tr class="even">
<td id="T_9fcca_row3_col0" class="data row3 col0">0.331</td>
<td id="T_9fcca_row3_col1" class="data row3 col1">0.596</td>
<td id="T_9fcca_row3_col2" class="data row3 col2">0.067</td>
<td id="T_9fcca_row3_col3" class="data row3 col3">0.006</td>
<td id="T_9fcca_row3_col4" class="data row3 col4">0.000</td>
<td id="T_9fcca_row3_col5" class="data row3 col5">0.000</td>
<td id="T_9fcca_row3_col6" class="data row3 col6">0.000</td>
<td id="T_9fcca_row3_col7" class="data row3 col7">0.000</td>
<td id="T_9fcca_row3_col8" class="data row3 col8">0.000</td>
</tr>
<tr class="odd">
<td id="T_9fcca_row4_col0" class="data row4 col0">0.226</td>
<td id="T_9fcca_row4_col1" class="data row4 col1">0.407</td>
<td id="T_9fcca_row4_col2" class="data row4 col2">0.046</td>
<td id="T_9fcca_row4_col3" class="data row4 col3">0.004</td>
<td id="T_9fcca_row4_col4" class="data row4 col4">0.317</td>
<td id="T_9fcca_row4_col5" class="data row4 col5">0.000</td>
<td id="T_9fcca_row4_col6" class="data row4 col6">0.000</td>
<td id="T_9fcca_row4_col7" class="data row4 col7">0.000</td>
<td id="T_9fcca_row4_col8" class="data row4 col8">0.000</td>
</tr>
<tr class="even">
<td id="T_9fcca_row5_col0" class="data row5 col0">0.216</td>
<td id="T_9fcca_row5_col1" class="data row5 col1">0.389</td>
<td id="T_9fcca_row5_col2" class="data row5 col2">0.044</td>
<td id="T_9fcca_row5_col3" class="data row5 col3">0.004</td>
<td id="T_9fcca_row5_col4" class="data row5 col4">0.304</td>
<td id="T_9fcca_row5_col5" class="data row5 col5">0.044</td>
<td id="T_9fcca_row5_col6" class="data row5 col6">0.000</td>
<td id="T_9fcca_row5_col7" class="data row5 col7">0.000</td>
<td id="T_9fcca_row5_col8" class="data row5 col8">0.000</td>
</tr>
<tr class="odd">
<td id="T_9fcca_row6_col0" class="data row6 col0">0.152</td>
<td id="T_9fcca_row6_col1" class="data row6 col1">0.273</td>
<td id="T_9fcca_row6_col2" class="data row6 col2">0.031</td>
<td id="T_9fcca_row6_col3" class="data row6 col3">0.003</td>
<td id="T_9fcca_row6_col4" class="data row6 col4">0.213</td>
<td id="T_9fcca_row6_col5" class="data row6 col5">0.031</td>
<td id="T_9fcca_row6_col6" class="data row6 col6">0.298</td>
<td id="T_9fcca_row6_col7" class="data row6 col7">0.000</td>
<td id="T_9fcca_row6_col8" class="data row6 col8">0.000</td>
</tr>
<tr class="even">
<td id="T_9fcca_row7_col0" class="data row7 col0">0.147</td>
<td id="T_9fcca_row7_col1" class="data row7 col1">0.265</td>
<td id="T_9fcca_row7_col2" class="data row7 col2">0.030</td>
<td id="T_9fcca_row7_col3" class="data row7 col3">0.003</td>
<td id="T_9fcca_row7_col4" class="data row7 col4">0.207</td>
<td id="T_9fcca_row7_col5" class="data row7 col5">0.030</td>
<td id="T_9fcca_row7_col6" class="data row7 col6">0.289</td>
<td id="T_9fcca_row7_col7" class="data row7 col7">0.030</td>
<td id="T_9fcca_row7_col8" class="data row7 col8">0.000</td>
</tr>
<tr class="odd">
<td id="T_9fcca_row8_col0" class="data row8 col0">0.114</td>
<td id="T_9fcca_row8_col1" class="data row8 col1">0.205</td>
<td id="T_9fcca_row8_col2" class="data row8 col2">0.023</td>
<td id="T_9fcca_row8_col3" class="data row8 col3">0.002</td>
<td id="T_9fcca_row8_col4" class="data row8 col4">0.160</td>
<td id="T_9fcca_row8_col5" class="data row8 col5">0.023</td>
<td id="T_9fcca_row8_col6" class="data row8 col6">0.224</td>
<td id="T_9fcca_row8_col7" class="data row8 col7">0.023</td>
<td id="T_9fcca_row8_col8" class="data row8 col8">0.226</td>
</tr>
</tbody>
</table>
</div>
</div>
If you want to see this in action, I recommend Karpathy’s video <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span> at around <a href="https://youtu.be/kCc8FmEb1nY?t=3368">56 minutes in</a>.
</section></details>
</section>

<section id="sec-nextup" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Summary and Further Topics</h1>
<p>We’ve looked at the main dynamic in Transformer models, namely the Attention process. We now have a grasp of not only what it is and how it works, but why it fulfills its intended purpose well. There’s a lot more we could still cover, but I’m going stop here for now. To fully explain the Transformer model, we’ll want to take a look at Positional Encoding, and the residual connections, and more.</p>
<p>However, now that you’re (hopefully) over the basic “hump” of the Attention mechanism, you may find the rest of the Transformer model to be fairly straightforward. You should be able to read any other guide to Transformer models and they should make <em>a lot</em> more sense now – or wait until I write a follow-up post! For now, check out the <strong>References</strong> in <a href="#sec-references">Section&nbsp;8</a> below for a list of good resources you can use to carry on with your studies.</p>
</section>
<section id="afterward-i-wrote-a-song" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Afterward: I Wrote A Song</h1>
<p>While I was away on a solo writer’s retreat to a shack in the woods, I started fiddling with a little song idea inspired by this tweet:</p>
<div class="cell">
<div class="cell-output cell-output-display">

<blockquote class="twitter-tweet blockquote"><p lang="en" dir="ltr">"Traumatized Transformers": In which certain words are intrinsically weighted more that others.<br>(alternately, "Annoying Transformers Who Won't Shut Up About Their Favorite Topics")</p>— Horrific ML Ideas (@ml_ideas) <a href="https://twitter.com/ml_ideas/status/1683716504821325824?ref_src=twsrc%5Etfw">July 25, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>
<p>Here’s a video of a rough take I sent to friends:</p>
<div class="cell">
<div class="cell-output cell-output-display">

<iframe width="560" height="315" src="https://www.youtube.com/embed/W65eENFUM_0?start=28" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>
</div>
</section>
<section id="sec-references" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> References</h1>
<p>I recommend any of the following. Seriously, I studied all of these and they’re great. Things just didn’t “click” for me until later in life I guess. LOL.</p>
<p>My trajectory was first the original paper<span class="citation" data-cites="aiayn"><a href="#ref-aiayn" role="doc-biblioref">[1]</a></span>, then Jay Alammar’s “The Illustrated Transformer” <span class="citation" data-cites="alammar_illustrated"><a href="#ref-alammar_illustrated" role="doc-biblioref">[6]</a></span>, then the Coursera course <span class="citation" data-cites="coursera"><a href="#ref-coursera" role="doc-biblioref">[7]</a></span> co-taught by one of the original Transformer paper authors, then Brendan Rohrer’s great post <span class="citation" data-cites="rohrer"><a href="#ref-rohrer" role="doc-biblioref">[8]</a></span>, then Jeremy Jordan’s tutorial <span class="citation" data-cites="jeremyj_transformers"><a href="#ref-jeremyj_transformers" role="doc-biblioref">[9]</a></span>. Then a couple years later Karpathy’s video <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span>, with a follow-up of CodeEmporiums’s video <span class="citation" data-cites="codeemporium"><a href="#ref-codeemporium" role="doc-biblioref">[10]</a></span>. But hopefully after reading my post(s) you won’t need to do all that!</p>
<p>I also recommend Daniel Dugas’ “GPT on a Napkin” <span class="citation" data-cites="dugas_gpt_napkin"><a href="#ref-dugas_gpt_napkin" role="doc-biblioref">[11]</a></span> and Sebastian Rashka’s post on Attention <span class="citation" data-cites="raschka_understanding_attn"><a href="#ref-raschka_understanding_attn" role="doc-biblioref">[12]</a></span>, but didn’t learn of these until after I started writing this post.</p>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-aiayn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Vaswani <em>et al.</em>, <span>“Attention is <span>All</span> you <span>Need</span>,”</span> in <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a></div>
</div>
<div id="ref-andrej_karpathy_lets_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. Karpathy, <span>“Let’s build <span>GPT</span>: From scratch, in code, spelled out.”</span> Jan. 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">https://www.youtube.com/watch?v=kCc8FmEb1nY</a></div>
</div>
<div id="ref-vit_paper" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">A. Dosovitskiy <em>et al.</em>, <span>“An <span>Image</span> is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image</span> <span>Recognition</span> at <span>Scale</span>,”</span> Oct. 2020. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a></div>
</div>
<div id="ref-bahdanau_attn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">D. Bahdanau, K. Cho, and Y. Bengio, <span>“Neural machine translation by jointly learning to align and translate,”</span> in <em>3rd international conference on learning representations, <span>ICLR</span> 2015, san diego, CA, USA, may 7-9, 2015, conference track proceedings</em>, Y. Bengio and Y. LeCun, Eds., 2015. Available: <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a></div>
</div>
<div id="ref-blog-vec-orthog" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">S. H. Hawley, <span>“Likelihood of vector orthogonality in high-dimensional spaces.”</span> Jan. 2022. Accessed: Aug. 18, 2023. [Online]. Available: <a href="https://drscotthawley.github.io/blog/posts/2022-01-24-multidim-dotproducts.html">https://drscotthawley.github.io/blog/posts/2022-01-24-multidim-dotproducts.html</a></div>
</div>
<div id="ref-alammar_illustrated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">J. Alammar, <span>“The <span>Illustrated</span> <span>Transformer</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></div>
</div>
<div id="ref-coursera" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Y. B. Mourri and Ł. Kaiser, <span>“Natural <span>Language</span> <span>Processing</span> with <span>Attention</span> <span>Models</span>,”</span> <em>Coursera</em>. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.coursera.org/learn/attention-models-in-nlp">https://www.coursera.org/learn/attention-models-in-nlp</a></div>
</div>
<div id="ref-rohrer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">B. Rohrer, <span>“Transformers from <span>Scratch</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://e2eml.school/transformers.html">https://e2eml.school/transformers.html</a></div>
</div>
<div id="ref-jeremyj_transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">J. Jordan, <span>“Understanding the <span>Transformer</span> architecture for neural networks,”</span> <em>Jeremy Jordan</em>. May 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.jeremyjordan.me/transformer-architecture/">https://www.jeremyjordan.me/transformer-architecture/</a></div>
</div>
<div id="ref-codeemporium" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">CodeEmporium, <span>“Transformer <span>Decoder</span> coded from scratch.”</span> Mar. 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.youtube.com/watch?v=MqDehUoMk-E">https://www.youtube.com/watch?v=MqDehUoMk-E</a></div>
</div>
<div id="ref-dugas_gpt_napkin" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">D. Dugas, <span>“The <span>GPT</span>-3 <span>Architecture</span>, on a <span>Napkin</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">https://dugas.ch/artificial_curiosity/GPT_architecture.html</a></div>
</div>
<div id="ref-raschka_understanding_attn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">S. Raschka, <span>“Understanding and <span>Coding</span> the <span>Self</span>-<span>Attention</span> <span>Mechanism</span> of <span>Large</span> <span>Language</span> <span>Models</span> <span>From</span> <span>Scratch</span>,”</span> <em>Sebastian Raschka, PhD</em>. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></div>
</div>
</div>
</section>
<section id="sec-softmax-fun" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Appendix: Fun with Softmax - Interactive!</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section is optional.</p>
</div>
</div>
<p>By rescaling the inputs to the softmax function (i.e., the “logits”), we can change the “sharpness” of the result. Try playing with the slider below to rescale the sample weights from our introductory sentence!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" data-startfrom="2" data-source-offset="-1"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1;"><span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> {plotter} <span class="im">from</span> <span class="st">"@kjerandp/plotter"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">// This is written in Observable JS, cf. https://quarto.org/docs/interactive/ojs/</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">apply_s</span>(logits<span class="op">,</span> s) {</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits<span class="op">.</span><span class="fu">map</span>(<span class="kw">function</span>(x) { <span class="cf">return</span> x <span class="op">*</span> (s<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>s<span class="op">+</span><span class="fl">1.0e-8</span>))<span class="op">;</span> })<span class="op">;</span>  </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">softmax</span>(logits) {</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> maxLogit <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">max</span>(<span class="op">...</span>logits)<span class="op">;</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> scores <span class="op">=</span> logits<span class="op">.</span><span class="fu">map</span>(l <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(l <span class="op">-</span> maxLogit))<span class="op">;</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> denom <span class="op">=</span> scores<span class="op">.</span><span class="fu">reduce</span>((a<span class="op">,</span> b) <span class="kw">=&gt;</span> a <span class="op">+</span> b)<span class="op">;</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores<span class="op">.</span><span class="fu">map</span>(s <span class="kw">=&gt;</span> s <span class="op">/</span> denom)<span class="op">;</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">'Please'</span><span class="op">,</span><span class="st">'go'</span><span class="op">,</span><span class="st">'to'</span><span class="op">,</span><span class="st">'the'</span><span class="op">,</span><span class="st">'store'</span><span class="op">,</span><span class="st">'and'</span><span class="op">,</span><span class="st">'get'</span><span class="op">,</span><span class="st">'some'</span><span class="op">,</span><span class="st">'milk'</span>]<span class="op">;</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> words<span class="op">.</span><span class="at">length</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> [<span class="fl">0.114</span><span class="op">,</span> <span class="fl">0.205</span><span class="op">,</span> <span class="fl">0.023</span><span class="op">,</span> <span class="fl">0.002</span><span class="op">,</span> <span class="fl">0.160</span><span class="op">,</span> <span class="fl">0.023</span><span class="op">,</span> <span class="fl">0.224</span><span class="op">,</span> <span class="fl">0.023</span><span class="op">,</span> <span class="fl">0.226</span>]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> attn_weights<span class="op">.</span><span class="fu">map</span>(x <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">log</span>(x<span class="op">+</span><span class="fl">1.0e-8</span>))<span class="op">;</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>viewof s <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">label</span><span class="op">:</span> <span class="st">"Sharpness S:"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="op">.</span><span class="bn">01</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span><span class="fl">0.5</span><span class="op">,</span> <span class="dt">width</span><span class="op">:</span><span class="dv">460</span>})<span class="op">;</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>numbers <span class="op">=</span> <span class="fu">softmax</span>(<span class="fu">apply_s</span>(logits<span class="op">,</span>s))<span class="op">;</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co">// make a bar plot manually by plotting a bunch of rectangles. </span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>xlocs <span class="op">=</span> [<span class="dv">0</span><span class="op">,</span><span class="dv">1</span><span class="op">,</span><span class="dv">2</span><span class="op">,</span><span class="dv">3</span><span class="op">,</span><span class="dv">4</span><span class="op">,</span><span class="dv">5</span><span class="op">,</span><span class="dv">6</span><span class="op">,</span><span class="dv">7</span><span class="op">,</span><span class="dv">8</span>]<span class="op">;</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>basic_plot <span class="op">=</span> {</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> p <span class="op">=</span> <span class="fu">plotter</span>( {<span class="dt">height</span><span class="op">:</span><span class="dv">350</span><span class="op">,</span> <span class="dt">width</span><span class="op">:</span><span class="dv">600</span><span class="op">,</span> <span class="dt">xDomain</span><span class="op">:</span> [xlocs[<span class="dv">0</span>]<span class="op">-</span><span class="fl">0.1</span><span class="op">,</span> xlocs[n<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>]<span class="op">,</span> <span class="dt">yDomain</span><span class="op">:</span> [<span class="dv">1</span><span class="op">,</span> <span class="op">-.</span><span class="dv">1</span>]<span class="op">,</span> <span class="dt">xLabel</span><span class="op">:</span><span class="st">'x'</span><span class="op">,</span> <span class="dt">yLabel</span><span class="op">:</span><span class="st">'softmax(xS)'</span> } )<span class="op">;</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(<span class="st">'softmax(xS/(1-S))'</span><span class="op">,</span> <span class="op">-</span><span class="fl">0.4</span><span class="op">,</span> <span class="fl">1.02</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'white'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span><span class="op">,</span> <span class="dt">anchor</span><span class="op">:</span> <span class="st">'start'</span> })<span class="op">;</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(<span class="st">'x'</span><span class="op">,</span> xlocs[n<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="fl">1.15</span><span class="op">,</span> <span class="op">-.</span><span class="bn">02</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'white'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> })<span class="op">;</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">rectangle</span>(<span class="op">-</span><span class="dv">10</span><span class="op">,</span><span class="dv">10</span><span class="op">,</span><span class="dv">10</span><span class="op">,-</span><span class="dv">10</span><span class="op">,</span> {<span class="dt">fill</span><span class="op">:</span><span class="st">'white'</span><span class="op">,</span> <span class="dt">opacity</span><span class="op">:</span><span class="fl">0.0</span>})<span class="op">;</span>  <span class="co">// background</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">rectangle</span>(xlocs[i]<span class="op">,</span> numbers[i]<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="dv">1</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> { <span class="dt">fill</span><span class="op">:</span> <span class="st">'cyan'</span><span class="op">,</span> <span class="dt">opacity</span><span class="op">:</span><span class="fl">0.65</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span><span class="st">'gray'</span> })<span class="op">;</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">text</span>(words[i]<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> <span class="op">-.</span><span class="dv">1</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'yellow'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">text</span>(numbers[i]<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">2</span>)<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> numbers[i]<span class="op">+.</span><span class="bn">01</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'grey'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> p<span class="op">.</span><span class="at">node</span><span class="op">;</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-10" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-11" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<p>For a “sharpness” parameter <span class="math inline">\(S=0.5\)</span>, we have normal softmax. Whereas for <span class="math inline">\(S=0\)</span> we turn softmax into an <em>ordinary average</em>, and for <span class="math inline">\(S=1\)</span> we return <em>only the maximum</em> input. (For those in the know, this is like a thermodynamic system with temperature <span class="math inline">\(\,T = (1-S)/S\)</span>.) So softmax is a pretty versatile tool!<span class="math inline">\(^*\)</span></p>
<p><span class="math inline">\(^*\)</span><small>Oooo and if you really want to dive deep, look up “Gumbel-Softmax Trick.” But that’s for another day.</small></p>
<hr>
<p>(c) 2023 Scott H. Hawley</p>


</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"\nimport {plotter} from \"@kjerandp/plotter\"\n// This is written in Observable JS, cf. https://quarto.org/docs/interactive/ojs/\n\nfunction apply_s(logits, s) {\n    return logits.map(function(x) { return x * (s/(1-s+1.0e-8)); });  \n}\n\nfunction softmax(logits) {\n    const maxLogit = Math.max(...logits);\n    const scores = logits.map(l => Math.exp(l - maxLogit));\n    const denom = scores.reduce((a, b) => a + b);\n    return scores.map(s => s / denom);\n}\n\n\nwords = ['Please','go','to','the','store','and','get','some','milk'];\nn = words.length\nattn_weights = [0.114, 0.205, 0.023, 0.002, 0.160, 0.023, 0.224, 0.023, 0.226]\nlogits = attn_weights.map(x => Math.log(x+1.0e-8));\n\nviewof s = Inputs.range([0, 1], {label: \"Sharpness S:\", step: .01, value:0.5, width:460});\nnumbers = softmax(apply_s(logits,s));\n\n// make a bar plot manually by plotting a bunch of rectangles. \nxlocs = [0,1,2,3,4,5,6,7,8];\nbasic_plot = {\n  let p = plotter( {height:350, width:600, xDomain: [xlocs[0]-0.1, xlocs[n-1]+1], yDomain: [1, -.1], xLabel:'x', yLabel:'softmax(xS)' } );\n  p.text('softmax(xS/(1-S))', -0.4, 1.02, { stroke: 'white', weight: 6, anchor: 'start' });\n  p.text('x', xlocs[n-1]+1.15, -.02, { stroke: 'white', weight: 6 });\n  p.rectangle(-10,10,10,-10, {fill:'white', opacity:0.0});  // background\n\n  for (let i = 0; i < n; i++) {\n      p.rectangle(xlocs[i], numbers[i], xlocs[i]+1, 0, { fill: 'cyan', opacity:0.65, stroke:'gray' });\n      p.text(words[i], xlocs[i]+0.5, -.1, { stroke: 'yellow', weight: 6 } )\n      p.text(numbers[i].toFixed(2), xlocs[i]+0.5, numbers[i]+.01, { stroke: 'grey', weight: 6 } )\n  }\n  return p.node;\n}\n"},{"methodName":"interpretQuiet","source":"shinyInput('s')"}]}
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../posts";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>