<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-01-30">
<meta name="description" content="First in a series on understanding neural network models.">

<title>blog - My First Neural Network, Part 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - My First Neural Network, Part 1">
<meta name="twitter:description" content="First in a series on understanding neural network models.">
<meta name="twitter:image" content="https://i.imgur.com/7Oj5Ksn.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">My First Neural Network, Part 1</h1>
                  <div>
        <div class="description">
          First in a series on understanding neural network models.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">foundations</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 30, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-sample-problem" id="toc-the-sample-problem" class="nav-link active" data-scroll-target="#the-sample-problem">The Sample Problem</a></li>
  <li><a href="#actual-code" id="toc-actual-code" class="nav-link" data-scroll-target="#actual-code">Actual Code</a></li>
  <li><a href="#change-the-activation-function" id="toc-change-the-activation-function" class="nav-link" data-scroll-target="#change-the-activation-function">Change the activation function</a></li>
  <li><a href="#exercise-read-a-7-segment-display" id="toc-exercise-read-a-7-segment-display" class="nav-link" data-scroll-target="#exercise-read-a-7-segment-display">Exercise: Read a 7-segment display</a>
  <ul class="collapse">
  <li><a href="#diagram-of-the-network" id="toc-diagram-of-the-network" class="nav-link" data-scroll-target="#diagram-of-the-network">Diagram of the network</a></li>
  <li><a href="#create-the-dataset" id="toc-create-the-dataset" class="nav-link" data-scroll-target="#create-the-dataset">Create the dataset</a></li>
  <li><a href="#initialize-the-weights" id="toc-initialize-the-weights" class="nav-link" data-scroll-target="#initialize-the-weights">Initialize the weights</a></li>
  <li><a href="#train-the-network" id="toc-train-the-network" class="nav-link" data-scroll-target="#train-the-network">Train the network</a></li>
  </ul></li>
  <li><a href="#final-check-keras-version" id="toc-final-check-keras-version" class="nav-link" data-scroll-target="#final-check-keras-version">Final Check: Keras version</a></li>
  <li><a href="#follow-up-remarks" id="toc-follow-up-remarks" class="nav-link" data-scroll-target="#follow-up-remarks">Follow-up: Remarks</a>
  <ul class="collapse">
  <li><a href="#re-stating-what-we-just-did" id="toc-re-stating-what-we-just-did" class="nav-link" data-scroll-target="#re-stating-what-we-just-did">Re-stating what we just did</a></li>
  <li><a href="#one-thing-we-glossed-over-batch-size" id="toc-one-thing-we-glossed-over-batch-size" class="nav-link" data-scroll-target="#one-thing-we-glossed-over-batch-size">One thing we glossed over: “batch size”</a></li>
  <li><a href="#optional-if-you-want-to-go-really-crazy" id="toc-optional-if-you-want-to-go-really-crazy" class="nav-link" data-scroll-target="#optional-if-you-want-to-go-really-crazy">Optional: If you want to go really crazy</a></li>
  <li><a href="#additional-optional-exercise-binary-math-vs.-one-hot-encoding" id="toc-additional-optional-exercise-binary-math-vs.-one-hot-encoding" class="nav-link" data-scroll-target="#additional-optional-exercise-binary-math-vs.-one-hot-encoding">Additional Optional Exercise: Binary Math vs.&nbsp;One-Hot Encoding</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Links to lessons: <a href="https://drscotthawley.github.io/blog/2017/02/23/Following-Gravity-Colab.html">Part 0</a>, <a href="https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html">Part 1</a>, <a href="https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html">Part 2</a>, <a href="https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html">Part 3</a></p>
<p>We will be reproducing work from Andrew Trask’s excellent tutorial <a href="https://iamtrask.github.io/2015/07/12/basic-python-network/">“A Neural Network in 11 lines of Python”</a>, albeit with a different emphasis, and in a different way. You may regard this treatment and his original treatment as complimentary, and feel free to refer to both. This lesson is written with the intent of building on the lesson about linear regression – which we might call “Part 0” – at the link <a href="https://drscotthawley.github.io/blog/Following-Gravity/">“Following Gravity - ML Foundations Part Ia.”</a></p>
<section id="the-sample-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-sample-problem">The Sample Problem</h2>
<p>Consider a system that tries to map groups of 3 inputs to some corresponding output which is a single number. In the following picture, we’ll show each set of 3 inputs as a row of a matrix <span class="math inline">\(X\)</span>, and each output as the corresponding row of <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[ \overbrace{
\left[ {\begin{array}{ccc}
   0 &amp; 0 &amp; 1 \\
   0 &amp; 1 &amp; 1\\
   1 &amp; 0 &amp; 1\\
   1 &amp; 1 &amp; 1\\
  \end{array} } \right]
}^{X} \rightarrow
\overbrace{
\left[ {\begin{array}{c}
   0   \\
   0  \\
   1  \\
   1 \\
  \end{array} } \right]
  }^Y.
\]</span></p>
<p>Even though this system has an exact solution (namely, <span class="math inline">\(Y\)</span> equals the first column of <span class="math inline">\(X\)</span>), usually we’ll need to be satisfied with a system that maps our inputs <span class="math inline">\(X\)</span> to some approximate “prediction” <span class="math inline">\(\tilde{Y}\)</span>, which we hope to bring closer to the “target” <span class="math inline">\(Y\)</span> by means of successive improvements.</p>
<p>The way we’ll get our prediction <span class="math inline">\(\tilde{Y}\)</span> is by means of a weighted sum of each set of 3 inputs, and some nonlinear function <span class="math inline">\(f\)</span> which we call the “<a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a>” (or just “activation”). Pictorially, the process looks like the following, for each row <span class="math inline">\(i\)</span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, (where the columns of <span class="math inline">\(X\)</span> are shown arranged vertically instead of horizonally):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.imgur.com/7Oj5Ksn.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image of NN</figcaption>
</figure>
</div>
<p>In terms of matrix multiplication, since X is a 4x3 matrix, and Y is a 4x1 matrix, that implies that our weights should be a 3x1 matrix consisting of (unknown) values <span class="math inline">\(w_0\)</span>, <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>. The calculation can be written as:</p>
<p><span class="math display">\[
f\left(
  \overbrace{
\left[ {\begin{array}{ccc}
   0 &amp; 0 &amp; 1 \\
   0 &amp; 1 &amp; 1\\
   1 &amp; 0 &amp; 1\\
   1 &amp; 1 &amp; 1\\
  \end{array} } \right]
}^\text{X}
\overbrace{
   \left[ {\begin{array}{c}
   w_0  \\
    w_1\\
   w_2\\
  \end{array} } \right]
}^{w}
\right)
  =
  \overbrace{
\left[ {\begin{array}{c}
   0   \\
   0  \\
   1  \\
   1 \\
  \end{array} } \right]
}^{\tilde{Y}}
\]</span></p>
<p>Our nonlinear activation function <span class="math inline">\(f\)</span> is taken to operate on each row element one at a time, and we’ll let <span class="math inline">\(f_i\)</span> denote the <span class="math inline">\(i\)</span>th row of this completed activation, i.e.:</p>
<p><span class="math display">\[
f_i = f\left( \sum_j X_{ij}w_j \right) = \tilde{Y}_i .
\]</span></p>
<p>The particular activation function we will use is the “sigmoid”,</p>
<p><span class="math display">\[
f(x) = {1\over{1+e^{-x}}},
\]</span> – [click here to see a plot of this function](https://www.google.com/search?q=plot+1%2F(1%2Bexp(-x) – which has the derivative</p>
<p><span class="math display">\[
{df\over dx} = {e^{-x}\over(1 + e^{-x})^2}
\]</span> which can be shown (<em>Hint: exercise for “mathy” students!</em>) to simplify to <span class="math display">\[
{df\over dx}= f(1-f).
\]</span></p>
<p>The overall problem then amounts to finding the values of the “weights” <span class="math inline">\(w_0, w_1,\)</span> and <span class="math inline">\(w_2\)</span> so that the <span class="math inline">\(\tilde{Y}\)</span> we calculate is as close to the target <span class="math inline">\(Y\)</span> as possible.</p>
<p>To do this, we will seek to minimize a loss function defined as a sum across all data points we have, i.e.&nbsp;all 4 rows. The loss function <span class="math inline">\(L\)</span> we will choose is the mean square error loss, or MSE (note: later in <a href="https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html">Part 2</a> we will use a ‘better’ loss function for this problem):</p>
<p><span class="math display">\[
L = {1\over N}\sum_{i=0}^{N-1} \left[ \tilde{Y}_i - Y_i\right]^2,
\]</span> or in terms of the activation function <span class="math display">\[
L = {1\over N}\sum_{i=0}^{N-1} \left[ f_i - Y_i\right]^2.
\]</span></p>
<p>Each of the weights <span class="math inline">\(w_j\)</span> (<span class="math inline">\(j=0..2\)</span>) will start with random values, and then be updated via gradient descent, i.e.&nbsp;</p>
<p><span class="math display">\[
w_j^{new} = w_j^{old} - \alpha{\partial L\over \partial w_j}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the <em>learning rate</em>, chosen to be some small parameter. For the MSE loss shown above, the partial derivatives with respect to each of the weights is</p>
<p><span class="math display">\[
{\partial L\over \partial w_j} = {2\over N}\sum_{i=0}^{N-1} \left[ \tilde{Y}_i - Y_i\right]{\partial f_i \over \partial w_j}\\
= {2\over N}\sum_{i=0}^{N-1} \left[ \tilde{Y}_i - Y_i\right]f_i(1-f_i)X_{ij}.
\]</span></p>
<p>Absorbing the factor of 2/N into our choice of <span class="math inline">\(\alpha\)</span>, and writing the summation as a dot product, and noting that <span class="math inline">\(f_i = \tilde{Y}_i\)</span>, we can write the update for all the weights together as</p>
<p><span class="math display">\[
w = w - \alpha  X^T \cdot \left( [\tilde{Y}-Y]*\tilde{Y}*(1-\tilde{Y})\right)
\]</span> where the <span class="math inline">\(\cdot\)</span> denotes a matrix-matrix product (i.e.&nbsp;a dot product for successive rows of <span class="math inline">\(X^T\)</span>) and <span class="math inline">\(*\)</span> denotes elementwise multiplication.</p>
<p>To clarify the above expression in terms of matrix dimensions, we can see that <span class="math inline">\(w\)</span>, a 3x1 matrix, can be made by multipyting <span class="math inline">\(X^T\)</span> (a 3 x4 matrix) with the term in parentheses, i.e.&nbsp;the product of elementwise terms involving <span class="math inline">\(\tilde{Y}\)</span>, which is a 4x1 matrix. In other words, a 3x4 matrix, times a 4x1 matrix, yields a 3x1 matrix.</p>
</section>
<section id="actual-code" class="level2">
<h2 class="anchored" data-anchor-id="actual-code">Actual Code</h2>
<p>The full code for all of this is then…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: Slightly modified from Andrew Trask's code posted at </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#         https://iamtrask.github.io/2015/07/12/basic-python-network/</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid activation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x,deriv<span class="op">=</span><span class="va">False</span>): </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(deriv<span class="op">==</span><span class="va">True</span>):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># input dataset</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([  [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>] ])</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># target output dataset            </span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]]).T</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># seed random numbers to make calculation</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># deterministic (just a good practice)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights randomly with mean 0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((<span class="dv">3</span>,<span class="dv">1</span>)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">1.0</span>   <span class="co"># learning rate</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>loss_history <span class="op">=</span> []    <span class="co"># keep a record of how the loss proceeded, blank for now</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward propagation</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> sigmoid(np.dot(X,w)) <span class="co"># prediction, i.e. tilde{Y}</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># how much did we miss?</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> Y_pred <span class="op">-</span> Y</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    loss_history.append((diff<span class="op">**</span><span class="dv">2</span>).mean())   <span class="co"># add to the history of the loss</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update weights</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> alpha <span class="op">*</span> np.dot( X.T, diff<span class="op">*</span>sigmoid(Y_pred, deriv<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output After Training:"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Y_pred = (should be two 0's followed by two 1's)</span><span class="ch">\n</span><span class="st">"</span>,Y_pred)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights =</span><span class="ch">\n</span><span class="st">"</span>,w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output After Training:
Y_pred = (should be two 0's followed by two 1's)
 [[0.03178421]
 [0.02576499]
 [0.97906682]
 [0.97414645]]
weights =
 [[ 7.26283009]
 [-0.21614618]
 [-3.41703015]]</code></pre>
</div>
</div>
<p>Note that, because of our nonlinear activation, we <em>don’t</em> get the solution <span class="math inline">\(w_0=1, w_1=0, w_2=0\)</span>.</p>
<p>Plotting the loss vs.&nbsp;iteration number, we see…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.loglog(loss_history)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2019-01-30-My-First-Neural-Network_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="change-the-activation-function" class="level2">
<h2 class="anchored" data-anchor-id="change-the-activation-function">Change the activation function</h2>
<p>Another popular choice of activation function is the <em>rectified linear unit</em> or ReLU. The function ReLU(x) is zero for x &lt;= 0, and equal to x (i.e.&nbsp;a straight line at 45 degrees for) x &gt;0. It can be written as max(x,0) or x * (x&gt;0), and its derivative is 1 for positive x, and zero otherwise.</p>
<p><a href="https://www.desmos.com/calculator/vjqfxlgmzl">Click here to see a graph of ReLU</a></p>
<p>Modifying our earlier code to use ReLU activation instead of sigmoid looks like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x,deriv<span class="op">=</span><span class="va">False</span>):   <span class="co"># relu activation</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(deriv<span class="op">==</span><span class="va">True</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>) </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x<span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># seed random numbers to make calculation</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># deterministic (just a good practice)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights randomly  (but only &gt;0 because ReLU clips otherwise)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.random((<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.3</span>   <span class="co"># learning rate</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>new_loss_history <span class="op">=</span> []    <span class="co"># keep a record of how the error proceeded</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward propagation</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> relu(np.dot(X,w))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># how much did we miss?</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> Y_pred <span class="op">-</span> Y</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    new_loss_history.append((diff<span class="op">**</span><span class="dv">2</span>).mean())   <span class="co"># add to the record of the loss</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update weights</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> alpha <span class="op">*</span> np.dot( X.T, diff<span class="op">*</span>relu(Y_pred, deriv<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output After Training:"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Y_pred = (should be two 0's followed by two 1's)</span><span class="ch">\n</span><span class="st">"</span>,Y_pred)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights =</span><span class="ch">\n</span><span class="st">"</span>,w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output After Training:
Y_pred = (should be two 0's followed by two 1's)
 [[-0.]
 [-0.]
 [ 1.]
 [ 1.]]
weights =
 [[ 1.01784368e+00]
 [ 8.53961786e-17]
 [-1.78436793e-02]]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Aside/Trivia (you can skip this cell): I find it interesting that apparently w2 = 1-w0</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( w[<span class="dv">2</span>] <span class="op">-</span> (<span class="dv">1</span><span class="op">-</span>w[<span class="dv">0</span>]) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-3.46944695e-17]</code></pre>
</div>
</div>
<p>Plot old results with new results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.loglog(loss_history,label<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.loglog(new_loss_history,label<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2019-01-30-My-First-Neural-Network_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Looks like ReLU may be a better choice than sigmoid for this problem!</p>
</section>
<section id="exercise-read-a-7-segment-display" class="level2">
<h2 class="anchored" data-anchor-id="exercise-read-a-7-segment-display">Exercise: Read a 7-segment display</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Seven-segment_display">7-segment display</a> is used for displaying numerical digits 0 through 9, usually by lighting up LEDs or parts of a liquid crystal display (LCD). The segments are labelled <span class="math inline">\(a\)</span> through <span class="math inline">\(g\)</span> according to the following diagram:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.imgur.com/ZyHGDKy.png%20=100x150" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">7-segment diagram</figcaption>
</figure>
</div>
<section id="diagram-of-the-network" class="level3">
<h3 class="anchored" data-anchor-id="diagram-of-the-network">Diagram of the network</h3>
<p>The 7 inputs “a” through “g” will be mapped to 10 outputs for the individual digits, and each output can range from 0 (“false” or “no”) to 1 (“true” or “yes”) for that digit. The input and outputs will be connected by a matrix of weights. Pictorially, this looks like the following (Not shown: activation function <span class="math inline">\(f\)</span>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.imgur.com/mERzmFE.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">diagram of 7-seg network</figcaption>
</figure>
</div>
<p>…where again, this network operates on a single data point at a time, datapoints which are <em>rows</em> of <em>X</em> and <em>Y</em>. What is shown in the above diagram are the <em>columns</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for a single row (/ single data point).</p>
</section>
<section id="create-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="create-the-dataset">Create the dataset</h3>
<p>Let the input X be the segments <span class="math inline">\(a\)</span> through <span class="math inline">\(g\)</span> are the columns of the input <span class="math inline">\(X\)</span>, and are either 1 for on or 0 for off. Let the columns of the target <span class="math inline">\(Y\)</span> be the digits 0-9 themselves arranged in a <a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/">“one hot” encoding</a> scheme, as follows:</p>

<table>
<tbody><tr>
<td>
Digit
</td>
<td>
One-Hot Encoding for <span class="math inline">\(Y\)</span>
</td>
</tr>
<tr>
<td>
0
</td>
<td>
1,0,0,0,0,0,0,0,0,0
</td>
</tr>
<tr>
<td>
1
</td>
<td>
0,1,0,0,0,0,0,0,0,0
</td>
</tr>
<tr>
<td>
2
</td>
<td>
0,0,1,0,0,0,0,0,0,0
</td>
</tr>
<tr>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<td>
9
</td>
<td>
0,0,0,0,0,0,0,0,0,1
</td>
</tr>

</tbody></table>
<p>The values in the columns for <span class="math inline">\(Y\)</span> are essentially true/false “bits” for each digit, answering the question “Is this digit the appropriate output?” with a “yes”(=1) or “no” (=0) response.</p>
<p>The input <span class="math inline">\(X\)</span> will be a 10x7 matrix, and the target <span class="math inline">\(Y\)</span> will be a 10x10 matrix. Each row of <span class="math inline">\(X\)</span> will be the segments to produce the digit for that row. For example, the zeroth row of <span class="math inline">\(X\)</span> should show segments on which make an image of the digit zero, namely segments a, b, c, d, e, and f but not g, so that the zeroth row of X should be [1,1,1,1,1,1,0].</p>
<p>Define numpy arrays for both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (Hint: for <span class="math inline">\(Y\)</span>, check out <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html">np.eye()</a>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Students: fill these out completely for what the X and Y *should* be</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for the 7-segment display.  The following is just a "stub" to get you started.</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>               [],</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>               [],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>               [] ])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>               [],</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>               [] ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="initialize-the-weights" class="level3">
<h3 class="anchored" data-anchor-id="initialize-the-weights">Initialize the weights</h3>
<p>Previously the dimensions of the weight matrix <span class="math inline">\(w\)</span> were 3x1 because we were mapping each row of 3 elements in <span class="math inline">\(X\)</span> to each row of 1 element of <span class="math inline">\(Y\)</span>. For this new problem, each row of <span class="math inline">\(X\)</span> has 7 elements, and we want to map those to the 10 elements in each 1-hot-encoded row of <span class="math inline">\(Y\)</span>, so what should the dimensions of the weights matrix <span class="math inline">\(w\)</span> be?</p>
<p>Write some numpy code to randomly initialize the weights matrix:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)  <span class="co"># initial RNG so everybody gets similar results</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.random(( , ))    <span class="co"># Students, fill in the array dimensions here</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>SyntaxError: ignored</code></pre>
</div>
</div>
</section>
<section id="train-the-network" class="level3">
<h3 class="anchored" data-anchor-id="train-the-network">Train the network</h3>
<p>Having created an <span class="math inline">\(X\)</span> and its matching <span class="math inline">\(Y\)</span>, and initalized the weights <span class="math inline">\(w\)</span> randomly, train a neural network such as the ones above to learn to map a row of X to a row of Y, i.e.&nbsp;train it to recognize digits on 7-segment displays. Do this below.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Students, copy training code from above and paste it here. </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  Use sigmoid activation, and 1000 iterations, and learning rate of 0.9</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    Question: What happens if you use ReLU instead? Try it later. Is ReLU always the best choice?</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># And then print out your Y_pred &amp; weights matrix, and limit it to 3 significant digits</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output After Training:"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(formatter<span class="op">=</span>{<span class="st">'float'</span>: <span class="kw">lambda</span> x: <span class="st">"</span><span class="sc">{0:0.3f}</span><span class="st">"</span>.<span class="bu">format</span>(x)}) <span class="co"># 3 sig figs</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Y_pred=</span><span class="ch">\n</span><span class="st">"</span>,Y_pred)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights =</span><span class="ch">\n</span><span class="st">"</span>,<span class="bu">repr</span>(w))  <span class="co"># the repr() makes it so it can be copied &amp; pasted back into Python code</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="final-check-keras-version" class="level2">
<h2 class="anchored" data-anchor-id="final-check-keras-version">Final Check: Keras version</h2>
<p><a href="https://keras.io/">Keras</a> is a neural network library that lets us write NN applications very compactly. Try running the following using the X and Y from your 7-segment dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Just a demo of how one might similarly train a network using Keras</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Activation</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">10</span>, input_shape<span class="op">=</span>(<span class="dv">7</span>,)),</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    Activation(<span class="st">'sigmoid'</span>) ])</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,   <span class="co"># We'll talk about optimizer choices and loss choices later</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>model.fit(X, Y, epochs<span class="op">=</span><span class="dv">200</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Y_tilde = </span><span class="ch">\n</span><span class="st">"</span>, model.predict(X) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="follow-up-remarks" class="level1">
<h1>Follow-up: Remarks</h1>
<section id="re-stating-what-we-just-did" class="level3">
<h3 class="anchored" data-anchor-id="re-stating-what-we-just-did">Re-stating what we just did</h3>
<p>The original problem (posed at the top of this notebook) involves mapping some points from a 3-dimensional space into points in a 1-dimensional space, i.e.&nbsp;to points on the number line. The mapping is done by the combination of a weighted sum (a linear operation) and a nonlinear “<a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a>” applied to that sum. The use of an activation function like a sigmoid was originally intended to serve as an analogy of activation of biological neurons. Nonlinear activation functions are source of the “power” of neural networks (essentially we approximate some other function by means of a sum of <em>basis functions</em> in some <a href="https://en.wikipedia.org/wiki/Function_space">function space</a>, but don’t worry about that if you’re not math-inclined). The algorithm ‘learns’ to approximate this operation via supervised learning and gradient descent according to some loss function. We used the mean squared error (MSE) for our loss, but <a href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23">lots</a> and <a href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0">lots</a> of different loss functions could be used, a few of which we’ll look at another time.</p>
<p><strong>Question for reflection:</strong> Unlike fitting a line <span class="math inline">\(y = mx+b\)</span>, the weighted sum in our models in this notebook had no constant “bias” term like <span class="math inline">\(b\)</span>. How might we include such a term?</p>
</section>
<section id="one-thing-we-glossed-over-batch-size" class="level3">
<h3 class="anchored" data-anchor-id="one-thing-we-glossed-over-batch-size">One thing we glossed over: “<a href="https://www.youtube.com/watch?v=vVX9vld3vrY">batch size</a>”</h3>
<p>Question: Should we apply the gradient descent “update” to the weights <em>each time</em> we process a single row of <span class="math inline">\(X\)</span> &amp; <span class="math inline">\(Y\)</span>, or should we compute the combined loss of all the rows together at the same time, and <em>then</em> do the update? This is essentially asking the same question as “When fitting a line <span class="math inline">\(mx+b\)</span> to a bunch of data points, should we use all the points together to update <span class="math inline">\(m\)</span> and <span class="math inline">\(b,\)</span> or should we do this one point at a time – compute the gradients of the loss at one point, update the weights, compute gradients at another point, etc.?”</p>
<p>The number of points you use is called the <em>batch size</em> and it is what’s known as a “hyperparameter” – it is not part of the model <em>per se</em>, but it is a(n important) choice <em>you</em> make when training the model. The batch size affects the learning as follows: Averaging the gradints for many data points (i..e. a large batch size) will produce a smoother loss function and will also usually make the code execute more quickly through the dataset, but updating the weights for every point will cause the algorithm to learn with fewer iterations.</p>
<p>One quick way to observe this is to go up to the Keras code above and change <code>batch_size</code> from 1 to 10, and re-execute the cell. How is the accuracy after 200 iteractions, compared to when <code>batch_size=1</code>?</p>
<p><em>Terminology:</em> Technically, it’s called “batch training” when you sum the gradients for <em>all</em> the data points before updating the weights, whereas using fewer points is “minibatch training”, and updating for each point (i.e.&nbsp;each row, for us) is Stochastic Gradient Descent* (SGD – more on these terms <a href="https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/">here</a>). In practice, there is a tradeoff between smaller vs.&nbsp;larger (mini)batches, which has been the subject of intense scrutiny by researchers over the years. We will have more to say on this later.</p>
<p><strong>For discussion later:</strong> In our presentation above, were we using batch training, minibatch training or SGD?</p>
<p>.</p>
<p>*Note: many people will regard SGD as an optimization algorithm <em>per se</em>, and refer to doing SGD <em>even</em> for (mini)batch sizes larger than 1.</p>
</section>
<section id="optional-if-you-want-to-go-really-crazy" class="level2">
<h2 class="anchored" data-anchor-id="optional-if-you-want-to-go-really-crazy">Optional: If you want to go really crazy</h2>
<p>How about training on this dataset: <span class="math display">\[ \overbrace{
\left[ {\begin{array}{cc}
   0 &amp; 0 \\
   0 &amp; 1 \\
   1 &amp; 0 \\
   1 &amp; 1 \\
  \end{array} } \right]
}^{X} \rightarrow
\overbrace{
\left[ {\begin{array}{c}
   0   \\
   1  \\
   1  \\
   0 \\
  \end{array} } \right]
  }^Y.
\]</span> Good luck! ;-)<br>
(Hint 1: This problem features prominently in <a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">the history of Neural Networks</a>, involving Marvin Minsky and “AI Winter.”<br>
Hint 2: This whole lesson could instead be entitled “My First Artificial Neuron.”)</p>
<p>Next time, we will go on to <a href="https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html">Part 2: Bias and CE Loss</a>.</p>
</section>
<section id="additional-optional-exercise-binary-math-vs.-one-hot-encoding" class="level2">
<h2 class="anchored" data-anchor-id="additional-optional-exercise-binary-math-vs.-one-hot-encoding">Additional Optional Exercise: Binary Math vs.&nbsp;One-Hot Encoding</h2>
<p>For the 7-segment display, we used a one-hot encoding for our output, namely a set of true/false “bits” for each digit. One may wonder how effective this ouput-encoding method is, compared to a different bit-setting encoding method, namely binary representations.</p>
<ol type="1">
<li>Construct the target output matrix <span class="math inline">\(Y\)</span> for binary representations of the numbers 0 through 9. Your target matrix should have 10 rows and 4 columns (i.e, output bits for 1s, 2s, 4s, and 8s).</li>
<li>Using this <span class="math inline">\(Y\)</span> array, train the network as before, and plot the loss as a function of iteration.</li>
</ol>
<p>Question: Which method works ‘better’? One-hot encoding or binary encoding?</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/drscotthawley\.github\.io\/blog");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>