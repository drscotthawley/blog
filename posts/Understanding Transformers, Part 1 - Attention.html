<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Scott H. Hawley">
<meta name="dcterms.date" content="2023-08-14">
<meta name="description" content="If you can get this one thing, the rest will make sense.">

<title>blog - Understanding Transfomers, Part 1: Attention</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - Understanding Transfomers, Part 1: Attention">
<meta name="twitter:description" content="If you can get this one thing, the rest will make sense.">
<meta name="twitter:image" content="https://drscotthawley.github.io/blog/posts/images/TransformerOvalInOut.gif">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Understanding Transfomers, Part 1: Attention</h1>
                  <div>
        <div class="description">
          If you can get this one thing, the rest will make sense.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">architectures</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Scott H. Hawley </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 14, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface"><span class="header-section-number">1</span> Preface</a></li>
  <li><a href="#intuition" id="toc-intuition" class="nav-link" data-scroll-target="#intuition"><span class="header-section-number">2</span> Intuition</a>
  <ul class="collapse">
  <li><a href="#weighted-averages" id="toc-weighted-averages" class="nav-link" data-scroll-target="#weighted-averages"><span class="header-section-number">2.1</span> Weighted Averages</a></li>
  </ul></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context"><span class="header-section-number">3</span> Historical “Context”</a></li>
  <li><a href="#making-it-mathematical" id="toc-making-it-mathematical" class="nav-link" data-scroll-target="#making-it-mathematical"><span class="header-section-number">4</span> Making it Mathematical</a>
  <ul class="collapse">
  <li><a href="#similarity" id="toc-similarity" class="nav-link" data-scroll-target="#similarity"><span class="header-section-number">4.1</span> Similarity</a></li>
  </ul></li>
  <li><a href="#what-are-q-k-and-v" id="toc-what-are-q-k-and-v" class="nav-link" data-scroll-target="#what-are-q-k-and-v"><span class="header-section-number">5</span> What are Q, K, and V?</a></li>
  <li><a href="#afterward-i-wrote-a-song" id="toc-afterward-i-wrote-a-song" class="nav-link" data-scroll-target="#afterward-i-wrote-a-song"><span class="header-section-number">6</span> Afterward: I Wrote A Song</a></li>
  <li><a href="#sec-references" id="toc-sec-references" class="nav-link" data-scroll-target="#sec-references"><span class="header-section-number">7</span> References</a></li>
  <li><a href="#appendix-fun-with-softmax" id="toc-appendix-fun-with-softmax" class="nav-link" data-scroll-target="#appendix-fun-with-softmax"><span class="header-section-number">8</span> Appendix: Fun with Softmax</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/TransformerOvalInOut.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">(GIF animation, zooming in on Attention part of Transformer model diagram)</figcaption>
</figure>
</div>
<p>— work in progress. not ready yet —</p>
<section id="preface" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preface</h1>
<p>Transformer neural network models <span class="citation" data-cites="aiayn"><a href="#ref-aiayn" role="doc-biblioref">[1]</a></span> burst on the scene in 2017 and eventually “took over” as the preferred architecture for many machine learning tasks. [TODO: mention gpt and such]. Yet I found their complexity daunting, and this hindered my numerous, ardent attempts at study – see the References in <a href="#sec-references">Section&nbsp;7</a> for a list of all the papers, tutorials, and videos I consumed.</p>
<p>Things finally “clicked” for me this summer – years later! What finally helped was watching Andrej Karpathy’s excellent YouTube tutorial <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span> for the <em>second time</em>, when I noticed he homed in on the Attention part of the model and explained it as being a <em>weighted average of words.</em> I believe that if you grasp that part, then everything else about the Transformer is just “bells and whistles” that make it work better.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Throughout this tutorial I will use the phrase “Attention is just…” to share different perspectives that have helped me understand.</p>
</div>
</div>
</section>
<section id="intuition" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Intuition</h1>
<p>Certain parts of an input matter more than others. For example, when you look at an image, you probably focus on objects in the foreground more than the background. For example, in this picture, most people (and AI systems!) will focus attention on the dog:</p>
<div id="fig-doggie" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/doggie_vit_attn_map.png" width="96%" align="left" class="figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Example of attention being given to a foreground object, like this cute doggy! In right image, the background is darkened compared to the foreground, indicating that the foreground is receiving more attention than the background. (Source: Eunkwang Jeon, <a href="https://github.com/jeonsworld/ViT-pytorch">ViT-pytorch</a>)</figcaption>
</figure>
</div>
<p>Alternatively, when you’re reading or listening to a speaker, certain words carry more weight than others and contribute differently to the meaning formed in your mind. Consider the following sentence:</p>
<blockquote class="blockquote">
<p>“Please go to the store and get some milk.”</p>
</blockquote>
<p>If you’re trying understand the command being given, the words “go”, “store”, “get”, and “milk” probably matter the most to you. If what you’re interested in is the <em>tone</em> (e.g.&nbsp;“is the speaker being polite?”) then probably the “Please” would matter most.</p>
<p>Since the topic of Attention often comes up in the context of Natural Language Processing (NLP), we’ll stick to text for this lesson, but know that “<a href="https://paperswithcode.com/method/vision-transformer">Vision Transformers</a>” for image processing are definitely a thing <span class="citation" data-cites="vit_paper"><a href="#ref-vit_paper" role="doc-biblioref">[3]</a></span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a way to give certain parts of an input more weight than others.</p>
</div>
</div>
<section id="weighted-averages" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="weighted-averages"><span class="header-section-number">2.1</span> Weighted Averages</h2>
<p>On computers, the way we often emphasize certain parts of an array more than others is to multiply them by another array that has the “weights” to be assigned. In some other contexts, the “weights” array may be also called a “mask”, such as a “hard mask” of 1’s and 0’s to turn on or off parts of the array, or a “soft mask” with floating-point values to emphasize certain elements.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a soft mask.</p>
</div>
</div>
<p>Let’s use the sample sentence above, and <em>make up</em> some mask weights to stress the relative importants of different words. We’ll display the weights a colorbar to visualize their magnitude (darker = higher weight):</p>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML, display</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> <span class="st">"Please go to the store and get some milk"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">input</span>.split()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.array([<span class="fl">0.5</span>,<span class="fl">.8</span>,<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">.7</span>,<span class="fl">0.1</span>,<span class="fl">.9</span>,<span class="fl">0.1</span>,<span class="fl">.99</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>:words,<span class="st">"weight"</span>:weights})</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>display(df.set_index(<span class="st">'word'</span>).T.style.background_gradient(vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of mask weights = </span><span class="sc">{</span>mask<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="word-weights-0" class="cell-output cell-output-display">
<style type="text/css">
#T_2ca6b_row0_col0 {
  background-color: #73a9cf;
  color: #f1f1f1;
}
#T_2ca6b_row0_col1 {
  background-color: #045382;
  color: #f1f1f1;
}
#T_2ca6b_row0_col2, #T_2ca6b_row0_col5, #T_2ca6b_row0_col7 {
  background-color: #f0eaf4;
  color: #000000;
}
#T_2ca6b_row0_col3 {
  background-color: #fff7fb;
  color: #000000;
}
#T_2ca6b_row0_col4 {
  background-color: #0567a2;
  color: #f1f1f1;
}
#T_2ca6b_row0_col6 {
  background-color: #034a74;
  color: #f1f1f1;
}
#T_2ca6b_row0_col8 {
  background-color: #023d60;
  color: #f1f1f1;
}
</style>

<table id="T_2ca6b" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_2ca6b_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_2ca6b_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_2ca6b_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_2ca6b_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_2ca6b_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_2ca6b_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_2ca6b_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_2ca6b_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_2ca6b_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_2ca6b_row0_col0" class="data row0 col0">0.500</td>
<td id="T_2ca6b_row0_col1" class="data row0 col1">0.900</td>
<td id="T_2ca6b_row0_col2" class="data row0 col2">0.100</td>
<td id="T_2ca6b_row0_col3" class="data row0 col3">0.000</td>
<td id="T_2ca6b_row0_col4" class="data row0 col4">0.800</td>
<td id="T_2ca6b_row0_col5" class="data row0 col5">0.100</td>
<td id="T_2ca6b_row0_col6" class="data row0 col6">0.930</td>
<td id="T_2ca6b_row0_col7" class="data row0 col7">0.100</td>
<td id="T_2ca6b_row0_col8" class="data row0 col8">0.980</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sum of mask weights = 4.2</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above example, I made up a weight of nearly zero for “the” because it really doesn’t matter. The meaning of the sentence would still be unambiguous without “the”. We’ll use this trick of setting some weights to zero later on when we talk about “Masked Attention” for the Decoder part of the Transformer.</p>
</div>
</div>
<p>We’ll make it so that the weights all add up to one. If all <span class="math inline">\(N\)</span> words were weighted equally, the weight values would all be <span class="math inline">\(1/N\)</span>. For other weighting schemes, we divide the mask weights by their sum to get our attention weights. For the example sentence above, dividing by the sum gives us:</p>
<div class="cell">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> mask<span class="op">/</span>mask.<span class="bu">sum</span>()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>:words,<span class="st">"weight"</span>:attention})</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>display(df.set_index(<span class="st">'word'</span>).T.style.background_gradient(vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="fl">.22</span>).<span class="bu">format</span>(precision<span class="op">=</span><span class="dv">3</span>).hide())</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of attention weights = </span><span class="sc">{</span>attention<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="word-weights-1" class="cell-output cell-output-display">
<style type="text/css">
#T_8d408_row0_col0 {
  background-color: #60a1ca;
  color: #f1f1f1;
}
#T_8d408_row0_col1 {
  background-color: #045b8f;
  color: #f1f1f1;
}
#T_8d408_row0_col2, #T_8d408_row0_col5, #T_8d408_row0_col7 {
  background-color: #efe9f3;
  color: #000000;
}
#T_8d408_row0_col3 {
  background-color: #fef6fa;
  color: #000000;
}
#T_8d408_row0_col4 {
  background-color: #056fae;
  color: #f1f1f1;
}
#T_8d408_row0_col6 {
  background-color: #023e62;
  color: #f1f1f1;
}
#T_8d408_row0_col8 {
  background-color: #023858;
  color: #f1f1f1;
}
</style>

<table id="T_8d408" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th id="T_8d408_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Please</th>
<th id="T_8d408_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">go</th>
<th id="T_8d408_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">to</th>
<th id="T_8d408_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">the</th>
<th id="T_8d408_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">store</th>
<th id="T_8d408_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">and</th>
<th id="T_8d408_level0_col6" class="col_heading level0 col6" data-quarto-table-cell-role="th">get</th>
<th id="T_8d408_level0_col7" class="col_heading level0 col7" data-quarto-table-cell-role="th">some</th>
<th id="T_8d408_level0_col8" class="col_heading level0 col8" data-quarto-table-cell-role="th">milk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_8d408_row0_col0" class="data row0 col0">0.119</td>
<td id="T_8d408_row0_col1" class="data row0 col1">0.190</td>
<td id="T_8d408_row0_col2" class="data row0 col2">0.024</td>
<td id="T_8d408_row0_col3" class="data row0 col3">0.002</td>
<td id="T_8d408_row0_col4" class="data row0 col4">0.167</td>
<td id="T_8d408_row0_col5" class="data row0 col5">0.024</td>
<td id="T_8d408_row0_col6" class="data row0 col6">0.214</td>
<td id="T_8d408_row0_col7" class="data row0 col7">0.024</td>
<td id="T_8d408_row0_col8" class="data row0 col8">0.236</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sum of attention weights = 1</code></pre>
</div>
</div>
<p>When we convert the word strings to numbers (later), we will weight them appropriately. These weights are <em>literally</em> the attention. To put it differently:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a weighted average of inputs.</p>
</div>
</div>
<p>So how should we actually <em>get</em> those weights – how should they be determined? That’s what we’ll cover for the rest of this lesson. Firstly, it helps if we understand the context of something called “context vectors.”</p>
</section>
</section>
<section id="historical-context" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Historical “Context”</h1>
<p>There are a few different attention-like schemes that have arisen over the years. The cleanest starting point for our lesson is the work of Bahdanau et al <span class="citation" data-cites="bahdanau_attn"><a href="#ref-bahdanau_attn" role="doc-biblioref">[4]</a></span> who were making improvements to automated language translation mdoels that at the time were using a model architecture called a Recurrent Neural Network (RNN, which you don’t need to understand for this lesson, or the fact that specific flavor of RNN was called an LSTM which stood for Long Short-Term Memory).</p>
<p>At issue was traditional models’ management of the “context vector” of numbers that would store the “meaning” of the text as it was being processed. You can think of the context vector as being like the “state” of the machine as it works through the text.</p>
<p>TODO: fill in more later…</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention is just a weighted Bag of Words.</p>
</div>
</div>
</section>
<section id="making-it-mathematical" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Making it Mathematical</h1>
<p>TODO: fill in more later…</p>
<section id="similarity" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="similarity"><span class="header-section-number">4.1</span> Similarity</h2>
<p>TODO: fill in more later…</p>
<p>euclidean distance</p>
<p>cosine similarity</p>
<p>“scaled dot product attention”</p>
<p>softmax is best but you could use something else</p>
<p>note connection to thermodynamics</p>
</section>
</section>
<section id="what-are-q-k-and-v" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> What are Q, K, and V?</h1>
<p>TODO: fill in more later…</p>
</section>
<section id="afterward-i-wrote-a-song" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Afterward: I Wrote A Song</h1>
<p>While I was away on a solo writer’s retreat to a shack in the woods, I started fiddling with a little song idea inspired by this tweet:</p>
<div class="cell">
<div class="cell-output cell-output-display">

<blockquote class="twitter-tweet blockquote"><p lang="en" dir="ltr">"Traumatized Transformers": In which certain words are intrinsically weighted more that others.<br>(alternately, "Annoying Transformers Who Won't Shut Up About Their Favorite Topics")</p>— Horrific ML Ideas (@ml_ideas) <a href="https://twitter.com/ml_ideas/status/1683716504821325824?ref_src=twsrc%5Etfw">July 25, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>
<p>Here’s a video of a rough take I sent to friends:</p>
<div class="cell">
<div class="cell-output cell-output-display">

<iframe width="560" height="315" src="https://www.youtube.com/embed/W65eENFUM_0?start=28" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>
</div>
</section>
<section id="sec-references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<p>I recommend any of these the following. Seriously, I studied all of these and they’re great. Things just didn’t “click” for me until later in life I guess. LOL.</p>
<p>My trajectory was first the original paper<span class="citation" data-cites="aiayn"><a href="#ref-aiayn" role="doc-biblioref">[1]</a></span>, then Jay Alammar’s “The Illustrated Transformer” <span class="citation" data-cites="alammar_illustrated"><a href="#ref-alammar_illustrated" role="doc-biblioref">[5]</a></span>, then the Coursera course <span class="citation" data-cites="coursera"><a href="#ref-coursera" role="doc-biblioref">[6]</a></span> co-taught by one of the original Transformer paper authors, then Brendan Rohrer’s great post <span class="citation" data-cites="rohrer"><a href="#ref-rohrer" role="doc-biblioref">[7]</a></span>, then Jeremy Jordan’s tutorial <span class="citation" data-cites="jeremyj_transformers"><a href="#ref-jeremyj_transformers" role="doc-biblioref">[8]</a></span>. Then a couple years later Karpathy’s video <span class="citation" data-cites="andrej_karpathy_lets_2023"><a href="#ref-andrej_karpathy_lets_2023" role="doc-biblioref">[2]</a></span>, with a follow-up of CodeEmporiums’s video <span class="citation" data-cites="codeemporium"><a href="#ref-codeemporium" role="doc-biblioref">[9]</a></span>. But hopefully after reading my post(s) you won’t need to do all that!</p>
<p>I also recommend Daniel Dugas’ “GPT on a Napkin” <span class="citation" data-cites="dugas_gpt_napkin"><a href="#ref-dugas_gpt_napkin" role="doc-biblioref">[10]</a></span> and Sebastian Rashka’s post on Attention <span class="citation" data-cites="raschka_understanding_attn"><a href="#ref-raschka_understanding_attn" role="doc-biblioref">[11]</a></span>, but didn’t learn of these until after I started writing this post.</p>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-aiayn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Vaswani <em>et al.</em>, <span>“Attention is <span>All</span> you <span>Need</span>,”</span> in <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a></div>
</div>
<div id="ref-andrej_karpathy_lets_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. Karpathy, <span>“Let’s build <span>GPT</span>: From scratch, in code, spelled out.”</span> Jan. 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">https://www.youtube.com/watch?v=kCc8FmEb1nY</a></div>
</div>
<div id="ref-vit_paper" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">A. Dosovitskiy <em>et al.</em>, <span>“An <span>Image</span> is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image</span> <span>Recognition</span> at <span>Scale</span>,”</span> Oct. 2020. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a></div>
</div>
<div id="ref-bahdanau_attn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">D. Bahdanau, K. Cho, and Y. Bengio, <span>“Neural machine translation by jointly learning to align and translate,”</span> in <em>3rd international conference on learning representations, <span>ICLR</span> 2015, san diego, CA, USA, may 7-9, 2015, conference track proceedings</em>, Y. Bengio and Y. LeCun, Eds., 2015. Available: <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a></div>
</div>
<div id="ref-alammar_illustrated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">J. Alammar, <span>“The <span>Illustrated</span> <span>Transformer</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></div>
</div>
<div id="ref-coursera" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Y. B. Mourri and Ł. Kaiser, <span>“Natural <span>Language</span> <span>Processing</span> with <span>Attention</span> <span>Models</span>,”</span> <em>Coursera</em>. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.coursera.org/learn/attention-models-in-nlp">https://www.coursera.org/learn/attention-models-in-nlp</a></div>
</div>
<div id="ref-rohrer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">B. Rohrer, <span>“Transformers from <span>Scratch</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://e2eml.school/transformers.html">https://e2eml.school/transformers.html</a></div>
</div>
<div id="ref-jeremyj_transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J. Jordan, <span>“Understanding the <span>Transformer</span> architecture for neural networks,”</span> <em>Jeremy Jordan</em>. May 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.jeremyjordan.me/transformer-architecture/">https://www.jeremyjordan.me/transformer-architecture/</a></div>
</div>
<div id="ref-codeemporium" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">CodeEmporium, <span>“Transformer <span>Decoder</span> coded from scratch.”</span> Mar. 2023. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://www.youtube.com/watch?v=MqDehUoMk-E">https://www.youtube.com/watch?v=MqDehUoMk-E</a></div>
</div>
<div id="ref-dugas_gpt_napkin" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">D. Dugas, <span>“The <span>GPT</span>-3 <span>Architecture</span>, on a <span>Napkin</span>.”</span> Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">https://dugas.ch/artificial_curiosity/GPT_architecture.html</a></div>
</div>
<div id="ref-raschka_understanding_attn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">S. Raschka, <span>“Understanding and <span>Coding</span> the <span>Self</span>-<span>Attention</span> <span>Mechanism</span> of <span>Large</span> <span>Language</span> <span>Models</span> <span>From</span> <span>Scratch</span>,”</span> <em>Sebastian Raschka, PhD</em>. Accessed: Aug. 10, 2023. [Online]. Available: <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></div>
</div>
</div>
</section>
<section id="appendix-fun-with-softmax" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Appendix: Fun with Softmax</h1>
<p>This is optional.</p>
<p>If we multiply the inputs to the softmax function (i.e., the “logits”) by some scale factor, we can change the “sharpness” of the result. (For those in the know, this is analagous to varying the temperature of a thermodynamic system.) For a “sharpness” parameter of 1/2, we get ordinary softmax. For sharpness=0, <em>we turn softmax into an ordinary (non-weighted) averaging operator</em>. And for sharpness=1, we get only the largest input and all others basically zero.</p>
<p>Try playing with the slider below for 9 sample data values from our introductory sentence:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" data-startfrom="2" data-source-offset="-1"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1;"><span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> {plotter} <span class="im">from</span> <span class="st">"@kjerandp/plotter"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">apply_s</span>(logits<span class="op">,</span> s) {</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits<span class="op">.</span><span class="fu">map</span>(<span class="kw">function</span>(x) { <span class="cf">return</span> x <span class="op">*</span> (s<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>s<span class="op">+</span><span class="fl">1.0e-8</span>))<span class="op">;</span> })<span class="op">;</span>  </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">softmax</span>(logits) {</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> maxLogit <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">max</span>(<span class="op">...</span>logits)<span class="op">;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> scores <span class="op">=</span> logits<span class="op">.</span><span class="fu">map</span>(l <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(l <span class="op">-</span> maxLogit))<span class="op">;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> denom <span class="op">=</span> scores<span class="op">.</span><span class="fu">reduce</span>((a<span class="op">,</span> b) <span class="kw">=&gt;</span> a <span class="op">+</span> b)<span class="op">;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores<span class="op">.</span><span class="fu">map</span>(s <span class="kw">=&gt;</span> s <span class="op">/</span> denom)<span class="op">;</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">'Please'</span><span class="op">,</span><span class="st">'go'</span><span class="op">,</span><span class="st">'to'</span><span class="op">,</span><span class="st">'the'</span><span class="op">,</span><span class="st">'store'</span><span class="op">,</span><span class="st">'and'</span><span class="op">,</span><span class="st">'get'</span><span class="op">,</span><span class="st">'some'</span><span class="op">,</span><span class="st">'milk'</span>]<span class="op">;</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> [<span class="fl">0.119</span><span class="op">,</span> <span class="fl">0.190</span><span class="op">,</span>   <span class="fl">0.024</span><span class="op">,</span>  <span class="fl">0.002</span><span class="op">,</span>  <span class="fl">0.167</span><span class="op">,</span>  <span class="fl">0.024</span><span class="op">,</span>  <span class="fl">0.214</span><span class="op">,</span>  <span class="fl">0.024</span><span class="op">,</span>  <span class="fl">0.236</span>]</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> attn_weights<span class="op">.</span><span class="fu">map</span>(x <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">log</span>(x<span class="op">+</span><span class="fl">1.0e-8</span>))<span class="op">;</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>viewof s <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">label</span><span class="op">:</span> <span class="st">"Sharpness s:"</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="op">.</span><span class="bn">01</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span><span class="fl">0.5</span><span class="op">,</span> <span class="dt">width</span><span class="op">:</span><span class="dv">460</span>})<span class="op">;</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>numbers <span class="op">=</span> <span class="fu">softmax</span>(<span class="fu">apply_s</span>(logits<span class="op">,</span>s))<span class="op">;</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">// make a bar plot manually by plotting a bunch of rectangles. </span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>xlocs <span class="op">=</span> [<span class="dv">0</span><span class="op">,</span><span class="dv">1</span><span class="op">,</span><span class="dv">2</span><span class="op">,</span><span class="dv">3</span><span class="op">,</span><span class="dv">4</span><span class="op">,</span><span class="dv">5</span><span class="op">,</span><span class="dv">6</span><span class="op">,</span><span class="dv">7</span><span class="op">,</span><span class="dv">8</span>]<span class="op">;</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>basic_plot <span class="op">=</span> {</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> p <span class="op">=</span> <span class="fu">plotter</span>( {<span class="dt">height</span><span class="op">:</span><span class="dv">350</span><span class="op">,</span> <span class="dt">width</span><span class="op">:</span><span class="dv">600</span><span class="op">,</span> <span class="dt">xDomain</span><span class="op">:</span> [xlocs[<span class="dv">0</span>]<span class="op">-</span><span class="fl">0.1</span><span class="op">,</span> xlocs[xlocs<span class="op">.</span><span class="at">length</span><span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>]<span class="op">,</span> <span class="dt">yDomain</span><span class="op">:</span> [<span class="dv">1</span><span class="op">,</span> <span class="op">-.</span><span class="dv">1</span>]<span class="op">,</span> <span class="dt">xLabel</span><span class="op">:</span><span class="st">'x'</span><span class="op">,</span> <span class="dt">yLabel</span><span class="op">:</span><span class="st">'softmax(sx)'</span> } )<span class="op">;</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(<span class="st">'softmax(xs/(1-s))'</span><span class="op">,</span> <span class="op">-</span><span class="fl">0.3</span><span class="op">,</span> <span class="fl">1.02</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'white'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span><span class="op">,</span> <span class="dt">anchor</span><span class="op">:</span> <span class="st">'start'</span> })<span class="op">;</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(<span class="st">'x'</span><span class="op">,</span> xlocs[xlocs<span class="op">.</span><span class="at">length</span><span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="fl">1.15</span><span class="op">,</span> <span class="op">-.</span><span class="bn">02</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'white'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> })<span class="op">;</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">rectangle</span>(<span class="op">-</span><span class="dv">10</span><span class="op">,</span><span class="dv">10</span><span class="op">,</span><span class="dv">10</span><span class="op">,-</span><span class="dv">10</span><span class="op">,</span> {<span class="dt">fill</span><span class="op">:</span><span class="st">'white'</span><span class="op">,</span> <span class="dt">opacity</span><span class="op">:</span><span class="fl">0.0</span>})<span class="op">;</span>  <span class="co">// background</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bar chart</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> numbers<span class="op">.</span><span class="at">length</span><span class="op">-</span><span class="dv">1</span><span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">rectangle</span>(xlocs[i]<span class="op">,</span> numbers[i]<span class="op">,</span> xlocs[i<span class="op">+</span><span class="dv">1</span>]<span class="op">,</span> <span class="dv">0</span><span class="op">,</span> { <span class="dt">fill</span><span class="op">:</span> <span class="st">'cyan'</span><span class="op">,</span> <span class="dt">opacity</span><span class="op">:</span><span class="fl">0.65</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span><span class="st">'gray'</span> })<span class="op">;</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">text</span>(words[i]<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> <span class="op">-.</span><span class="dv">1</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'yellow'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>      p<span class="op">.</span><span class="fu">text</span>(numbers[i]<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">2</span>)<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> numbers[i]<span class="op">+.</span><span class="bn">01</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'grey'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">var</span> i <span class="op">=</span> xlocs<span class="op">.</span><span class="at">length</span><span class="op">-</span><span class="dv">1</span><span class="op">;</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">rectangle</span>(xlocs[i]<span class="op">,</span> numbers[i]<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="dv">1</span><span class="op">,</span> <span class="dv">0</span><span class="op">,</span> { <span class="dt">fill</span><span class="op">:</span> <span class="st">'cyan'</span><span class="op">,</span> <span class="dt">opacity</span><span class="op">:</span><span class="fl">0.65</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span><span class="st">'gray'</span> })</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(words[i]<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> <span class="op">-.</span><span class="dv">1</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'yellow'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>  p<span class="op">.</span><span class="fu">text</span>(numbers[i]<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">2</span>)<span class="op">,</span> xlocs[i]<span class="op">+</span><span class="fl">0.5</span><span class="op">,</span> numbers[i]<span class="op">+.</span><span class="bn">01</span><span class="op">,</span> { <span class="dt">stroke</span><span class="op">:</span> <span class="st">'grey'</span><span class="op">,</span> <span class="dt">weight</span><span class="op">:</span> <span class="dv">6</span> } )</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> p<span class="op">.</span><span class="at">node</span><span class="op">;</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-10" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<hr>
<p>(c) 2023 Scott H. Hawley</p>


</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"\nimport {plotter} from \"@kjerandp/plotter\"\n\nfunction apply_s(logits, s) {\n    return logits.map(function(x) { return x * (s/(1-s+1.0e-8)); });  \n}\n\nfunction softmax(logits) {\n    const maxLogit = Math.max(...logits);\n    const scores = logits.map(l => Math.exp(l - maxLogit));\n    const denom = scores.reduce((a, b) => a + b);\n    return scores.map(s => s / denom);\n}\n\n\nwords = ['Please','go','to','the','store','and','get','some','milk'];\nattn_weights = [0.119, 0.190,\t0.024,\t0.002,\t0.167,\t0.024,\t0.214,\t0.024,\t0.236]\nlogits = attn_weights.map(x => Math.log(x+1.0e-8));\n\nviewof s = Inputs.range([0, 1], {label: \"Sharpness s:\", step: .01, value:0.5, width:460});\nnumbers = softmax(apply_s(logits,s));\n\n\n// make a bar plot manually by plotting a bunch of rectangles. \nxlocs = [0,1,2,3,4,5,6,7,8];\nbasic_plot = {\n  let p = plotter( {height:350, width:600, xDomain: [xlocs[0]-0.1, xlocs[xlocs.length-1]+1], yDomain: [1, -.1], xLabel:'x', yLabel:'softmax(sx)' } );\n  p.text('softmax(xs/(1-s))', -0.3, 1.02, { stroke: 'white', weight: 6, anchor: 'start' });\n  p.text('x', xlocs[xlocs.length-1]+1.15, -.02, { stroke: 'white', weight: 6 });\n  p.rectangle(-10,10,10,-10, {fill:'white', opacity:0.0});  // background\n\n  // bar chart\n  for (let i = 0; i < numbers.length-1; i++) {\n      p.rectangle(xlocs[i], numbers[i], xlocs[i+1], 0, { fill: 'cyan', opacity:0.65, stroke:'gray' });\n      p.text(words[i], xlocs[i]+0.5, -.1, { stroke: 'yellow', weight: 6 } )\n      p.text(numbers[i].toFixed(2), xlocs[i]+0.5, numbers[i]+.01, { stroke: 'grey', weight: 6 } )\n  }\n  var i = xlocs.length-1;\n  p.rectangle(xlocs[i], numbers[i], xlocs[i]+1, 0, { fill: 'cyan', opacity:0.65, stroke:'gray' })\n  p.text(words[i], xlocs[i]+0.5, -.1, { stroke: 'yellow', weight: 6 } )\n  p.text(numbers[i].toFixed(2), xlocs[i]+0.5, numbers[i]+.01, { stroke: 'grey', weight: 6 } )\n\n  return p.node;\n}\n"},{"methodName":"interpretQuiet","source":"shinyInput('s')"}]}
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../posts";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>