<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2017-02-06">

<title>blog - Machine Learning Reference List</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - Machine Learning Reference List">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://www.publicdomainpictures.net/pictures/270000/t2/netzwerk-total-001.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Machine Learning Reference List</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">foundations</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 6, 2017</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This has been my personal reading list, first compiled ca. February 2016 &amp; updated very infrequently (e.g.&nbsp;Oct 2016, Feb 2017, Sept 2017). The field moves so quickly, much of this may have been superseded by now. If you find it useful as well, that’s great.</p>
<p><em>I’m mostly interested in audio processing, so…</em></p>
<section id="jump-right-into-audio-processing-via-rnns" class="level3">
<h3 class="anchored" data-anchor-id="jump-right-into-audio-processing-via-rnns">Jump Right Into : Audio Processing via RNNs:</h3>
<ul>
<li><p>Generation/Synthesis of new sounds based on training set:</p>
<ul>
<li>Jake Fiala: “Deep Learning and Sound” <a href="http://fiala.uk/notes/deep-learning-and-sound-01-intro" class="uri">http://fiala.uk/notes/deep-learning-and-sound-01-intro</a></li>
<li>GRUV: <a href="https://github.com/MattVitelli/GRUV" class="uri">https://github.com/MattVitelli/GRUV</a>. Btw, found that LSTM worked better than GRU.</li>
<li>John Glover: <a href="http://www.johnglover.net/blog/generating-sound-with-rnns.html" class="uri">http://www.johnglover.net/blog/generating-sound-with-rnns.html</a> Glover used LSTM fed by phase vocoder (really just STFT).</li>
<li>Google Magenta for MIDI: <a href="https://magenta.tensorflow.org/welcome-to-magenta" class="uri">https://magenta.tensorflow.org/welcome-to-magenta</a></li>
<li>Google WaveNet for Audio… <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" class="uri">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>
<ul>
<li>WaveNet is slow. “Fast Wavenet”: <a href="https://github.com/tomlepaine/fast-wavenet" class="uri">https://github.com/tomlepaine/fast-wavenet</a></li>
<li>WaveNet in Keras: <a href="https://github.com/basveeling/wavenet" class="uri">https://github.com/basveeling/wavenet</a></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="general-neural-network-references" class="level3">
<h3 class="anchored" data-anchor-id="general-neural-network-references">General Neural Network References:</h3>
<ul>
<li>Books/Guides on Deep/Machine Learning: (all excellent)
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com" class="uri">http://neuralnetworksanddeeplearning.com</a></li>
<li><a href="http://machinelearningmastery.com" class="uri">http://machinelearningmastery.com</a></li>
<li><a href="https://www.deeplearningbook.org" class="uri">https://www.deeplearningbook.org</a></li>
<li><a href="http://karpathy.github.io/neuralnets/">Hacker’s Guide to Neural Nets</a> by karpathy</li>
</ul></li>
<li>Tutorials/Videos:
<ul>
<li>Youtube Playlist on “Deep Learning”, t from Oxford U. by Nando de Freitas <a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu" class="uri">https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu</a></li>
<li>Andrew Ng’s online course on ML at Stanford comes highly recommended: <a href="http://www.youtube.com/view_play_list?p=A89DCFA6ADACE599" class="uri">http://www.youtube.com/view_play_list?p=A89DCFA6ADACE599</a></li>
<li>Stanford Tutorial: <a href="http://ufldl.stanford.edu/tutorial/" class="uri">http://ufldl.stanford.edu/tutorial/</a></li>
</ul></li>
<li>Concepts in NN/Deep Learning:
<ul>
<li>Backpropagation (i.e.&nbsp;the chain rule):
<ul>
<li><ul>
<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">neuralnetworksanddeeplearning.org book, chapter 2</a></li>
</ul></li>
<li><ul>
<li>Chris Olah on backprop: <a href="http://colah.github.io/posts/2015-08-Backprop/" class="uri">http://colah.github.io/posts/2015-08-Backprop/</a></li>
</ul></li>
<li><ul>
<li>Karpathy on backprop: <a href="http://cs231n.github.io/optimization-2/" class="uri">http://cs231n.github.io/optimization-2/</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="recurrent-neural-networks-rnn-which-mostly-feature-lstm-nowadays" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-networks-rnn-which-mostly-feature-lstm-nowadays">Recurrent Neural Networks (RNN) (which mostly feature LSTM nowadays):</h3>
<ul>
<li>RNNs in general:
<ul>
<li>Karpathy post: &lt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/, Karpathy talk: https://www.youtube.com/watch?v=yCC09vCHzF8&gt;
<ul>
<li>Excellent annotated Char-NN in Keras tutorial: <a href="http://ml4a.github.io/guides/recurrent_neural_networks/" class="uri">http://ml4a.github.io/guides/recurrent_neural_networks/</a></li>
</ul></li>
<li>Andrew Trask post/tutorial: <a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/" class="uri">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li>
<li>Denny Britz post: <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" class="uri">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li>
<li>Class notes/tutorial (long!): <a href="http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf" class="uri">http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf</a></li>
<li>CS class notes (short): <a href="https://www.willamette.edu/~gorr/classes/cs449/rnn1.html" class="uri">https://www.willamette.edu/~gorr/classes/cs449/rnn1.html</a></li>
<li>Excellent post by Ross Goodwin RNNs: <a href="https://medium.com/@rossgoodwin/adventures-in-narrated-reality-6516ff395ba3#.q2xh8dp5t" class="uri">https://medium.com/@rossgoodwin/adventures-in-narrated-reality-6516ff395ba3#.q2xh8dp5t</a></li>
<li>Great List of references; <a href="https://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html" class="uri">https://handong1587.github.io/deep_learning/2015/10/09/rnn-and-lstm.html</a></li>
</ul></li>
<li>in TensorFlow: <a href="https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html" class="uri">https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html</a></li>
<li>Theano tutorial: <a href="http://deeplearning.net/tutorial/rnnslu.html" class="uri">http://deeplearning.net/tutorial/rnnslu.html</a></li>
<li>Batch Normalization for: <a href="https://arxiv.org/abs/1510.01378" class="uri">https://arxiv.org/abs/1510.01378</a>: <em>“applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn’t help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn’t seem to improve the generalization performance”</em></li>
</ul>
<p><em>Traditional RNNs suffer from vanishing/exploding gradient. Hence LSTM &amp; others…</em></p>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM):</h3>
<ul>
<li>Tutorial: <a href="http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb" class="uri">http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb</a></li>
<li>Chris Olah post: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a></li>
<li>Zach Lipton post, “Demystifying LSTM” (with Tutorial theano code): <a href="http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/" class="uri">http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/</a></li>
<li>Demo: Lightweight Theano-LSTM: <a href="https://github.com/JonathanRaiman/theano_lstm" class="uri">https://github.com/JonathanRaiman/theano_lstm</a></li>
<li>Massive 33-page review article by Lipton et al: <a href="http://arxiv.org/abs/1506.00019" class="uri">http://arxiv.org/abs/1506.00019</a></li>
<li>As of March 2016, Keras forum posts show that “stated” RNNs are still an active dev issue. (As of last year, Keras has LSTM but was resetting the “state”, = inconvenient &amp; slow.)….Update Sept 2016: Seems to be fixed</li>
<li>LSTM tutorial in Tensorflow: <a href="https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html" class="uri">https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html</a></li>
<li>Stateful LSTM in Keras for time-series prediction: <a href="https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py" class="uri">https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py</a>
<ul>
<li>Much-need Docs on stateful LSTM in Keras: <a href="http://philipperemy.github.io/keras-stateful-lstm/" class="uri">http://philipperemy.github.io/keras-stateful-lstm/</a></li>
</ul></li>
<li>Tensorflow sequence prediction: <a href="http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html" class="uri">http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html</a></li>
<li>LSTM backpropagation tutorial :-) <a href="http://arunmallya.github.io/writeups/nn/lstm/index.html#/" class="uri">http://arunmallya.github.io/writeups/nn/lstm/index.html#/</a></li>
</ul>
</section>
<section id="lstm-alternativesadvances" class="level3">
<h3 class="anchored" data-anchor-id="lstm-alternativesadvances">LSTM Alternatives/advances:</h3>
<ul>
<li>GRU (Gated Recurrent Unit) by Cho et al, “Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation”, <a href="http://arxiv.org/pdf/1406.1078v3.pdf" class="uri">http://arxiv.org/pdf/1406.1078v3.pdf</a> (2014)
<ul>
<li>Chung et al.&nbsp;Good exp of GRU &amp; LSTM, say GRU comparable to LSTM, <a href="http://arxiv.org/abs/1412.3555" class="uri">http://arxiv.org/abs/1412.3555</a></li>
<li>But GRUV/MVitelli found that LSTM outperformed GRU for audio accuracy</li>
<li>GRU’s are a bit simpler than LSTM, Britz blog/tutorial: <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano" class="uri">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano</a></li>
</ul></li>
<li>ClockWork-RNN by Koutnik et al, <a href="http://arxiv.org/pdf/1402.3511v1.pdf" class="uri">http://arxiv.org/pdf/1402.3511v1.pdf</a></li>
<li>Highway networks…</li>
<li>Echo State Networks (ESN). (2008) Comparison of MLP, RNN &amp; ESN for sequence modeling: <a href="https://www.researchgate.net/publication/224374378_A_comparison_of_MLP_RNN_and_ESN_in_determining_harmonic_contributions_from_nonlinear_loads" class="uri">https://www.researchgate.net/publication/224374378_A_comparison_of_MLP_RNN_and_ESN_in_determining_harmonic_contributions_from_nonlinear_loads</a></li>
<li>Undecimated Fully Convolutional Neural Networks (UFCNN): <a href="http://arxiv.org/pdf/1508.00317.pdf" class="uri">http://arxiv.org/pdf/1508.00317.pdf</a></li>
<li>ConvNet for Audio – Spotify analysis &amp; recommendation: <a href="http://benanne.github.io/2014/08/05/spotify-cnns.html" class="uri">http://benanne.github.io/2014/08/05/spotify-cnns.html</a></li>
</ul>
</section>
<section id="lstm-for-sequence-to-sequence-learning" class="level3">
<h3 class="anchored" data-anchor-id="lstm-for-sequence-to-sequence-learning">LSTM for Sequence to Sequence Learning:</h3>
<ul>
<li>Main paper: <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" class="uri">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a></li>
<li>There’s an encoder step and a decoder step</li>
<li>Example: <a href="https://bigaidream.gitbooks.io/subsets_ml_cookbook/content/dl/theano/theano_keras_sequence2sequence.html#keras-for-sequence-to-sequence-learning" class="uri">https://bigaidream.gitbooks.io/subsets_ml_cookbook/content/dl/theano/theano_keras_sequence2sequence.html#keras-for-sequence-to-sequence-learning</a></li>
<li>Keras Seq2Seq extension: <a href="https://github.com/farizrahman4u/seq2seq" class="uri">https://github.com/farizrahman4u/seq2seq</a></li>
<li>Multiple blog pages, re. language model: <a href="https://indico.io/blog/sequence-modeling-neuralnets-part1/" class="uri">https://indico.io/blog/sequence-modeling-neuralnets-part1/</a></li>
<li>Tensor flow tutorial: <a href="https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html" class="uri">https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html</a> WARNING: “It takes about 18GB of disk space and several hours to prepare the training corpus.” :-(</li>
<li>SE post on pitch-shift mapping <a href="http://stats.stackexchange.com/questions/220307/rnn-learning-sine-waves-of-different-frequencies" class="uri">http://stats.stackexchange.com/questions/220307/rnn-learning-sine-waves-of-different-frequencies</a></li>
<li>Denoising:
<ul>
<li><a href="http://mlsp.cs.cmu.edu/people/rsingh/docs/waspaa2015.pdf" class="uri">http://mlsp.cs.cmu.edu/people/rsingh/docs/waspaa2015.pdf</a></li>
<li>Denoising Autoencoder: <a href="https://www.quora.com/Is-it-possible-to-create-an-adaptive-filter-using-neural-network-so-that-after-training-it-can-filter-noisy-signal-and-give-desired-output" class="uri">https://www.quora.com/Is-it-possible-to-create-an-adaptive-filter-using-neural-network-so-that-after-training-it-can-filter-noisy-signal-and-give-desired-output</a>, <a href="https://www.quora.com/Can-a-denoising-autoencoder-remove-or-filter-noise-in-a-noisy-signal-like-audio-and-recover-the-clean-signal" class="uri">https://www.quora.com/Can-a-denoising-autoencoder-remove-or-filter-noise-in-a-noisy-signal-like-audio-and-recover-the-clean-signal</a></li>
<li>Dereverberation:</li>
</ul></li>
</ul>
</section>
<section id="extended-memory-architectures" class="level3">
<h3 class="anchored" data-anchor-id="extended-memory-architectures">Extended Memory Architectures:</h3>
<ul>
<li>Memory Networks, Weston,
<ul>
<li>First paper: &lt;https://arxiv.org/abs/1410.3916. &gt;</li>
<li>Tutorial: <a href="http://www.thespermwhale.com/jaseweston/icml2016/" class="uri">http://www.thespermwhale.com/jaseweston/icml2016/</a></li>
<li>End-to-End version: <a href="http://arxiv.org/abs/1503.08895" class="uri">http://arxiv.org/abs/1503.08895</a></li>
<li>Keras version: <a href="https://github.com/fchollet/keras/blob/master/examples/babi_memnn.py" class="uri">https://github.com/fchollet/keras/blob/master/examples/babi_memnn.py</a></li>
</ul></li>
<li>Stack-Augmented Recurrent Nets, Joulin &amp; Mikolov “Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,” &lt;https://arxiv.org/pdf/1503.01007.pdf (2015)&gt;</li>
<li>Neural Turing Machines (NTM), Graves et al, <a href="https://arxiv.org/pdf/1410.5401v2.pdf" class="uri">https://arxiv.org/pdf/1410.5401v2.pdf</a></li>
<li>Neural Stack Machines:
<ul>
<li>Original paper: “Learning to Transduce with Unbounded Memory” by Grefenstette et al.: <a href="https://arxiv.org/abs/1506.02516" class="uri">https://arxiv.org/abs/1506.02516</a></li>
<li><a href="http://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/">Trask’s tutorial blog on Neural Stack Machines</a></li>
</ul></li>
</ul>
</section>
<section id="convolutional-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks">Convolutional Neural Networks:</h3>
<ul>
<li>Video: “What is wrong with convolutional neural nets?” Geoffrey Hinton, Fields Institute, August 2017”<a href="https://www.youtube.com/watch?v=Mqt8fs6ZbHk&amp;feature=youtu.be" class="uri">https://www.youtube.com/watch?v=Mqt8fs6ZbHk&amp;feature=youtu.be</a></li>
<li>Excellent: “History of major convnet architectures” (LeNet, AlexNet, Inception ResNet, VGG,…) <a href="https://culurciello.github.io/tech/2016/06/04/nets.html" class="uri">https://culurciello.github.io/tech/2016/06/04/nets.html</a></li>
<li>Excellent: “A guide to convolution arithmetic for deep learning” by Dumoulin and Visin <a href="https://arxiv.org/pdf/1603.07285.pdf" class="uri">https://arxiv.org/pdf/1603.07285.pdf</a></li>
<li>Glossary/Summary of conv net terms/concepts:
<ul>
<li><p><strong>Vector:</strong> <em>not</em> a true vector in the sense of vector calculus. Just a one-dimensional array. “N-dimensional vector” = 1-D array with N elements.</p></li>
<li><p><strong>Tensor:</strong> <em>not</em> a true tensor in the sense of differential geometry. Just a multi-dimensional arry or “matrix”.</p></li>
<li><p><strong>Affine Transformation:</strong> General math term; here we just mean multiplying by a tensor and (maybe) adding a constant bias (vector). Generalization of “linear transformation.”</p></li>
<li><p><strong>Convolution:</strong> Pretty much what you’d normally think of “convolution” in the DSP sense. <em>The following analogy helps me too: Evaluating a finite-difference stencil on a discretised scalar field via a banded (e.g.&nbsp;tridiagonal) matrix would be considered a convolution in the CNN sense, because said matrix is sparse and the same weights are used throughout.</em></p></li>
<li><p><strong>Channel Axis:</strong> (quoting D&amp;V): “is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track).”</p></li>
<li><p><strong>Feature Map:</strong> Generally, the output of running one particular convolution kernel over a data item (e.g over an image). However there are also <em>input feature maps</em>, examples of which are the “channels” referred to earlier (e.g.&nbsp;RGB, Left/Right).</p></li>
<li><p><strong>Flattening:</strong> turn a tensor into a vector</p></li>
<li><p><strong>Pooling:</strong> can think of it like a special type of convolution kernel (except it may not just add up the kernel’s inputs). Usually “Max Pooling”, as in: take the maximum value from the the kernel’s inputs. (On the other hand, “Average pooling” really is just a regular top-hat convolution.) In contrast to regular convolution, pooling does not involve zero padding, and pooling often takes place over non-overlapping regions of the input.</p></li>
<li><p><strong>(Zero-)Padding:</strong> Pretty much like in the DSP sense: add zeros to the front or end of a data stream or image, so that you can run convolution kernel all the way up to &amp; over the boundaries of where they data’s defined.</p></li>
<li><p><strong>Transposed Convolution:</strong> Analagous to transposing a matrix to get an output with oppositely-ordered shape, e.g.&nbsp;to go from an output feature map of one shape, back to the original shape of the input. <em>There seems to be some confusion, whereby some people treat the transpose as if it’s an inverse, like <span class="math display">\[A^T A = I\]</span>. ??</em></p></li>
<li><p><strong>1x1:</strong> Actually 1x1xC, where C is, e.g.&nbsp;the number color channels in a RGB image (3).</p></li>
<li><p><em>an observation: the bigger shape of the kernel, the smaller the shape of its output feature map</em></p></li>
</ul></li>
<li>Related: Deconvolutional Networks <a href="http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf" class="uri">http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf</a></li>
</ul>
</section>
<section id="reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning">Reinforcement Learning:</h3>
<ul>
<li>OpenAIGym: <a href="https://openai.com/blog/openai-gym-beta/" class="uri">https://openai.com/blog/openai-gym-beta/</a>
<ul>
<li>Tutorial for cart-pole problem: <a href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/" class="uri">http://kvfrans.com/simple-algoritms-for-solving-cartpole/</a></li>
</ul></li>
<li>For games: Giraffe</li>
<li>Atari DRL: Video: <a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" class="uri">https://www.youtube.com/watch?v=V1eYniJ0Rnk</a></li>
<li>BetaGo: <a href="https://github.com/maxpumperla/betago" class="uri">https://github.com/maxpumperla/betago</a></li>
</ul>
</section>
<section id="data-representation" class="level3">
<h3 class="anchored" data-anchor-id="data-representation">Data Representation:</h3>
<p>Scattering Hierarchy (multi-scale representation) by Mallat (2012-2014), Pablo Sprechman’s talk <a href="https://youtu.be/OS6rZXKVU1Y?t=20m44s" class="uri">https://youtu.be/OS6rZXKVU1Y?t=20m44s</a></p>
</section>
<section id="related-approaches-to-neural-networks-historical" class="level3">
<h3 class="anchored" data-anchor-id="related-approaches-to-neural-networks-historical">Related Approaches to Neural Networks: (historical)</h3>
<ul>
<li>Hidden Markov Models (HMM). Dahl used for text classification: George E. Dahl, Ryan P. Adams, and Hugo Larochelle. “Training restricted boltzmann machines on word observations.” arXiv:1202.5695v1 (2012)</li>
<li>Support Vector Machine (SVM). SVMs are globally convex, which is nice (whereas NNs are only locally convex). Very effective for classification tasks. But NNs have beat them out for complex datasets &amp; tasks. Audio app: e.g., Audio Classificiation by Gou &amp; Li (2003) <a href="http://www.ee.columbia.edu/~sfchang/course/spr-F05/papers/guo-li-svm-audio00.pdf" class="uri">http://www.ee.columbia.edu/~sfchang/course/spr-F05/papers/guo-li-svm-audio00.pdf</a></li>
<li>Restricted Boltzmann Machine (RBM). Hinton et al.&nbsp;mid-2000s</li>
</ul>
</section>
<section id="frameworks-too-many-to-choose-from" class="level3">
<h3 class="anchored" data-anchor-id="frameworks-too-many-to-choose-from">Frameworks (too many to choose from!):</h3>
<ul>
<li>Main Ones:
<ul>
<li><p><a href="http://deeplearning.net/software/theano/">Theano</a> - mature codebase, non-CUDA GPU support via libgpuarray</p></li>
<li><p><a href="https://www.tensorflow.org/">TensorFlow</a> - Google-supported, awesome viz tool TensorBoard</p></li>
<li><p><a href="http://keras.io">Keras</a>, runs on Theano or TensorFlow as backends. VERY popular</p></li>
<li><p><a href="http://torch.ch/">Torch</a> - used by LeCun &amp; Karpathy, scripting in Lua. Not Python.<br>
* <a href="http://pytorch.org/">PyTorch</a> Python bindings for Torch, includes ‘automatic differentiation’</p></li>
<li><p><a href="http://scikit-learn.org">Scikit-Learn</a> - General system for many methods; some Keras support. Allows ‘easy’ swapping of different ML methods &amp; models</p></li>
</ul></li>
<li>Others, not seeing these used as much:
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe</a>, supposed to be easy &amp; abstract</li>
<li><a href="http://lasagne.readthedocs.io/en/latest/">Lasagne</a> - Another Theano front end for abstraction &amp; ease of use</li>
<li><a href="https://github.com/hycis/Mozi">Mozi</a>, Another one build on Theano. Looks simple to use</li>
<li><a href="https://deeplearning4j.org/">DeepLearning4J</a>: The “J” is for “Java”</li>
<li>scikits.neural - not popular</li>
</ul></li>
<li>Which package to choose when starting out?
<ul>
<li>I say <strong>Keras</strong>. Everything’s super-easy and automated compared to others.</li>
</ul></li>
</ul>
</section>
<section id="more-tutorials-e.g.-app-specific" class="level3">
<h3 class="anchored" data-anchor-id="more-tutorials-e.g.-app-specific">More Tutorials (e.g., app-specific):</h3>
<ul>
<li>Lots in <a href="http://maachinelearningmastery.com" class="uri">http://maachinelearningmastery.com</a></li>
<li><a href="http://iamtrask.github.io">Andrew Trask’s Blog</a>: Andrew writes excellent tutorials. The first LSTM guide I read was his.</li>
<li>Tutorials on Theano, Keras, Lasagne, RNN: <a href="https://github.com/Vict0rSch/deep_learning" class="uri">https://github.com/Vict0rSch/deep_learning</a></li>
<li>Theano:
<ul>
<li>Theano basics: <a href="http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb" class="uri">http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb</a></li>
<li>Then crash course via code: <a href="https://github.com/Newmu/Theano-Tutorials" class="uri">https://github.com/Newmu/Theano-Tutorials</a></li>
<li>LSTM in Theano: <a href="http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb" class="uri">http://nbviewer.jupyter.org/github/JonathanRaiman/theano_lstm/blob/master/Tutorial.ipynb</a></li>
</ul></li>
<li>Tensorflow:
<ul>
<li>TensorFlow graph vis tutorial: <a href="https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html" class="uri">https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html</a></li>
<li>TensorFlow on AWS (tutorial video): <a href="https://www.youtube.com/watch?v=1QhCsO4jmoM" class="uri">https://www.youtube.com/watch?v=1QhCsO4jmoM</a></li>
<li>LSTM tutorial in Tensorflow: <a href="https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html" class="uri">https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html</a></li>
</ul></li>
<li>Torch:
<ul>
<li>First, Lua: “Learn Lua in 15 Minutes”: <a href="http://tylerneylon.com/a/learn-lua/" class="uri">http://tylerneylon.com/a/learn-lua/</a></li>
<li>Deep Learning in Torch: a 60-minute Blitz by Soumith Chintala at CVPR2015: <a href="https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb" class="uri">https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb</a></li>
<li>LSTM in Torch (by Zaremba) <a href="https://github.com/wojzaremba/lstm" class="uri">https://github.com/wojzaremba/lstm</a></li>
<li>Tutorial Videos: <a href="https://www.youtube.com/playlist?list=PLLHTzKZzVU9ebuL6DCclzI54MrPNFGqbW" class="uri">https://www.youtube.com/playlist?list=PLLHTzKZzVU9ebuL6DCclzI54MrPNFGqbW</a></li>
</ul></li>
</ul>
</section>
<section id="more-demos" class="level3">
<h3 class="anchored" data-anchor-id="more-demos">More Demos:</h3>
<ul>
<li>Anything by <a href="http://karpathy.github.io/"><span class="citation" data-cites="karpathy">@karpathy</span></a></li>
<li>Lightweight Theano-LSTM: <a href="https://github.com/JonathanRaiman/theano_lstm" class="uri">https://github.com/JonathanRaiman/theano_lstm</a></li>
<li>TensorFlow Playgrounds: &lt;http://playground.tensorflow.org &gt;</li>
</ul>
</section>
<section id="audio-applications" class="level3">
<h3 class="anchored" data-anchor-id="audio-applications">Audio Applications:</h3>
<ul>
<li>Huang et al, “Deep Recurrent NNs for Source Separation” <a href="http://posenhuang.github.io/papers/Joint_Optimization_of_Masks_and_Deep%20Recurrent_Neural_Networks_for_Monaural_Source_Separation_TASLP2015.pdf" class="uri">http://posenhuang.github.io/papers/Joint_Optimization_of_Masks_and_Deep%20Recurrent_Neural_Networks_for_Monaural_Source_Separation_TASLP2015.pdf</a>
<ul>
<li>Qutoe: “in parallel, for improving the efficiency of DRNN training, utterances are chopped into sequences of at most 100 time steps”</li>
</ul></li>
<li>Ron Weiss (Google) talk: “Training neural network acoustic models on waveforms” <a href="https://www.youtube.com/watch?v=sI_8EA0_ha8" class="uri">https://www.youtube.com/watch?v=sI_8EA0_ha8</a></li>
<li>Music comp: <a href="http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/" class="uri">http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/</a></li>
<li>Predict time sequence with LSTM &amp; Theano…GRUV</li>
<li>MULTI-RESOLUTION LINEAR PREDICTION BASED FEATURES FOR AUDIO ONSET DETECTION WITH BIDIRECTIONAL LSTM NEURAL NETWORKS Erik Marchi1 , Giacomo Ferroni2 , Florian Eyben1 , Leonardo Gabrielli2 , Stefano Squartini2 , Bjorn Schuller ¨ 3,1 <a href="http://mediatum.ub.tum.de/doc/1238131/865625.pdf" class="uri">http://mediatum.ub.tum.de/doc/1238131/865625.pdf</a></li>
<li>John Glover on generating instrument sounds with RNN: <a href="http://www.johnglover.net/blog/generating-sound-with-rnns.html" class="uri">http://www.johnglover.net/blog/generating-sound-with-rnns.html</a></li>
<li>Example of using spectrograms as images (for an image-based classifier): <a href="http://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change" class="uri">http://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change</a></li>
</ul>
</section>
<section id="datasets-of-mostly-musical-audio-for-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="datasets-of-mostly-musical-audio-for-machine-learning">Datasets of (mostly musical) Audio for Machine Learning:</h3>
<ul>
<li><p>IRMAS: for musical Instrument recognition: <a href="http://www.mtg.upf.edu/download/datasets/irmas" class="uri">http://www.mtg.upf.edu/download/datasets/irmas</a></p></li>
<li><p>Fraunhofer IDMT datasets: (Scroll down to “Published Datasets” on <a href="http://www.idmt.fraunhofer.de/en/business_units/m2d/research.html)" class="uri">http://www.idmt.fraunhofer.de/en/business_units/m2d/research.html)</a></p>
<ul>
<li><pre><code>  IDMT-SMT-Bass  An audio database for bass transcription and signal processing</code></pre></li>
<li><pre><code>  IDMT-SMT-Audio-Effects An audio database for automatic effect detection in recordings of electric guitar and bass</code></pre></li>
<li><pre><code>  IDMT-SMT-Bass Synthesis A Digital Waveguide Model of the Electric Bass Guitar including Different Playing Techniques</code></pre></li>
<li><pre><code>  IDMT-SMT-BASS-SINGLE-TRACK</code></pre></li>
<li><pre><code>  Multi-track studio recordings of live performances in Swing, Blues and Funk styles</code></pre></li>
<li><pre><code>  IDMT-SMT-Guitar An audio database for guitar transcription and signal processing</code></pre></li>
<li><pre><code>  IDMT-SMT-Drums An audio database for drum transcription and source separation</code></pre></li>
<li><pre><code>  Multicodec Invdec Tampering Dataset</code></pre></li>
</ul></li>
<li><p>Massive list of datasets (most are MIDI though): <a href="http://www.audiocontentanalysis.org/data-sets/" class="uri">http://www.audiocontentanalysis.org/data-sets/</a></p></li>
<li><p>Another massive list of datasets (with many repeats from above): <a href="http://wiki.schindler.eu.com/doku.php?id=datasets:overview" class="uri">http://wiki.schindler.eu.com/doku.php?id=datasets:overview</a></p></li>
<li><p>Melody annotation dataset: <a href="http://medleydb.weebly.com/description.html" class="uri">http://medleydb.weebly.com/description.html</a></p></li>
<li><p>Binaural audio: :</p>
<ul>
<li>Antoine Deleforge: <a href="http://perception.inrialpes.fr/~Deleforge/AVASM_Dataset/" class="uri">http://perception.inrialpes.fr/~Deleforge/AVASM_Dataset/</a></li>
</ul></li>
</ul>
</section>
<section id="activations-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="activations-optimizers">Activations &amp; Optimizers</h3>
<ul>
<li>ELU: Exponential Linear Unit, seems to work better than ReLU in many cases <a href="https://arxiv.org/pdf/1511.07289v1.pdf" class="uri">https://arxiv.org/pdf/1511.07289v1.pdf</a></li>
</ul>
</section>
<section id="in-physics" class="level2">
<h2 class="anchored" data-anchor-id="in-physics">In Physics:</h2>
<ul>
<li>“Fast cosmological parameter estimation using neural networks”, T. Auld, M. Bridges, M.P. Hobson and S.F. Gull, MNRAS. 000, 1–6 (2004), <a href="https://arxiv.org/pdf/astro-ph/0608174.pdf" class="uri">https://arxiv.org/pdf/astro-ph/0608174.pdf</a></li>
<li>“Parameterized Neural Networks for High-Energy Physics”, Baldi, P., Cranmer, K., Faucett, T., Sadowski, P., Whiteson, D. The European Physical Journal C. 76, 235, 1-7, May 2016, 2016, <a href="https://arxiv.org/pdf/1601.07913.pdf" class="uri">https://arxiv.org/pdf/1601.07913.pdf</a></li>
</ul>
<section id="hardware" class="level3">
<h3 class="anchored" data-anchor-id="hardware">Hardware:</h3>
<ul>
<li>Amazon Web Services (AWS):
<ul>
<li>Stanford disk image (AMI) with everything preinstalled: <a href="https://cs231n.github.io/aws-tutorial/" class="uri">https://cs231n.github.io/aws-tutorial/</a></li>
<li>…or just grab some other “Community AMI” with CUDA etc installed</li>
<li>Another AWS setup: <a href="https://github.com/andreasjansson/simple-aws-gpu-setup" class="uri">https://github.com/andreasjansson/simple-aws-gpu-setup</a></li>
<li>TensorFlow on AWS (tutorial video): <a href="https://www.youtube.com/watch?v=1QhCsO4jmoM" class="uri">https://www.youtube.com/watch?v=1QhCsO4jmoM</a></li>
<li>My AWS Aetup: <code>ami-a96285c4</code> ( AMI: old cuda but works: ami-63bf8209 do not like: 11777_MML (ami-37a58f5d) or DeepestLearning ) When u create your own AMI it brings your server down. :-(</li>
</ul></li>
</ul>
<pre><code>pip install --upgrade pip
pip install -U numpy
sudo pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
sudo pip install awscli h5py
git clone &lt;https://github.com/fchollet/keras.git&gt;
cd keras
sudo python setup.py install</code></pre>
<p>FACTOR OF 10 SPEEDUP using the g2.xlarge GPUs vs my Macbook Pro (no GPU)!!</p>
</section>
<section id="checkpointing" class="level3">
<h3 class="anchored" data-anchor-id="checkpointing">Checkpointing:</h3>
<p>run ‘watch’ command to execute AWS transfer to S3 ever <n> seconds …and spot instance went down without any checkpoint to allow uploading from EC2 to S3 it’s convoluted: install aws cli create an “IAM” user. Grant the user permissions to upload to s3 via <a href="https://forums.aws.amazon.com/thread.jspa?messageID=600007" class="uri">https://forums.aws.amazon.com/thread.jspa?messageID=600007</a> aws configure …good to go.</n></p>
<p><code>watch -n 550 aws s3 cp /tmp/weights.hdf5 s3://hawleymainbucket</code></p>
<ul>
<li>Theano GPU setup guide: <a href="https://github.com/andreasjansson/simple-aws-gpu-setup" class="uri">https://github.com/andreasjansson/simple-aws-gpu-setup</a></li>
<li>OpenMP: Don’t forget to enable multiple OpenMP threads! Can get you at least a factor of 2 speedup!
<ul>
<li>In most ‘modern’ Python installations (e.g.&nbsp;anaconda) OpenMP is automatic</li>
</ul></li>
<li>My proposed PC build: <a href="https://pcpartpicker.com/user/drscotthawley/saved/bFZ8dC" class="uri">https://pcpartpicker.com/user/drscotthawley/saved/bFZ8dC</a></li>
<li></li>
</ul>
</section>
<section id="self-organizing-maps" class="level3">
<h3 class="anchored" data-anchor-id="self-organizing-maps">Self-Organizing Maps:</h3>
<ul>
<li>“Unsupervised Classification of Audio Signals by Self-Organizing Maps and Bayesian Labeling”: <a href="http://link.springer.com/chapter/10.1007%2F978-3-642-28942-2_6" class="uri">http://link.springer.com/chapter/10.1007%2F978-3-642-28942-2_6</a></li>
<li>“Visualization of Tonal Content in the Symbolic and Audio Domains“ <a href="http://www.ccarh.org/publications/cm/15/cm15-10-toiviainen.pdf" class="uri">http://www.ccarh.org/publications/cm/15/cm15-10-toiviainen.pdf</a></li>
</ul>
</section>
<section id="weird-stuff" class="level3">
<h3 class="anchored" data-anchor-id="weird-stuff">“Weird Stuff”:</h3>
<ul>
<li>Stochastic path Deep NN for image rec: <a href="http://arxiv.org/pdf/1603.09382v1.pdf" class="uri">http://arxiv.org/pdf/1603.09382v1.pdf</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/drscotthawley\.github\.io\/blog");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>