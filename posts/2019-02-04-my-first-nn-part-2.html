<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-02-04">
<meta name="description" content="Bias and cross-entropy loss">

<title>blog - My First Neural Network, Part 2. Bias and CE Loss</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">My First Neural Network, Part 2. Bias and CE Loss</h1>
                  <div>
        <div class="description">
          Bias and cross-entropy loss
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 4, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#add-a-bias-term" id="toc-add-a-bias-term" class="nav-link active" data-scroll-target="#add-a-bias-term">1. Add a bias term</a></li>
  <li><a href="#video-interlude-logistic-regression" id="toc-video-interlude-logistic-regression" class="nav-link" data-scroll-target="#video-interlude-logistic-regression">Video Interlude: Logistic Regression</a></li>
  <li><a href="#use-a-different-loss-function-cross-entropy-loss" id="toc-use-a-different-loss-function-cross-entropy-loss" class="nav-link" data-scroll-target="#use-a-different-loss-function-cross-entropy-loss">2. Use a different loss function: Cross-Entropy loss</a></li>
  <li><a href="#excercise" id="toc-excercise" class="nav-link" data-scroll-target="#excercise">Excercise:</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Links to lessons: <a href="https://drscotthawley.github.io/blog/2017/02/23/Following-Gravity-Colab.html">Part 0</a>, <a href="https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html">Part 1</a>, <a href="https://drscotthawley.github.io/blog/2019/02/04/My-First-NN-Part-2.html">Part 2</a>, <a href="https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html">Part 3</a></p>
<p>Moving on from our <a href="https://drscotthawley.github.io/blog/2019/01/30/My-First-Neural-Network.html">our previous notebook</a>, we will investigate three things we could do to improve the models developed previously:</p>
<ol type="1">
<li>Add a bias term</li>
<li>Use a different loss function</li>
<li>Add more layers to the network <em>(postponed to next lesson)</em></li>
</ol>
<section id="add-a-bias-term" class="level2">
<h2 class="anchored" data-anchor-id="add-a-bias-term">1. Add a bias term</h2>
<p>Our weighted sums did not include any constant offset or “bias” term. This may be fine for some data, but not for many others. For example, in a simple linear model <span class="math inline">\(y = mx+b\)</span>, the choice of <span class="math inline">\(b=0\)</span> limits the model’s ability to accurately fit some data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.imgur.com/5CbsjVW.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image of bias vs no biast</figcaption><p></p>
</figure>
</div>
<p>That is effectively what we did with our weighted sum <span class="math inline">\(\sum_j X_{ij}w_j\)</span>: there was no constant offset. To correct this lack, we could add a new variable <span class="math inline">\(b\)</span> and make our weighted sum <span class="math inline">\(b + \sum_j X_{ij}w_j\)</span>. Equivalently, and more conveniently for the purposes of coding, we could put an additional column of 1’s in the input <span class="math inline">\(X\)</span>, and a new row to our weight matrix <span class="math inline">\(w\)</span>. By convention, this is usually done with the zeroth element, so that <span class="math inline">\(X_{i0}=1\)</span> and the columns of <span class="math inline">\(X\)</span> are moved to the right, and <span class="math inline">\(w_0 = b\)</span> will be the new constant offset (because <span class="math inline">\(1*w_0 = w_0\)</span>.)</p>
<p>For the first problem (Trask’s first problem), this change makes our new matrix equation look like (with new bias terms in red)</p>
<p><span class="math display">\[
f\left(
  \overbrace{
\left[ {\begin{array}{ccc}
   \color{red}1 &amp; 0 &amp; 0 &amp; 1 \\
   \color{red}1 &amp; 0 &amp; 1 &amp; 1\\
   \color{red}1 &amp; 1 &amp; 0 &amp; 1\\
   \color{red}1 &amp; 1 &amp; 1 &amp; 1\\
  \end{array} } \right]
}^\text{X}
\overbrace{
   \left[ {\begin{array}{c}
   \color{red}{w_0}  \\
    w_1\\
   w_2\\
   w_3
  \end{array} } \right]
}^{w}
\right)
  =
  \overbrace{
\left[ {\begin{array}{c}
   0   \\
   0  \\
   1  \\
   1 \\
  \end{array} } \right]
}^{\tilde{Y}}
\]</span> <strong><em>Foreshadowing</em></strong>: <em>Note that in this problem, the rightmost column of <span class="math inline">\(X\)</span> already was a column of all 1’s, and so already has something akin to a bias. Thus, adding a new column of all 1’s will not add any information, and so for this problem we expect that adding the bias won’t improve the model performance.)</em></p>
<p>With this change, we can still write our weighted sum as <span class="math inline">\(\sum_j X_{ij}w_j\)</span>, it’s just that <span class="math inline">\(j\)</span> now runs over 0..3 instead of 0..2. To emphasize: We can leave the rest of our code the same as before, provided we change <span class="math inline">\(X\)</span> by adding a column of 1’s.</p>
<p>In terms of coding the change to <span class="math inline">\(X\)</span>, we can either rewrite it by hand, or pull a numpy trick:</p>
<div class="cell" data-outputid="778b436e-d428-4223-f443-ed877697cc3b">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># old data</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([  [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>] ])</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># add a column of 1's</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>new_col <span class="op">=</span> np.ones((X.shape[<span class="dv">0</span>],<span class="dv">1</span>)) <span class="co"># array of 1s, w/ same # of rows as X, 1 col wide</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X_bias <span class="op">=</span> np.hstack((new_col,X))   <span class="co"># stack them horizontally</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1. 0. 0. 1.]
 [1. 0. 1. 1.]
 [1. 1. 0. 1.]
 [1. 1. 1. 1.]]</code></pre>
</div>
</div>
<p>Let’s compare our the use of the bias term without. We’ll define functions for the gradient descent and for the plotting of the loss history, so we can call these again later in this lesson.</p>
<div class="cell" data-outputid="cc806186-33f1-4d5f-ee90-be81b9ac1468">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]]).T  <span class="co"># target output dataset </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x,deriv<span class="op">=</span><span class="va">False</span>): </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(deriv<span class="op">==</span><span class="va">True</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_loss(Y_pred, Y, X, w, activ, loss_type<span class="op">=</span><span class="st">"mse"</span>):  <span class="co"># MSE loss</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  diff <span class="op">=</span> Y_pred <span class="op">-</span> Y</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> (diff<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  gradient <span class="op">=</span> np.dot( X.T, diff<span class="op">*</span>activ(Y_pred, deriv<span class="op">=</span><span class="va">True</span>)) <span class="co"># for weight update</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> loss, gradient</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(X, Y, activ, use_bias<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">3.0</span>, maxiter<span class="op">=</span><span class="dv">10000</span>, loss_type<span class="op">=</span><span class="st">'mse'</span>):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">  fit: Generic routine for doing our gradient decent</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">  Required arguments:</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co">      X:     input matrix</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co">      Y:     target output</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co">      activ: reference to an activation function</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co">      </span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co">  Keywork arguments (optional):</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co">      use_bias: Flag for whether to use bias in the model</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co">      alpha:    learning rate. Tip: Use the largest alpha 'you can get away with'</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co">      maxiter:  maximum number of iterations</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co">      loss_type: Set to MSE for now but we'll extend this later. </span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> use_bias:                       <span class="co"># add a column of 1's to X</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    new_col <span class="op">=</span> np.ones((X.shape[<span class="dv">0</span>],<span class="dv">1</span>)) </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.hstack((new_col,X))        </span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Define weights</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>  np.random.seed(<span class="dv">1</span>)                  <span class="co"># for reproducibility </span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> activ <span class="op">==</span> sigmoid:</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.random((X.shape[<span class="dv">1</span>],Y.shape[<span class="dv">1</span>])) <span class="op">-</span> <span class="dv">1</span>  <span class="co"># -1..1 </span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.random.random((X.shape[<span class="dv">1</span>],Y.shape[<span class="dv">1</span>]))<span class="op">/</span><span class="dv">10</span>  <span class="co"># only positive weights (for later)</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>  loss_hist <span class="op">=</span> []                     <span class="co"># start with an empty list</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(maxiter):</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> activ(np.dot(X,w))      <span class="co"># compute prediction, i.e. tilde{Y}</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    loss, gradient <span class="op">=</span> calc_loss(Y_pred, Y, X, w, activ, loss_type)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    loss_hist.append(loss)           <span class="co"># add to the history of the loss</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> alpha <span class="op">*</span> gradient            <span class="co"># weight update</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> w, Y_pred, loss_hist        <span class="co"># send these values back</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Now call the fit function twice, to compare with and without bias:</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>w_old, Y_pred_old, loss_hist_old <span class="op">=</span> fit(X, Y, sigmoid, use_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>w_new, Y_pred_new, loss_hist_new <span class="op">=</span> fit(X, Y, sigmoid)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results.  Make a function so we can call this again later</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_new_old(loss_hist_old, loss_hist_new, labels<span class="op">=</span>[<span class="st">"no bias"</span>, <span class="st">"with bias"</span>]):</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>  plt.loglog(loss_hist_old, label<span class="op">=</span>labels[<span class="dv">0</span>])</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>  plt.loglog(loss_hist_new, label<span class="op">=</span>labels[<span class="dv">1</span>])</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">"MSE Loss (monitoring)"</span>)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>plot_new_old(loss_hist_old, loss_hist_new)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="co"># And print the final answers:</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"No bias: Y_pred ="</span>,Y_pred_old)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"With bias: Y_pred ="</span>,Y_pred_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-04-My-First-NN-Part-2_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>No bias: Y_pred = [[0.00553557]
 [0.00451069]
 [0.99632065]
 [0.9954838 ]]
With bias: Y_pred = [[0.00485703]
 [0.00433876]
 [0.99612094]
 [0.99565733]]</code></pre>
</div>
</div>
<p>As expected, <em>for this problem</em>, the inclusion of bias didn’t make any significant difference. Let’s try the same thing for the 7-segment display problem from Part 1. And let’s try two different runs, one with sigmoid activation, and another with ReLU:</p>
<div class="cell" data-outputid="adf51e4d-9990-4c8e-b439-aac1e035e8a7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X_7seg <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 0</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 1 </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 2</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 3</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 4 </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 5</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 6 </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],  <span class="co"># 7 </span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>],  <span class="co"># 8 </span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>                    [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]   <span class="co"># 9 </span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                 ])</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>Y_7seg <span class="op">=</span> np.eye(<span class="dv">10</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> X_7seg, Y_7seg</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x,deriv<span class="op">=</span><span class="va">False</span>):   <span class="co"># relu activation</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(deriv<span class="op">==</span><span class="va">True</span>):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>) </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x<span class="op">*</span>(x<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Call the fit routine twice, once for sigmoid activation, once for relu</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activ <span class="kw">in</span> [sigmoid, relu]:</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">--------- activ = "</span>,activ)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  alpha <span class="op">=</span> <span class="fl">0.5</span> <span class="cf">if</span> activ <span class="op">==</span> sigmoid <span class="cf">else</span> <span class="fl">0.005</span>    <span class="co"># assign learning rate</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  w_old, Y_pred_old, loss_hist_old <span class="op">=</span> fit(X, Y, activ, alpha<span class="op">=</span>alpha, use_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  w_new, Y_pred_new, loss_hist_new <span class="op">=</span> fit(X, Y, activ, alpha<span class="op">=</span>alpha)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Report results</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>  plot_new_old(loss_hist_old, loss_hist_new)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  np.set_printoptions(formatter<span class="op">=</span>{<span class="st">'float'</span>: <span class="kw">lambda</span> x: <span class="st">"</span><span class="sc">{0:0.2f}</span><span class="st">"</span>.<span class="bu">format</span>(x)}) <span class="co"># 2 sig figs</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"No bias: Y_pred =</span><span class="ch">\n</span><span class="st">"</span>,Y_pred_old)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"With bias: Y_pred =</span><span class="ch">\n</span><span class="st">"</span>,Y_pred_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>

--------- activ =  &lt;function sigmoid at 0x7fa465235d08&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-04-My-First-NN-Part-2_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>No bias: Y_pred =
 [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.02]
 [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00]
 [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.03 0.00]
 [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02]
 [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.03 0.02]
 [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02]
 [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00]
 [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00]
 [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.94 0.00]
 [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.02 0.96]]
With bias: Y_pred =
 [[0.98 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.01]
 [0.01 0.98 0.00 0.01 0.01 0.00 0.00 0.02 0.00 0.00]
 [0.00 0.00 0.99 0.01 0.00 0.00 0.01 0.00 0.02 0.00]
 [0.00 0.00 0.01 0.98 0.00 0.00 0.00 0.01 0.00 0.02]
 [0.00 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.02 0.02]
 [0.00 0.00 0.00 0.01 0.01 0.98 0.02 0.00 0.00 0.02]
 [0.01 0.00 0.00 0.00 0.00 0.01 0.98 0.00 0.03 0.00]
 [0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.98 0.00 0.00]
 [0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.95 0.01]
 [0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.01 0.97]]


--------- activ =  &lt;function relu at 0x7fa4651f2400&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-04-My-First-NN-Part-2_files/figure-html/cell-4-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>No bias: Y_pred =
 [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.01 0.00]
 [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00]
 [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.01 -0.00]
 [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00]
 [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.01 0.00]
 [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00]
 [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.01 -0.00]
 [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00]
 [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.96 -0.00]
 [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]]
With bias: Y_pred =
 [[1.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.00 0.00 0.00]
 [-0.00 1.00 -0.00 -0.00 0.00 -0.00 -0.00 0.00 -0.00 -0.00]
 [-0.00 -0.00 1.00 0.00 -0.00 -0.00 -0.00 -0.00 0.00 -0.00]
 [-0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00]
 [-0.00 0.00 -0.00 -0.00 1.00 -0.00 -0.00 -0.00 0.00 0.00]
 [-0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.00 -0.00 -0.00 0.00]
 [-0.00 -0.00 -0.00 -0.00 -0.00 0.00 1.00 -0.00 0.00 -0.00]
 [0.00 0.00 -0.00 0.00 -0.00 -0.00 -0.00 1.00 -0.00 -0.00]
 [0.00 -0.00 0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.99 -0.00]
 [-0.00 -0.00 -0.00 0.00 0.00 0.00 -0.00 -0.00 0.00 1.00]]</code></pre>
</div>
</div>
<p>…So for this problem, it seems that adding the bias gave us a bit more accuracy, both for the sigmoid and relu activations. <em>Note: in this example, the learning rates were chosen by experiment; you should get in the habit of going back and experimenting with different learning rates.</em></p>
</section>
<section id="video-interlude-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="video-interlude-logistic-regression">Video Interlude: Logistic Regression</h2>
<p>What we’ve been doing up until now has been a “classification” problem, with “yes”/“no” answers represented by 1’s and 0’s. This sort of operation is closely associated with the statistical method of Logistic Regression. It is akin to linear regression but with a sigmoid activation function. When doing Logistic Regression, one optimizes to fit by finding the maximum “likelihood” of a given model being correct.</p>
<p>To gain some insight on Logistic Regression, watch the following StatQuest video. (You can ignore his remarks about his preceding video, “R squared” and “p-value”, etc.)</p>
<div class="cell" data-scrolled="true" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">

        <iframe width="560" height="315" src="https://www.youtube.com/embed/yIYKR4sgzI8" frameborder="0" allowfullscreen=""></iframe>
        
</div>
</div>
<p>In what follows, we will be <em>minimizing</em> the <em>negative</em> of the <em>logarithm</em> of the likelihood, a quantity typically known as the Cross-Entropy loss. (This same quantity is also the non-constant part of the “Kullback-Leibler Divergence” or “K-L divergence,” so you may hear it called that sometimes.)</p>
</section>
<section id="use-a-different-loss-function-cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="use-a-different-loss-function-cross-entropy-loss">2. Use a different loss function: Cross-Entropy loss</h2>
<p>Let’s return to Trask’s first problem for which there is only one target per data point (row) of input, namely a target of 0 or 1. In this case, using the sigmoid function for this classification problem is one of <a href="https://www.youtube.com/watch?v=yIYKR4sgzI8">logistic regression</a>, even though we hadn’t it identified it as such.</p>
<p>We’ve been using mean squared error (MSE) loss, but other loss functions exist. In particular, for outputs which are either “yes” or “no” such as the <em>classification problem</em> we are solving, a function called “<a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy">cross entropy</a>” is typically preferred. The cross-entropy loss is written like this:</p>
<p><span class="math display">\[L_{CE} = -\sum_i \left[ Y_i\log(\tilde{Y}_i) + (1-Y_i)\log(1-\tilde{Y}_i) \right]\]</span></p>
<p>Note that since the function <span class="math inline">\(\log(x)\)</span> is undefined for <span class="math inline">\(x\le0\)</span>, we need to make sure <span class="math inline">\(0&lt;\tilde{Y}_i&lt;1\)</span> for all <span class="math inline">\(i\)</span>. One way to ensure this is to use sigmoid activation! Thus, for classification problems, it is very common to see sigmoid activation (or its multi-class relative “<a href="http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/">softmax</a>”) immediately before the output, even for many-layer neural networks with all kinds of other activations in other places.</p>
<p>To use the CE loss with gradient descent, we need its derivative with respect to the weights. First let’s write the CE loss in terms of the inputs <span class="math inline">\(X\)</span>, weights <span class="math inline">\(w\)</span> and activation function <span class="math inline">\(f\)</span>:</p>
<p>…wait, for compactness, let’s write the weighted sum as <span class="math inline">\(S_i = \sum_j X_{ij}w_j\)</span>. Ok, now going forward….</p>
<p><span class="math display">\[L_{CE} = -\sum_i\left[ Y_i\log\left(f\left(S_i \right)\right) + (1-Y_i)\log\left(1- f\left(S_{i}\right)\right) \right]\]</span></p>
<p>For any function <span class="math inline">\(g(x)\)</span>, the derivative of <span class="math inline">\(\log(g(x))\)</span> with respect to x is just <span class="math inline">\(1/g*(du/dx)\)</span>, so our partial derivatives with respect to weights look like</p>
<p><span class="math display">\[{\partial L_{CE}\over \partial w_j} = -\sum_i\left[ {Y_i\over\tilde{Y_i}}{\partial f(S_i)\over \partial w_j}  -
{1-Y_i\over 1-\tilde{Y}_i} {\partial f(S_i)\over \partial w_j} \right]\\
= -\sum_i {\partial f(S_i) \over \partial S_i}{\partial S_i\over\partial w_j} \left[  {Y_i\over\tilde{Y_i}}  -
{1-Y_i\over 1-\tilde{Y}_i}  \right]
\]</span> And if we multiply by <span class="math inline">\(2/N\)</span>, we can write this as <span class="math display">\[
{\partial L_{CE}\over \partial w_j}
= {2\over N} \sum_{i=0}^{N-1}  {\partial f(S_i) \over \partial S_i}X_{ij}
\left[
{\tilde{Y_i}-Y_i\over \tilde{Y_i}(1-\tilde{Y_i}) }\right]\]</span> This is similar to the partial derivatives for our MSE loss, except the term in the denominator is new. To see this more clearly, recall that the weight update for MSE (from Part 1) was <span class="math display">\[
w := w - \alpha  X^T \cdot \left( [\tilde{Y}-Y]*\tilde{Y}*(1-\tilde{Y})\right)
\]</span> whereas for CE we actually get a bit of a simplification because the term in the denominator cancels with a similar term in the numerator: <span class="math display">\[
w := w - \alpha  X^T \cdot \left( [\tilde{Y}-Y]*\tilde{Y}*(1-\tilde{Y})\right) / (\tilde{Y}*(1-\tilde{Y})) \\
w := w - \alpha  X^T \cdot [\tilde{Y}-Y].
\]</span> Thus despite all this seeming complication, our CE weight update is actually simpler than what it was before as MSE!</p>
<p>Let’s try this out with code now:</p>
<div class="cell" data-outputid="3f867001-c4b3-4a32-9ed3-495d683f0098">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we'll "overwrite" the earlier calc_loss function</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_loss(Y_pred, Y, X, w, activ, loss_type<span class="op">=</span><span class="st">'ce'</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  diff <span class="op">=</span> Y_pred <span class="op">-</span> Y</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> (diff<span class="op">**</span><span class="dv">2</span>).mean()         <span class="co"># MSE loss</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="st">'ce'</span> <span class="op">==</span> loss_type:</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> diff <span class="op">/</span> (Y_pred<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Y_pred))     <span class="co"># use this for gradient</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#loss = -(Y*np.log(Y_tilde) + (1-Y)*np.log1p(-Y_tilde)).mean()  # CE Loss</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  Actually we don't care what the loss itself is. </span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  Let's use MSE loss for 'monitoring' regardless, so we can compare the </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  effects of using different gradients-of-loss functions</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  gradient <span class="op">=</span> np.dot( X.T, diff<span class="op">*</span>activ(Y_pred, deriv<span class="op">=</span><span class="va">True</span>))      <span class="co"># same as before</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> loss, gradient</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">#---- </span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X_bias</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]]).T  <span class="co"># target output dataset </span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare old and new</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>w_mse, Y_pred_mse, loss_hist_mse <span class="op">=</span> fit(X, Y, sigmoid, alpha<span class="op">=</span><span class="fl">0.5</span>, loss_type<span class="op">=</span><span class="st">'mse'</span>)  </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>w_ce, Y_pred_ce, loss_hist_ce    <span class="op">=</span> fit(X, Y, sigmoid, alpha<span class="op">=</span><span class="fl">0.5</span>, loss_type<span class="op">=</span><span class="st">'ce'</span>)   <span class="co"># fit</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plot_new_old(loss_hist_mse, loss_hist_ce, [<span class="st">"MSE, with bias"</span>, <span class="st">"CE, with bias"</span>])</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># And print the final answers:</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MSE _loss: Y_pred =</span><span class="ch">\n</span><span class="st">"</span>,Y_pred_mse)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CE loss: Y_pred =</span><span class="ch">\n</span><span class="st">"</span>,Y_pred_ce)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-04-My-First-NN-Part-2_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE _loss: Y_pred =
 [[0.01]
 [0.01]
 [0.99]
 [0.99]]
CE loss: Y_pred =
 [[0.00]
 [0.00]
 [1.00]
 [1.00]]</code></pre>
</div>
</div>
<p>This works a lot better! To understand why, note that the gradients for MSE loss scale like <span class="math display">\[[\tilde{Y}-Y]*\tilde{Y}*(1-\tilde{Y})\]</span> and thus <strong>these gradients go to zero</strong> as <span class="math inline">\(\tilde{Y}\rightarrow 0\)</span>, and/or <span class="math inline">\(\tilde{Y}\rightarrow 1\)</span>, which makes training <strong>very slow</strong>! In contrast, the extra denominator in the CE gradients effectively cancels out this behavior, leaving the remaining term of <span class="math display">\[[\tilde{Y}-Y]\]</span> which varies <em>linearly</em> with the difference from the target value. This makes training much more efficient.</p>
<div class="cell" data-outputid="f0f69bf7-e65d-4357-9be2-d73055a2e5d3">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Aside: What happens if we try ReLU activation with CE loss?  Bad things, probably.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Recall that ReLU maps negative numbers to 0, and isn't bounded from above.</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  Thus the "denominator" in the 'diff term' in our earlier code will tend to 'explode'.  </span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#  Put differently, note that log(x) is undefined for x=0, as is log(1-x) for x=1.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>w_relu_ce, Y_pred_relu_ce, loss_hist_relu_ce <span class="op">=</span> fit(X, Y, relu, alpha<span class="op">=</span><span class="fl">0.001</span>, loss_type<span class="op">=</span><span class="st">'ce'</span>) </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plot_new_old(loss_hist_ce, loss_hist_relu_ce, [<span class="st">"CE, sigmoid"</span>, <span class="st">"CE, ReLU"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2019-02-04-My-First-NN-Part-2_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="excercise" class="level1">
<h1>Excercise:</h1>
<p>Do the same comparison for the 7-segment display problem: Make a plot showing a comparison of the loss history use MSE loss vs.&nbsp;using CE loss. And print out the final values of <code>Y_pred</code> for each. Use a learning rate of 0.5 and sigmoid activation, with bias.</p>
<p>Take a screenshot of the output and upload it to your instructor.</p>
<p>(<em>Note:</em> for the 7-segment display, since the target <span class="math inline">\(Y\)</span> has multiple columns, we should “normalize” the output in order to be able to properly interpret the output values <span class="math inline">\(\tilde{Y}\)</span> as probabilities. To do so, we would use a <code>softmax</code> activation. For now, we haven’t bothered with this because it would add a bit more math, and is not actually necessary to solve this problem. )</p>
<p>Next time: <a href="https://drscotthawley.github.io/blog/2019/02/08/My-1st-NN-Part-3-Multi-Layer-and-Backprop.html">Part 3: Multi-Layer Networks and Backpropagation</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>