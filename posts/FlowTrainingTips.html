<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Scott H. Hawley">
<meta name="dcterms.date" content="2025-02-05">
<meta name="description" content="So you want to scale beyond 2D dots?">

<title>blog - DRAFT: Practical Guide for Training Flow-Based Generative Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
details > summary {
    color: #00966f;   /* the greenish tinge that appears in my blog */
    cursor: pointer; /* lil triangle thingy */
}
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="blog - DRAFT: Practical Guide for Training Flow-Based Generative Models">
<meta property="og:description" content="So you want to scale beyond 2D dots?">
<meta property="og:image" content="https://drscotthawley.github.io/blog/posts/images/flow_melty_flowers.png">
<meta property="og:site-name" content="blog">
<meta property="og:image:height" content="833">
<meta property="og:image:width" content="1255">
<meta name="twitter:title" content="blog - DRAFT: Practical Guide for Training Flow-Based Generative Models">
<meta name="twitter:description" content="So you want to scale beyond 2D dots?">
<meta name="twitter:image" content="https://drscotthawley.github.io/blog/posts/images/flow_melty_flowers.png">
<meta name="twitter:image-height" content="833">
<meta name="twitter:image-width" content="1255">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">DRAFT: Practical Guide for Training Flow-Based Generative Models</h1>
                  <div>
        <div class="description">
          So you want to scale beyond 2D dots?
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">generative</div>
                <div class="quarto-category">flows</div>
                <div class="quarto-category">scaling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Scott H. Hawley </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 5, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#this-is-not-ready-for-public-viewing-not-by-a-long-shot.-check-back-in-weeksmonths." id="toc-this-is-not-ready-for-public-viewing-not-by-a-long-shot.-check-back-in-weeksmonths." class="nav-link active" data-scroll-target="#this-is-not-ready-for-public-viewing-not-by-a-long-shot.-check-back-in-weeksmonths."><span class="header-section-number">1</span> THIS IS NOT READY FOR PUBLIC VIEWING, not by a long shot. Check back in weeks/months.</a></li>
  <li><a href="#practical-guide-to-developing-flow-based-generative-models" id="toc-practical-guide-to-developing-flow-based-generative-models" class="nav-link" data-scroll-target="#practical-guide-to-developing-flow-based-generative-models"><span class="header-section-number">2</span> Practical Guide to Developing Flow-Based Generative Models</a>
  <ul class="collapse">
  <li><a href="#or-why-do-my-flow-models-outputs-look-so-bad-and-what-i-can-i-do-to-fix-them" id="toc-or-why-do-my-flow-models-outputs-look-so-bad-and-what-i-can-i-do-to-fix-them" class="nav-link" data-scroll-target="#or-why-do-my-flow-models-outputs-look-so-bad-and-what-i-can-i-do-to-fix-them"><span class="header-section-number">2.0.1</span> Or: “Why Do My Flow Model’s Outputs Look So Bad? And What I Can I Do to Fix Them?”</a></li>
  </ul></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">3</span> Introduction</a></li>
  <li><a href="#outline-of-this-tutorial" id="toc-outline-of-this-tutorial" class="nav-link" data-scroll-target="#outline-of-this-tutorial"><span class="header-section-number">4</span> Outline of This Tutorial</a>
  <ul class="collapse">
  <li><a href="#repurpose-existing-diffusion-codes" id="toc-repurpose-existing-diffusion-codes" class="nav-link" data-scroll-target="#repurpose-existing-diffusion-codes"><span class="header-section-number">4.1</span> Repurpose Existing Diffusion Codes</a></li>
  <li><a href="#just-let-it-keep-training" id="toc-just-let-it-keep-training" class="nav-link" data-scroll-target="#just-let-it-keep-training"><span class="header-section-number">4.2</span> Just let it keep training</a></li>
  <li><a href="#get-high-throughput" id="toc-get-high-throughput" class="nav-link" data-scroll-target="#get-high-throughput"><span class="header-section-number">4.3</span> Get high “throughput”:</a></li>
  <li><a href="#metrics-how-are-we-doing" id="toc-metrics-how-are-we-doing" class="nav-link" data-scroll-target="#metrics-how-are-we-doing"><span class="header-section-number">4.4</span> Metrics: how are we doing?</a></li>
  <li><a href="#keeping-it-stable" id="toc-keeping-it-stable" class="nav-link" data-scroll-target="#keeping-it-stable"><span class="header-section-number">4.5</span> Keeping it stable</a></li>
  <li><a href="#cheat-outputs-by-fine-tuning-your-decoder" id="toc-cheat-outputs-by-fine-tuning-your-decoder" class="nav-link" data-scroll-target="#cheat-outputs-by-fine-tuning-your-decoder"><span class="header-section-number">4.6</span> ‘Cheat’ outputs by fine-tuning your Decoder</a></li>
  <li><a href="#the-usual-nn-training-stuff" id="toc-the-usual-nn-training-stuff" class="nav-link" data-scroll-target="#the-usual-nn-training-stuff"><span class="header-section-number">4.7</span> The usual NN training stuff:</a></li>
  <li><a href="#trainingfine-tuning-your-own-vae-vqvae" id="toc-trainingfine-tuning-your-own-vae-vqvae" class="nav-link" data-scroll-target="#trainingfine-tuning-your-own-vae-vqvae"><span class="header-section-number">4.8</span> Training/Fine-Tuning Your Own VAE / VQVAE</a></li>
  <li><a href="#inference-applications" id="toc-inference-applications" class="nav-link" data-scroll-target="#inference-applications"><span class="header-section-number">4.9</span> Inference Applications:</a></li>
  <li><a href="#other-random-notes" id="toc-other-random-notes" class="nav-link" data-scroll-target="#other-random-notes"><span class="header-section-number">4.10</span> other random notes</a></li>
  <li><a href="#aside---on-reading-probability-math." id="toc-aside---on-reading-probability-math." class="nav-link" data-scroll-target="#aside---on-reading-probability-math."><span class="header-section-number">4.11</span> Aside - On reading probability-math….</a></li>
  </ul></li>
  <li><a href="#notes-key-insights-from-references" id="toc-notes-key-insights-from-references" class="nav-link" data-scroll-target="#notes-key-insights-from-references"><span class="header-section-number">5</span> Notes / Key insights from References:</a>
  <ul class="collapse">
  <li><a href="#og-paper-lipman2023flow-andor-metas-guide-and-code-lipman2024flowmatchingguidecode" id="toc-og-paper-lipman2023flow-andor-metas-guide-and-code-lipman2024flowmatchingguidecode" class="nav-link" data-scroll-target="#og-paper-lipman2023flow-andor-metas-guide-and-code-lipman2024flowmatchingguidecode"><span class="header-section-number">5.1</span> OG paper <span class="citation" data-cites="lipman2023flow">[2]</span> and/or Meta’s “Guide and Code” <span class="citation" data-cites="lipman2024flowmatchingguidecode">[3]</span></a></li>
  <li><a href="#sd3-paper-sd3_paper" id="toc-sd3-paper-sd3_paper" class="nav-link" data-scroll-target="#sd3-paper-sd3_paper"><span class="header-section-number">5.2</span> SD3 paper: <span class="citation" data-cites="sd3_paper">[1]</span></a></li>
  <li><a href="#improving-the-training-of-rf-improving_rf" id="toc-improving-the-training-of-rf-improving_rf" class="nav-link" data-scroll-target="#improving-the-training-of-rf-improving_rf"><span class="header-section-number">5.3</span> Improving the Training of RF: <span class="citation" data-cites="improving_rf">[4]</span></a></li>
  <li><a href="#ben-recommended-equivariant_fm" id="toc-ben-recommended-equivariant_fm" class="nav-link" data-scroll-target="#ben-recommended-equivariant_fm"><span class="header-section-number">5.4</span> Ben recommended: <span class="citation" data-cites="equivariant_fm">[5]</span></a></li>
  <li><a href="#jasco-jasco" id="toc-jasco-jasco" class="nav-link" data-scroll-target="#jasco-jasco"><span class="header-section-number">5.5</span> JASCO <span class="citation" data-cites="jasco">[6]</span>:</a></li>
  <li><a href="#pnp-flow-pnp_flow-httpsgithub.comannegnxpnp-flow" id="toc-pnp-flow-pnp_flow-httpsgithub.comannegnxpnp-flow" class="nav-link" data-scroll-target="#pnp-flow-pnp_flow-httpsgithub.comannegnxpnp-flow"><span class="header-section-number">5.6</span> PnP-Flow: <span class="citation" data-cites="pnp_flow">[7]</span>: https://github.com/annegnx/PnP-Flow</a></li>
  <li><a href="#stable-flow-stableflow-httpsomriavrahami.comstable-flow" id="toc-stable-flow-stableflow-httpsomriavrahami.comstable-flow" class="nav-link" data-scroll-target="#stable-flow-stableflow-httpsomriavrahami.comstable-flow"><span class="header-section-number">5.7</span> Stable Flow: <span class="citation" data-cites="stableflow">[8]</span>: https://omriavrahami.com/stable-flow/</a></li>
  <li><a href="#not-sure-about" id="toc-not-sure-about" class="nav-link" data-scroll-target="#not-sure-about"><span class="header-section-number">5.8</span> Not sure about:</a></li>
  </ul></li>
  <li><a href="#outputs-from-shameless-use-of-llms-deepresearch-gptresearcher-more" id="toc-outputs-from-shameless-use-of-llms-deepresearch-gptresearcher-more" class="nav-link" data-scroll-target="#outputs-from-shameless-use-of-llms-deepresearch-gptresearcher-more"><span class="header-section-number">6</span> Outputs from Shameless Use of LLMs (DeepResearch, GPTResearcher &amp; More):</a>
  <ul class="collapse">
  <li><a href="#introduction-to-flow-matching-and-rectified-flow" id="toc-introduction-to-flow-matching-and-rectified-flow" class="nav-link" data-scroll-target="#introduction-to-flow-matching-and-rectified-flow"><span class="header-section-number">6.1</span> <strong>1. Introduction to Flow Matching and Rectified Flow</strong></a>
  <ul class="collapse">
  <li><a href="#overview-of-flow-matching" id="toc-overview-of-flow-matching" class="nav-link" data-scroll-target="#overview-of-flow-matching"><span class="header-section-number">6.1.1</span> <strong>1.1 Overview of Flow Matching</strong></a></li>
  <li><a href="#rectified-flow-a-scalable-extension" id="toc-rectified-flow-a-scalable-extension" class="nav-link" data-scroll-target="#rectified-flow-a-scalable-extension"><span class="header-section-number">6.1.2</span> <strong>1.2 Rectified Flow: A Scalable Extension</strong></a></li>
  </ul></li>
  <li><a href="#challenges-in-scaling-fmrf-models" id="toc-challenges-in-scaling-fmrf-models" class="nav-link" data-scroll-target="#challenges-in-scaling-fmrf-models"><span class="header-section-number">6.2</span> <strong>2. Challenges in Scaling FM/RF Models</strong></a></li>
  <li><a href="#practical-steps-for-training-fmrf-models" id="toc-practical-steps-for-training-fmrf-models" class="nav-link" data-scroll-target="#practical-steps-for-training-fmrf-models"><span class="header-section-number">6.3</span> <strong>3. Practical Steps for Training FM/RF Models</strong></a>
  <ul class="collapse">
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation"><span class="header-section-number">6.3.1</span> <strong>3.1 Data Preparation</strong></a></li>
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture"><span class="header-section-number">6.3.2</span> <strong>3.2 Model Architecture</strong></a></li>
  </ul></li>
  <li><a href="#accelerating-training-convergence" id="toc-accelerating-training-convergence" class="nav-link" data-scroll-target="#accelerating-training-convergence"><span class="header-section-number">6.4</span> <strong>4. Accelerating Training Convergence</strong></a>
  <ul class="collapse">
  <li><a href="#noise-sampling-techniques" id="toc-noise-sampling-techniques" class="nav-link" data-scroll-target="#noise-sampling-techniques"><span class="header-section-number">6.4.1</span> <strong>4.1 Noise Sampling Techniques</strong></a></li>
  <li><a href="#optimization-strategies" id="toc-optimization-strategies" class="nav-link" data-scroll-target="#optimization-strategies"><span class="header-section-number">6.4.2</span> <strong>4.2 Optimization Strategies</strong></a></li>
  </ul></li>
  <li><a href="#improving-output-quality" id="toc-improving-output-quality" class="nav-link" data-scroll-target="#improving-output-quality"><span class="header-section-number">6.5</span> <strong>5. Improving Output Quality</strong></a>
  <ul class="collapse">
  <li><a href="#post-training-refinements" id="toc-post-training-refinements" class="nav-link" data-scroll-target="#post-training-refinements"><span class="header-section-number">6.5.1</span> <strong>5.1 Post-Training Refinements</strong></a></li>
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions"><span class="header-section-number">6.5.2</span> <strong>5.2 Loss Functions</strong></a></li>
  </ul></li>
  <li><a href="#case-study-scaling-rectified-flow-transformers" id="toc-case-study-scaling-rectified-flow-transformers" class="nav-link" data-scroll-target="#case-study-scaling-rectified-flow-transformers"><span class="header-section-number">6.6</span> <strong>6. Case Study: Scaling Rectified Flow Transformers</strong></a>
  <ul class="collapse">
  <li><a href="#key-contributions-of-esser-et-al.-2024" id="toc-key-contributions-of-esser-et-al.-2024" class="nav-link" data-scroll-target="#key-contributions-of-esser-et-al.-2024"><span class="header-section-number">6.6.1</span> <strong>6.1 Key Contributions of Esser et al.&nbsp;(2024)</strong></a></li>
  <li><a href="#implementation-tips" id="toc-implementation-tips" class="nav-link" data-scroll-target="#implementation-tips"><span class="header-section-number">6.6.2</span> <strong>6.2 Implementation Tips</strong></a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6.7</span> <strong>7. Conclusion</strong></a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6.8</span> <strong>References</strong></a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">6.9</span> <strong>Additional Resources</strong></a></li>
  <li><a href="#introduction-to-flow-matching-and-rectified-flow-1" id="toc-introduction-to-flow-matching-and-rectified-flow-1" class="nav-link" data-scroll-target="#introduction-to-flow-matching-and-rectified-flow-1"><span class="header-section-number">6.10</span> <strong>1. Introduction to Flow Matching and Rectified Flow</strong></a>
  <ul class="collapse">
  <li><a href="#overview-of-flow-matching-1" id="toc-overview-of-flow-matching-1" class="nav-link" data-scroll-target="#overview-of-flow-matching-1"><span class="header-section-number">6.10.1</span> <strong>1.1 Overview of Flow Matching</strong></a></li>
  <li><a href="#rectified-flow-a-scalable-extension-1" id="toc-rectified-flow-a-scalable-extension-1" class="nav-link" data-scroll-target="#rectified-flow-a-scalable-extension-1"><span class="header-section-number">6.10.2</span> <strong>1.2 Rectified Flow: A Scalable Extension</strong></a></li>
  </ul></li>
  <li><a href="#challenges-in-scaling-fmrf-models-1" id="toc-challenges-in-scaling-fmrf-models-1" class="nav-link" data-scroll-target="#challenges-in-scaling-fmrf-models-1"><span class="header-section-number">6.11</span> <strong>2. Challenges in Scaling FM/RF Models</strong></a></li>
  <li><a href="#practical-steps-for-training-fmrf-models-1" id="toc-practical-steps-for-training-fmrf-models-1" class="nav-link" data-scroll-target="#practical-steps-for-training-fmrf-models-1"><span class="header-section-number">6.12</span> <strong>3. Practical Steps for Training FM/RF Models</strong></a>
  <ul class="collapse">
  <li><a href="#data-preparation-1" id="toc-data-preparation-1" class="nav-link" data-scroll-target="#data-preparation-1"><span class="header-section-number">6.12.1</span> <strong>3.1 Data Preparation</strong></a></li>
  <li><a href="#model-architecture-1" id="toc-model-architecture-1" class="nav-link" data-scroll-target="#model-architecture-1"><span class="header-section-number">6.12.2</span> <strong>3.2 Model Architecture</strong></a></li>
  </ul></li>
  <li><a href="#accelerating-training-convergence-1" id="toc-accelerating-training-convergence-1" class="nav-link" data-scroll-target="#accelerating-training-convergence-1"><span class="header-section-number">6.13</span> <strong>4. Accelerating Training Convergence</strong></a>
  <ul class="collapse">
  <li><a href="#noise-sampling-techniques-1" id="toc-noise-sampling-techniques-1" class="nav-link" data-scroll-target="#noise-sampling-techniques-1"><span class="header-section-number">6.13.1</span> <strong>4.1 Noise Sampling Techniques</strong></a></li>
  <li><a href="#optimization-strategies-1" id="toc-optimization-strategies-1" class="nav-link" data-scroll-target="#optimization-strategies-1"><span class="header-section-number">6.13.2</span> <strong>4.2 Optimization Strategies</strong></a></li>
  </ul></li>
  <li><a href="#improving-output-quality-1" id="toc-improving-output-quality-1" class="nav-link" data-scroll-target="#improving-output-quality-1"><span class="header-section-number">6.14</span> <strong>5. Improving Output Quality</strong></a>
  <ul class="collapse">
  <li><a href="#post-training-refinements-1" id="toc-post-training-refinements-1" class="nav-link" data-scroll-target="#post-training-refinements-1"><span class="header-section-number">6.14.1</span> <strong>5.1 Post-Training Refinements</strong></a></li>
  <li><a href="#loss-functions-1" id="toc-loss-functions-1" class="nav-link" data-scroll-target="#loss-functions-1"><span class="header-section-number">6.14.2</span> <strong>5.2 Loss Functions</strong></a></li>
  </ul></li>
  <li><a href="#case-study-scaling-rectified-flow-transformers-1" id="toc-case-study-scaling-rectified-flow-transformers-1" class="nav-link" data-scroll-target="#case-study-scaling-rectified-flow-transformers-1"><span class="header-section-number">6.15</span> <strong>6. Case Study: Scaling Rectified Flow Transformers</strong></a>
  <ul class="collapse">
  <li><a href="#key-contributions-of-esser-et-al.-2024-1" id="toc-key-contributions-of-esser-et-al.-2024-1" class="nav-link" data-scroll-target="#key-contributions-of-esser-et-al.-2024-1"><span class="header-section-number">6.15.1</span> <strong>6.1 Key Contributions of Esser et al.&nbsp;(2024)</strong></a></li>
  <li><a href="#implementation-tips-1" id="toc-implementation-tips-1" class="nav-link" data-scroll-target="#implementation-tips-1"><span class="header-section-number">6.15.2</span> <strong>6.2 Implementation Tips</strong></a></li>
  </ul></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">6.16</span> <strong>7. Conclusion</strong></a></li>
  <li><a href="#references-1" id="toc-references-1" class="nav-link" data-scroll-target="#references-1"><span class="header-section-number">6.17</span> <strong>References</strong></a></li>
  <li><a href="#additional-resources-1" id="toc-additional-resources-1" class="nav-link" data-scroll-target="#additional-resources-1"><span class="header-section-number">6.18</span> <strong>Additional Resources</strong></a></li>
  </ul></li>
  <li><a href="#references-2" id="toc-references-2" class="nav-link" data-scroll-target="#references-2"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<style>
figcaption {
  margin: auto;
  text-align: center;
</style>
<section id="this-is-not-ready-for-public-viewing-not-by-a-long-shot.-check-back-in-weeksmonths." class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> THIS IS NOT READY FOR PUBLIC VIEWING, not by a long shot. Check back in weeks/months.</h1>
<p>Just gathering my thoughts. This is a draft.</p>
</section>
<section id="practical-guide-to-developing-flow-based-generative-models" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Practical Guide to Developing Flow-Based Generative Models</h1>
<section id="or-why-do-my-flow-models-outputs-look-so-bad-and-what-i-can-i-do-to-fix-them" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="or-why-do-my-flow-models-outputs-look-so-bad-and-what-i-can-i-do-to-fix-them"><span class="header-section-number">2.0.1</span> Or: “Why Do My Flow Model’s Outputs Look So Bad? And What I Can I Do to Fix Them?”</h3>
<p>Flow matching and rectified flow models have emerged as leading choices for high-quality data synthesis, offering state-of-the-art results with remarkable simplicity, flexibility and speed. While these models can be readily adapted from existing diffusion codes, the development of new systems and/or scaling from low-dimensional (e.g., 2D) examples to real-world applications can present unanticipated challenges. This tutorial distills key guidelines from multiple research groups to help newcomers accelerate training and improve output quality. We’ll cover essential techniques for optimizing training throughput, monitoring progress, and maintaining stability, along with practical features like latent space autoencoding and inpainting. Code will be provided, though due to time constraints we will show pre-computed training results (as with a “cooking show”). Experienced practitioners are encouraged to attend and contribute their own insights.</p>
</section>
</section>
<section id="introduction" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Introduction</h1>
<p>Sometimes stuff takes too long to converge. This looks bad:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/flow_melty_flowers.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Fig. 1: Horrific “melty flowers” seen during mid-training of a 128x128 conditional flow matching model on the Oxford Flowers dataset.</figcaption>
</figure>
</div>
<hr>
</section>
<section id="outline-of-this-tutorial" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Outline of This Tutorial</h1>
<section id="repurpose-existing-diffusion-codes" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="repurpose-existing-diffusion-codes"><span class="header-section-number">4.1</span> Repurpose Existing Diffusion Codes</h2>
<p>There are only a couple parts you need to change. Many of the following tips/insights apply equal well to diffusion model development.</p>
<p>If you don’t want to use their full codes, you can at the very least grab their UNets which often have efficient implementations of attention, embeddings for time &amp; other conditions, positional embeddings, along with features like gradient checkpointing etc…</p>
</section>
<section id="just-let-it-keep-training" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="just-let-it-keep-training"><span class="header-section-number">4.2</span> Just let it keep training</h2>
<p>Really. If you have the GPUs to burn, just keep it going while you do other things. However, you may not notice much improvement without other improvements (below).</p>
</section>
<section id="get-high-throughput" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="get-high-throughput"><span class="header-section-number">4.3</span> Get high “throughput”:</h2>
<ul>
<li>Be mindful of batch size. As with many NN codes, there is a “sweet spot” between huge batches vs.&nbsp;frequent backprop steps</li>
<li>try encoding to latent space, VAE, VQVAE / VQGAN (try VAE for starters)
<ul>
<li>Pre-encode everything (with augmentations encoded). h/t SD3 <span class="citation" data-cites="sd3_paper"><a href="#ref-sd3_paper" role="doc-biblioref">[1]</a></span></li>
</ul></li>
</ul>
</section>
<section id="metrics-how-are-we-doing" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="metrics-how-are-we-doing"><span class="header-section-number">4.4</span> Metrics: how are we doing?</h2>
<ul>
<li>Remember the loss is always “wrong” and is nearly useless as a metric.</li>
<li>There is no reconstruction loss, but could do a distribution measure on integrated endpoints, e.g….
<ul>
<li>Wasserstein loss / approximations. cf <a href="https://www.kernel-operations.io/geomloss/">geomloss</a></li>
</ul></li>
</ul>
</section>
<section id="keeping-it-stable" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="keeping-it-stable"><span class="header-section-number">4.5</span> Keeping it stable</h2>
<ul>
<li>EMA</li>
<li>attention trick h/t SD3</li>
</ul>
</section>
<section id="cheat-outputs-by-fine-tuning-your-decoder" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="cheat-outputs-by-fine-tuning-your-decoder"><span class="header-section-number">4.6</span> ‘Cheat’ outputs by fine-tuning your Decoder</h2>
<ul>
<li>adversarial loss</li>
<li>Wasserstein loss / approximations. cf <a href="https://www.kernel-operations.io/geomloss/">geomloss</a></li>
<li>For throughput (#2), train ReFlow first
<ul>
<li>and again, pre-gen reflowed gen data,</li>
<li>Reflow’d model you can actually do back-prop on if you want.</li>
</ul></li>
</ul>
</section>
<section id="the-usual-nn-training-stuff" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="the-usual-nn-training-stuff"><span class="header-section-number">4.7</span> The usual NN training stuff:</h2>
<ul>
<li>learning rate schedules (but cf “just let it keep training”)</li>
<li>weight decay</li>
</ul>
</section>
<section id="trainingfine-tuning-your-own-vae-vqvae" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="trainingfine-tuning-your-own-vae-vqvae"><span class="header-section-number">4.8</span> Training/Fine-Tuning Your Own VAE / VQVAE</h2>
<ul>
<li>In my experience, using pretrained VQVAE from Stable Diffusion for my domain yielded unacceptable resu.ts</li>
<li>Don’t try to roll your own quantizer. Use <span class="citation" data-cites="lucidrain"><a href="#ref-lucidrain" role="doc-biblioref"><strong>lucidrain?</strong></a></span>’s <code>vector_quantize_pytorch</code></li>
</ul>
</section>
<section id="inference-applications" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="inference-applications"><span class="header-section-number">4.9</span> Inference Applications:</h2>
<ul>
<li>Inpainting.</li>
</ul>
</section>
<section id="other-random-notes" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="other-random-notes"><span class="header-section-number">4.10</span> other random notes</h2>
<ul>
<li>If doing RK4 steps, you don’t need a lot. (tried 25 vs 100 vs 400, found no diff)</li>
</ul>
</section>
<section id="aside---on-reading-probability-math." class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="aside---on-reading-probability-math."><span class="header-section-number">4.11</span> Aside - On reading probability-math….</h2>
<hr>
</section>
</section>
<section id="notes-key-insights-from-references" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Notes / Key insights from References:</h1>
<section id="og-paper-lipman2023flow-andor-metas-guide-and-code-lipman2024flowmatchingguidecode" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="og-paper-lipman2023flow-andor-metas-guide-and-code-lipman2024flowmatchingguidecode"><span class="header-section-number">5.1</span> OG paper <span class="citation" data-cites="lipman2023flow"><a href="#ref-lipman2023flow" role="doc-biblioref">[2]</a></span> and/or Meta’s “Guide and Code” <span class="citation" data-cites="lipman2024flowmatchingguidecode"><a href="#ref-lipman2024flowmatchingguidecode" role="doc-biblioref">[3]</a></span></h2>
<ul>
<li>Useless AFAICT. Sat through over an hour of math, then “So now you’ve got a trained model.” Are you ****ing kidding me.</li>
<li>Their image example is ImageNet? We have to download all of ImageNet??? And then submit to a SLURM cluster??</li>
</ul>
</section>
<section id="sd3-paper-sd3_paper" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sd3-paper-sd3_paper"><span class="header-section-number">5.2</span> SD3 paper: <span class="citation" data-cites="sd3_paper"><a href="#ref-sd3_paper" role="doc-biblioref">[1]</a></span></h2>
<ul>
<li>pre-encode data (VAE) before training flow</li>
<li>trick in attention for better stability</li>
</ul>
</section>
<section id="improving-the-training-of-rf-improving_rf" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="improving-the-training-of-rf-improving_rf"><span class="header-section-number">5.3</span> Improving the Training of RF: <span class="citation" data-cites="improving_rf"><a href="#ref-improving_rf" role="doc-biblioref">[4]</a></span></h2>
<ul>
<li>key insight: “one reflow is enough”.</li>
</ul>
</section>
<section id="ben-recommended-equivariant_fm" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="ben-recommended-equivariant_fm"><span class="header-section-number">5.4</span> Ben recommended: <span class="citation" data-cites="equivariant_fm"><a href="#ref-equivariant_fm" role="doc-biblioref">[5]</a></span></h2>
<ul>
<li>related: morph input data (e.g.&nbsp;rescale gaussian) to match output more closely before starting</li>
</ul>
</section>
<section id="jasco-jasco" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="jasco-jasco"><span class="header-section-number">5.5</span> JASCO <span class="citation" data-cites="jasco"><a href="#ref-jasco" role="doc-biblioref">[6]</a></span>:</h2>
<ul>
<li>Flow in latent space of Meta’s audio Encodec (RVQ):</li>
</ul>
<blockquote class="blockquote">
<p>“EnCodec first encodes it into a continuous latent tensor <span class="math inline">\(z\)</span>… Then, z is quantized into <span class="math inline">\(q\)</span>… we use the continuous tensor <span class="math inline">\(z\)</span> as the latent representation, while leveraging the discrete representation <span class="math inline">\(q\)</span> for audio conditioning”</p>
</blockquote>
<p>…“The model is trained to predict the vector field of the continuous latent audio variable <span class="math inline">\(z\)</span>, given <span class="math inline">\(t\)</span> and a set of conditions <span class="math inline">\(Y\)</span>. Formally, the model minimizes the regression loss</p>
<p><span class="math display">\[\mathcal{L}\text{CFM}(\theta; z_0, z_1, t|Y) = ||v\theta(z,t|Y)-(z_1-(1-\sigma_\text{min})\cdot z_0)||^2\]</span></p>
<p>where <span class="math inline">\(z_0 \sim \mathcal{N}(0,I)\)</span> is a sampled noise, <span class="math inline">\(z_1 \sim \mathcal{S}\)</span> is the latent representation of a data sample, and</p>
<p><span class="math display">\[z = (1-(1-\sigma_\text{min})\cdot t)\cdot z_0 + t\cdot z_1\]</span></p>
<p>is an interpolation between the noise and the data sample. <span style="background-color: #999900">For numerical stability, we use a small value <span class="math inline">\(\sigma_\text{min}=10^{-5}\)</span> in both terms.</span> During inference we follow an iterative process, starting with the prior noise <span class="math inline">\(z \leftarrow z_0 \sim \mathcal{N}(0,1)\)</span> and with <span class="math inline">\(t=0\)</span>. In each step, we translate the estimated vector field <span class="math inline">\(v_\theta(z,t|Y)\)</span> into an updated latent sequence <span class="math inline">\(z\)</span>, and gradually converge toward the data distribution.”</p>
</section>
<section id="pnp-flow-pnp_flow-httpsgithub.comannegnxpnp-flow" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="pnp-flow-pnp_flow-httpsgithub.comannegnxpnp-flow"><span class="header-section-number">5.6</span> PnP-Flow: <span class="citation" data-cites="pnp_flow"><a href="#ref-pnp_flow" role="doc-biblioref">[7]</a></span>: https://github.com/annegnx/PnP-Flow</h2>
<ul>
<li>??</li>
</ul>
</section>
<section id="stable-flow-stableflow-httpsomriavrahami.comstable-flow" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="stable-flow-stableflow-httpsomriavrahami.comstable-flow"><span class="header-section-number">5.7</span> Stable Flow: <span class="citation" data-cites="stableflow"><a href="#ref-stableflow" role="doc-biblioref">[8]</a></span>: https://omriavrahami.com/stable-flow/</h2>
<ul>
<li>??</li>
</ul>
</section>
<section id="not-sure-about" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="not-sure-about"><span class="header-section-number">5.8</span> Not sure about:</h2>
<ul>
<li>“Discrete Flow Matching”? : <span class="citation" data-cites="discrete_fm"><a href="#ref-discrete_fm" role="doc-biblioref">[9]</a></span></li>
</ul>
<hr>
</section>
</section>
<section id="outputs-from-shameless-use-of-llms-deepresearch-gptresearcher-more" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Outputs from Shameless Use of LLMs (DeepResearch, GPTResearcher &amp; More):</h1>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ot">[</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">{</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"tutorial"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"conclusion"</span><span class="fu">:</span> <span class="st">"In conclusion, training flow matching and rectified flow generative models involves a combination of effective architecture design, meticulous data preparation, strategic training practices, and robust evaluation methods. By following the insights and practical tips outlined in this tutorial, practitioners can enhance the performance of their models, leading to high-quality generated outputs. Continuous research and adaptation of new techniques, as demonstrated in works like Esser et al., will further advance the capabilities of FM/RF models in generative tasks."</span><span class="fu">,</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"introduction"</span><span class="fu">:</span> <span class="st">"This tutorial focuses on practical tips for training flow matching and rectified flow (FM/RF) generative models, drawing insights from the paper by Esser et al. on 'Scaling Rectified Flow Transformers for High-Resolution Image Synthesis'. FM/RF models are designed to generate high-quality images from noise by learning efficient mappings between data and noise distributions. This tutorial will cover essential aspects of model architecture, data preparation, training strategies, evaluation metrics, common challenges, and concluding thoughts to enhance the training process and output quality."</span><span class="fu">,</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"training_tips"</span><span class="fu">:</span> <span class="st">"To accelerate convergence during training, consider the following tips: 1. Use advanced optimizers like AdamW with appropriate learning rates and warmup steps. 2. Implement mixed-precision training to reduce memory usage and speed up computations. 3. Experiment with different noise samplers and timestep schedules to find the most effective configuration for your specific model. 4. Regularly monitor validation losses and adjust hyperparameters accordingly to avoid overfitting. 5. Utilize techniques like Direct Preference Optimization (DPO) to fine-tune models based on human preferences, enhancing output quality."</span><span class="fu">,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"data_preparation"</span><span class="fu">:</span> <span class="st">"Data preparation is critical for training FM/RF models. It involves curating a high-quality dataset that includes diverse examples relevant to the task. For text-to-image models, datasets like CC12M or ImageNet with appropriate captions can be used. Additionally, precomputing image and text embeddings using frozen pretrained models can save time during training. It's also important to filter the dataset to remove low-quality or irrelevant samples, ensuring that the model learns from the best examples available."</span><span class="fu">,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"common_challenges"</span><span class="fu">:</span> <span class="st">"Training FM/RF models can present several challenges, including: 1. High computational costs associated with large model sizes and extensive datasets. 2. Difficulty in achieving stable training dynamics, particularly when scaling models. 3. The risk of overfitting to training data, leading to poor generalization. 4. Balancing the trade-off between model complexity and training efficiency. Addressing these challenges requires careful planning and experimentation with model configurations and training strategies."</span><span class="fu">,</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"evaluation_metrics"</span><span class="fu">:</span> <span class="st">"Evaluation of FM/RF models should include both quantitative and qualitative metrics. Common metrics include FID (Fréchet Inception Distance) and CLIP scores, which assess the quality and relevance of generated images. Human preference evaluations can also provide valuable insights into model performance, especially for subjective tasks like image generation. Regularly evaluate models on a validation set to track improvements and make necessary adjustments during training."</span><span class="fu">,</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"model_architecture"</span><span class="fu">:</span> <span class="st">"The model architecture for FM/RF generative models typically involves a transformer-based backbone that can handle multimodal inputs, such as text and images. The architecture should allow for bi-directional information flow between modalities, improving comprehension and output quality. Esser et al. propose a novel architecture that utilizes separate weights for text and image tokens, enabling effective interaction between the two. This design is crucial for tasks like text-to-image synthesis, where understanding the relationship between text prompts and visual outputs is essential."</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">}</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="ot">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<table class="table">
<tbody>
<tr class="odd">
<td># Output from GPT-Researcher:</td>
</tr>
<tr class="even">
<td>## Practical Tutorial on Training Flow Matching and Rectified Flow (FM/RF) Generative Models for High-Resolution Applications</td>
</tr>
<tr class="odd">
<td>Flow Matching (FM) and Rectified Flow (RF) generative models have emerged as powerful paradigms for generative modeling, offering a simpler and more efficient alternative to diffusion models. These techniques excel in generating high-quality outputs across a variety of domains, including image synthesis, text-to-image generation, and more. However, scaling these models beyond simple cases, such as 2D data or low-resolution images, presents unique challenges. This tutorial provides actionable advice on training FM/RF models for high-resolution applications, with a focus on accelerating training convergence and improving output quality. The insights are drawn from recent advancements, particularly the work of Esser et al.&nbsp;(2024) on scaling Rectified Flow Transformers (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</td>
</tr>
</tbody>
</table>
<section id="introduction-to-flow-matching-and-rectified-flow" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction-to-flow-matching-and-rectified-flow"><span class="header-section-number">6.1</span> <strong>1. Introduction to Flow Matching and Rectified Flow</strong></h2>
<section id="overview-of-flow-matching" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="overview-of-flow-matching"><span class="header-section-number">6.1.1</span> <strong>1.1 Overview of Flow Matching</strong></h3>
<p>Flow Matching (FM) is a generative modeling approach that learns to match the flow of data distributions by regressing onto conditional velocities. Unlike diffusion models, which require iterative sampling, FM directly learns an Ordinary Differential Equation (ODE) that transforms a noise distribution into the target data distribution. This makes FM computationally efficient and easier to implement (<a href="https://neurips.cc/virtual/2024/tutorial/99531">Chen et al., 2024</a>).</p>
</section>
<section id="rectified-flow-a-scalable-extension" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="rectified-flow-a-scalable-extension"><span class="header-section-number">6.1.2</span> <strong>1.2 Rectified Flow: A Scalable Extension</strong></h3>
<p>Rectified Flow (RF) builds upon FM by introducing a rectification process that favors straighter trajectories in the learned ODE. This “straightness” improves the efficiency of Euler discretization during inference, enabling faster and more accurate generation. RF also supports recursive refinement through the “Reflow” procedure, which iteratively improves the learned flow (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</p>
<hr>
</section>
</section>
<section id="challenges-in-scaling-fmrf-models" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="challenges-in-scaling-fmrf-models"><span class="header-section-number">6.2</span> <strong>2. Challenges in Scaling FM/RF Models</strong></h2>
<p>Scaling FM/RF models to high-resolution applications involves addressing several challenges:</p>
<ul>
<li><p><strong>Computational Complexity</strong>: High-resolution data requires more computational resources for training and inference.</p></li>
<li><p><strong>Convergence Speed</strong>: Training FM/RF models can be slow, especially for complex data distributions.</p></li>
<li><p><strong>Output Quality</strong>: Maintaining high fidelity and diversity in generated outputs becomes increasingly difficult as resolution and data complexity grow.</p></li>
</ul>
<hr>
</section>
<section id="practical-steps-for-training-fmrf-models" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="practical-steps-for-training-fmrf-models"><span class="header-section-number">6.3</span> <strong>3. Practical Steps for Training FM/RF Models</strong></h2>
<section id="data-preparation" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="data-preparation"><span class="header-section-number">6.3.1</span> <strong>3.1 Data Preparation</strong></h3>
<ol type="1">
<li><p><strong>Dataset Selection</strong>: Use high-quality datasets that are representative of the target domain. For example, the CelebA-HQ dataset is commonly used for high-resolution image synthesis (<a href="https://github.com/WangLuran/Optimal-control-flow-matching">GitHub - WangLuran, 2023</a>).</p></li>
<li><p><strong>Preprocessing</strong>: Normalize data and augment it with transformations such as cropping, scaling, and flipping to improve model robustness.</p></li>
</ol>
</section>
<section id="model-architecture" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="model-architecture"><span class="header-section-number">6.3.2</span> <strong>3.2 Model Architecture</strong></h3>
<ol type="1">
<li><p><strong>Transformer-Based Architectures</strong>: Esser et al.&nbsp;(2024) introduced Rectified Flow Transformers, which leverage the scalability and expressiveness of transformers for high-resolution image synthesis (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</p></li>
<li><p><strong>Layer Scaling</strong>: Use deeper architectures with attention mechanisms to capture fine-grained details in high-resolution data.</p></li>
</ol>
<hr>
</section>
</section>
<section id="accelerating-training-convergence" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="accelerating-training-convergence"><span class="header-section-number">6.4</span> <strong>4. Accelerating Training Convergence</strong></h2>
<section id="noise-sampling-techniques" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="noise-sampling-techniques"><span class="header-section-number">6.4.1</span> <strong>4.1 Noise Sampling Techniques</strong></h3>
<ol type="1">
<li><p><strong>Perceptually Relevant Noise</strong>: Bias noise sampling towards perceptually relevant scales to improve training efficiency and output quality (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</p></li>
<li><p><strong>Rectified Coupling</strong>: Use rectified coupling to initialize training with better start-end pairs, reducing the number of iterations required for convergence (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</p></li>
</ol>
</section>
<section id="optimization-strategies" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="optimization-strategies"><span class="header-section-number">6.4.2</span> <strong>4.2 Optimization Strategies</strong></h3>
<ol type="1">
<li><p><strong>Learning Rate Schedules</strong>: Employ cosine annealing or warm restarts to stabilize training and prevent overfitting.</p></li>
<li><p><strong>Gradient Clipping</strong>: Limit gradient magnitudes to avoid exploding gradients in deep architectures.</p></li>
</ol>
<hr>
</section>
</section>
<section id="improving-output-quality" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="improving-output-quality"><span class="header-section-number">6.5</span> <strong>5. Improving Output Quality</strong></h2>
<section id="post-training-refinements" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="post-training-refinements"><span class="header-section-number">6.5.1</span> <strong>5.1 Post-Training Refinements</strong></h3>
<ol type="1">
<li><p><strong>Fine-Tuning</strong>: Fine-tune the model on a smaller, high-quality subset of the data to enhance output fidelity (<a href="https://neurips.cc/virtual/2024/tutorial/99531">Chen et al., 2024</a>).</p></li>
<li><p><strong>Reflow Procedure</strong>: Apply the Reflow procedure iteratively to improve trajectory straightness and inference speed (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</p></li>
</ol>
</section>
<section id="loss-functions" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">6.5.2</span> <strong>5.2 Loss Functions</strong></h3>
<ol type="1">
<li><p><strong>Perceptual Loss</strong>: Incorporate perceptual loss functions (e.g., LPIPS) to align generated outputs with human perception (<a href="https://github.com/WangLuran/Optimal-control-flow-matching">GitHub - WangLuran, 2023</a>).</p></li>
<li><p><strong>Adversarial Loss</strong>: Combine FM/RF with adversarial training to improve the realism of generated images.</p></li>
</ol>
<hr>
</section>
</section>
<section id="case-study-scaling-rectified-flow-transformers" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="case-study-scaling-rectified-flow-transformers"><span class="header-section-number">6.6</span> <strong>6. Case Study: Scaling Rectified Flow Transformers</strong></h2>
<section id="key-contributions-of-esser-et-al.-2024" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="key-contributions-of-esser-et-al.-2024"><span class="header-section-number">6.6.1</span> <strong>6.1 Key Contributions of Esser et al.&nbsp;(2024)</strong></h3>
<ol type="1">
<li><p><strong>Transformer-Based RF Models</strong>: Introduced a novel transformer-based architecture for high-resolution image synthesis.</p></li>
<li><p><strong>Improved Noise Sampling</strong>: Proposed techniques for biasing noise sampling towards perceptually relevant scales.</p></li>
<li><p><strong>High-Resolution Results</strong>: Demonstrated state-of-the-art performance on high-resolution image synthesis tasks (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</p></li>
</ol>
</section>
<section id="implementation-tips" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="implementation-tips"><span class="header-section-number">6.6.2</span> <strong>6.2 Implementation Tips</strong></h3>
<ol type="1">
<li><p><strong>Hardware Requirements</strong>: Use GPUs with at least 24GB of VRAM for training high-resolution models.</p></li>
<li><p><strong>Batch Size</strong>: Experiment with smaller batch sizes and gradient accumulation to fit large models into memory.</p></li>
</ol>
<hr>
</section>
</section>
<section id="conclusion" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6.7</span> <strong>7. Conclusion</strong></h2>
<p>Flow Matching and Rectified Flow models offer a promising approach to generative modeling, particularly for high-resolution applications. By leveraging advanced architectures, efficient noise sampling techniques, and iterative refinement procedures, these models can achieve state-of-the-art performance. The insights from Esser et al.&nbsp;(2024) provide a valuable roadmap for scaling FM/RF models, making them suitable for real-world applications.</p>
<hr>
</section>
<section id="references" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="references"><span class="header-section-number">6.8</span> <strong>References</strong></h2>
<p>Chen, R. T. Q., Lipman, Y., &amp; Ben-Hamu, H. (2024). Flow Matching for Generative Modeling. NeurIPS Tutorial. Retrieved from https://neurips.cc/virtual/2024/tutorial/99531</p>
<p>Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., &amp; Rombach, R. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. Proceedings of the 41st International Conference on Machine Learning. Retrieved from https://arxiv.org/pdf/2403.03206</p>
<p>Rectified Flow Blog. (2024). Rectified Flow: Straight is Fast. Retrieved from https://rectifiedflow.github.io/blog/2024/intro/</p>
<p>GitHub - WangLuran. (2023). Optimal Control Flow Matching. Retrieved from https://github.com/WangLuran/Optimal-control-flow-matching</p>
<hr>
</section>
<section id="additional-resources" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">6.9</span> <strong>Additional Resources</strong></h2>
<ul>
<li><p><a href="https://github.com/G-U-N/Rectified-Diffusion">Rectified Diffusion GitHub Repository</a></p></li>
<li><p><a href="https://www.semanticscholar.org/paper/Scaling-Rectified-Flow-Transformers-for-Image-Esser-Kulal/41a66997ce0a366bba3becf7c3f37c9aebb13fbd">Scaling Rectified Flow Transformers on Semantic Scholar</a></p></li>
</ul>
<p>INFO: [18:53:07] 📝 Report written for ‘Create a practical tutorial on training flow matching and rectified flow (FM/RF) generative models, focusing on tips for scaling beyond simple cases (like 2D dots or tiny images). Include actionable advice on accelerating training convergence and improving output quality. Suggested reference: Esser et al.’s ’Scaling Rectified Flow Transformers for High-Resolution Image Synthesis’ (https://arxiv.org/pdf/2403.03206). Provide detailed content, not just an outline.’ - <a href="https://link.springer.com/chapter/10.1007/978-3-031-73007-8_20">SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow</a> Report: # Practical Tutorial on Training Flow Matching and Rectified Flow (FM/RF) Generative Models for High-Resolution Applications</p>
<p>Flow Matching (FM) and Rectified Flow (RF) generative models have emerged as powerful paradigms for generative modeling, offering a simpler and more efficient alternative to diffusion models. These techniques excel in generating high-quality outputs across a variety of domains, including image synthesis, text-to-image generation, and more. However, scaling these models beyond simple cases, such as 2D data or low-resolution images, presents unique challenges. This tutorial provides actionable advice on training FM/RF models for high-resolution applications, with a focus on accelerating training convergence and improving output quality. The insights are drawn from recent advancements, particularly the work of Esser et al.&nbsp;(2024) on scaling Rectified Flow Transformers (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</p>
<hr>
</section>
<section id="introduction-to-flow-matching-and-rectified-flow-1" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="introduction-to-flow-matching-and-rectified-flow-1"><span class="header-section-number">6.10</span> <strong>1. Introduction to Flow Matching and Rectified Flow</strong></h2>
<section id="overview-of-flow-matching-1" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="overview-of-flow-matching-1"><span class="header-section-number">6.10.1</span> <strong>1.1 Overview of Flow Matching</strong></h3>
<p>Flow Matching (FM) is a generative modeling approach that learns to match the flow of data distributions by regressing onto conditional velocities. Unlike diffusion models, which require iterative sampling, FM directly learns an Ordinary Differential Equation (ODE) that transforms a noise distribution into the target data distribution. This makes FM computationally efficient and easier to implement (<a href="https://neurips.cc/virtual/2024/tutorial/99531">Chen et al., 2024</a>).</p>
</section>
<section id="rectified-flow-a-scalable-extension-1" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="rectified-flow-a-scalable-extension-1"><span class="header-section-number">6.10.2</span> <strong>1.2 Rectified Flow: A Scalable Extension</strong></h3>
<p>Rectified Flow (RF) builds upon FM by introducing a rectification process that favors straighter trajectories in the learned ODE. This “straightness” improves the efficiency of Euler discretization during inference, enabling faster and more accurate generation. RF also supports recursive refinement through the “Reflow” procedure, which iteratively improves the learned flow (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</p>
<hr>
</section>
</section>
<section id="challenges-in-scaling-fmrf-models-1" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="challenges-in-scaling-fmrf-models-1"><span class="header-section-number">6.11</span> <strong>2. Challenges in Scaling FM/RF Models</strong></h2>
<p>Scaling FM/RF models to high-resolution applications involves addressing several challenges: - <strong>Computational Complexity</strong>: High-resolution data requires more computational resources for training and inference. - <strong>Convergence Speed</strong>: Training FM/RF models can be slow, especially for complex data distributions. - <strong>Output Quality</strong>: Maintaining high fidelity and diversity in generated outputs becomes increasingly difficult as resolution and data complexity grow.</p>
<hr>
</section>
<section id="practical-steps-for-training-fmrf-models-1" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="practical-steps-for-training-fmrf-models-1"><span class="header-section-number">6.12</span> <strong>3. Practical Steps for Training FM/RF Models</strong></h2>
<section id="data-preparation-1" class="level3" data-number="6.12.1">
<h3 data-number="6.12.1" class="anchored" data-anchor-id="data-preparation-1"><span class="header-section-number">6.12.1</span> <strong>3.1 Data Preparation</strong></h3>
<ol type="1">
<li><strong>Dataset Selection</strong>: Use high-quality datasets that are representative of the target domain. For example, the CelebA-HQ dataset is commonly used for high-resolution image synthesis (<a href="https://github.com/WangLuran/Optimal-control-flow-matching">GitHub - WangLuran, 2023</a>).</li>
<li><strong>Preprocessing</strong>: Normalize data and augment it with transformations such as cropping, scaling, and flipping to improve model robustness.</li>
</ol>
</section>
<section id="model-architecture-1" class="level3" data-number="6.12.2">
<h3 data-number="6.12.2" class="anchored" data-anchor-id="model-architecture-1"><span class="header-section-number">6.12.2</span> <strong>3.2 Model Architecture</strong></h3>
<ol type="1">
<li><strong>Transformer-Based Architectures</strong>: Esser et al.&nbsp;(2024) introduced Rectified Flow Transformers, which leverage the scalability and expressiveness of transformers for high-resolution image synthesis (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</li>
<li><strong>Layer Scaling</strong>: Use deeper architectures with attention mechanisms to capture fine-grained details in high-resolution data.</li>
</ol>
<hr>
</section>
</section>
<section id="accelerating-training-convergence-1" class="level2" data-number="6.13">
<h2 data-number="6.13" class="anchored" data-anchor-id="accelerating-training-convergence-1"><span class="header-section-number">6.13</span> <strong>4. Accelerating Training Convergence</strong></h2>
<section id="noise-sampling-techniques-1" class="level3" data-number="6.13.1">
<h3 data-number="6.13.1" class="anchored" data-anchor-id="noise-sampling-techniques-1"><span class="header-section-number">6.13.1</span> <strong>4.1 Noise Sampling Techniques</strong></h3>
<ol type="1">
<li><strong>Perceptually Relevant Noise</strong>: Bias noise sampling towards perceptually relevant scales to improve training efficiency and output quality (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</li>
<li><strong>Rectified Coupling</strong>: Use rectified coupling to initialize training with better start-end pairs, reducing the number of iterations required for convergence (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</li>
</ol>
</section>
<section id="optimization-strategies-1" class="level3" data-number="6.13.2">
<h3 data-number="6.13.2" class="anchored" data-anchor-id="optimization-strategies-1"><span class="header-section-number">6.13.2</span> <strong>4.2 Optimization Strategies</strong></h3>
<ol type="1">
<li><strong>Learning Rate Schedules</strong>: Employ cosine annealing or warm restarts to stabilize training and prevent overfitting.</li>
<li><strong>Gradient Clipping</strong>: Limit gradient magnitudes to avoid exploding gradients in deep architectures.</li>
</ol>
<hr>
</section>
</section>
<section id="improving-output-quality-1" class="level2" data-number="6.14">
<h2 data-number="6.14" class="anchored" data-anchor-id="improving-output-quality-1"><span class="header-section-number">6.14</span> <strong>5. Improving Output Quality</strong></h2>
<section id="post-training-refinements-1" class="level3" data-number="6.14.1">
<h3 data-number="6.14.1" class="anchored" data-anchor-id="post-training-refinements-1"><span class="header-section-number">6.14.1</span> <strong>5.1 Post-Training Refinements</strong></h3>
<ol type="1">
<li><strong>Fine-Tuning</strong>: Fine-tune the model on a smaller, high-quality subset of the data to enhance output fidelity (<a href="https://neurips.cc/virtual/2024/tutorial/99531">Chen et al., 2024</a>).</li>
<li><strong>Reflow Procedure</strong>: Apply the Reflow procedure iteratively to improve trajectory straightness and inference speed (<a href="https://rectifiedflow.github.io/blog/2024/intro/">Rectified Flow Blog, 2024</a>).</li>
</ol>
</section>
<section id="loss-functions-1" class="level3" data-number="6.14.2">
<h3 data-number="6.14.2" class="anchored" data-anchor-id="loss-functions-1"><span class="header-section-number">6.14.2</span> <strong>5.2 Loss Functions</strong></h3>
<ol type="1">
<li><strong>Perceptual Loss</strong>: Incorporate perceptual loss functions (e.g., LPIPS) to align generated outputs with human perception (<a href="https://github.com/WangLuran/Optimal-control-flow-matching">GitHub - WangLuran, 2023</a>).</li>
<li><strong>Adversarial Loss</strong>: Combine FM/RF with adversarial training to improve the realism of generated images.</li>
</ol>
<hr>
</section>
</section>
<section id="case-study-scaling-rectified-flow-transformers-1" class="level2" data-number="6.15">
<h2 data-number="6.15" class="anchored" data-anchor-id="case-study-scaling-rectified-flow-transformers-1"><span class="header-section-number">6.15</span> <strong>6. Case Study: Scaling Rectified Flow Transformers</strong></h2>
<section id="key-contributions-of-esser-et-al.-2024-1" class="level3" data-number="6.15.1">
<h3 data-number="6.15.1" class="anchored" data-anchor-id="key-contributions-of-esser-et-al.-2024-1"><span class="header-section-number">6.15.1</span> <strong>6.1 Key Contributions of Esser et al.&nbsp;(2024)</strong></h3>
<ol type="1">
<li><strong>Transformer-Based RF Models</strong>: Introduced a novel transformer-based architecture for high-resolution image synthesis.</li>
<li><strong>Improved Noise Sampling</strong>: Proposed techniques for biasing noise sampling towards perceptually relevant scales.</li>
<li><strong>High-Resolution Results</strong>: Demonstrated state-of-the-art performance on high-resolution image synthesis tasks (<a href="https://arxiv.org/pdf/2403.03206">Esser et al., 2024</a>).</li>
</ol>
</section>
<section id="implementation-tips-1" class="level3" data-number="6.15.2">
<h3 data-number="6.15.2" class="anchored" data-anchor-id="implementation-tips-1"><span class="header-section-number">6.15.2</span> <strong>6.2 Implementation Tips</strong></h3>
<ol type="1">
<li><strong>Hardware Requirements</strong>: Use GPUs with at least 24GB of VRAM for training high-resolution models.</li>
<li><strong>Batch Size</strong>: Experiment with smaller batch sizes and gradient accumulation to fit large models into memory.</li>
</ol>
<hr>
</section>
</section>
<section id="conclusion-1" class="level2" data-number="6.16">
<h2 data-number="6.16" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">6.16</span> <strong>7. Conclusion</strong></h2>
<p>Flow Matching and Rectified Flow models offer a promising approach to generative modeling, particularly for high-resolution applications. By leveraging advanced architectures, efficient noise sampling techniques, and iterative refinement procedures, these models can achieve state-of-the-art performance. The insights from Esser et al.&nbsp;(2024) provide a valuable roadmap for scaling FM/RF models, making them suitable for real-world applications.</p>
<hr>
</section>
<section id="references-1" class="level2" data-number="6.17">
<h2 data-number="6.17" class="anchored" data-anchor-id="references-1"><span class="header-section-number">6.17</span> <strong>References</strong></h2>
<p>Chen, R. T. Q., Lipman, Y., &amp; Ben-Hamu, H. (2024). Flow Matching for Generative Modeling. NeurIPS Tutorial. Retrieved from https://neurips.cc/virtual/2024/tutorial/99531</p>
<p>Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., &amp; Rombach, R. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. Proceedings of the 41st International Conference on Machine Learning. Retrieved from https://arxiv.org/pdf/2403.03206</p>
<p>Rectified Flow Blog. (2024). Rectified Flow: Straight is Fast. Retrieved from https://rectifiedflow.github.io/blog/2024/intro/</p>
<p>GitHub - WangLuran. (2023). Optimal Control Flow Matching. Retrieved from https://github.com/WangLuran/Optimal-control-flow-matching</p>
<hr>
</section>
<section id="additional-resources-1" class="level2" data-number="6.18">
<h2 data-number="6.18" class="anchored" data-anchor-id="additional-resources-1"><span class="header-section-number">6.18</span> <strong>Additional Resources</strong></h2>
<ul>
<li><a href="https://github.com/G-U-N/Rectified-Diffusion">Rectified Diffusion GitHub Repository</a></li>
<li><a href="https://www.semanticscholar.org/paper/Scaling-Rectified-Flow-Transformers-for-Image-Esser-Kulal/41a66997ce0a366bba3becf7c3f37c9aebb13fbd">Scaling Rectified Flow Transformers on Semantic Scholar</a></li>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-031-73007-8_20">SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow</a></li>
</ul>
<p>Research Costs: 0.09565546</p>
</section>
</section>
<section id="references-2" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-sd3_paper" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">P. Esser <em>et al.</em>, <span>“Scaling rectified flow transformers for high-resolution image synthesis,”</span> in <em>41st <span>I</span>nternational <span>C</span>onference on <span>M</span>achine <span>L</span>earning, <span>ICML</span></em>, Vienna, Austria, 2024. Available: <a href="https://openreview.net/forum?id=FPnUhsQJ5B">https://openreview.net/forum?id=FPnUhsQJ5B</a></div>
</div>
<div id="ref-lipman2023flow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, <span>“Flow matching for generative modeling,”</span> in <em><span class="nocase">The Eleventh International Conference on Learning Representations (ICLR)</span></em>, 2023. Available: <a href="https://openreview.net/forum?id=PqvMRDCJT9t">https://openreview.net/forum?id=PqvMRDCJT9t</a></div>
</div>
<div id="ref-lipman2024flowmatchingguidecode" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Y. Lipman <em>et al.</em>, <span>“Flow matching guide and code.”</span> 2024. Available: <a href="https://arxiv.org/abs/2412.06264">https://arxiv.org/abs/2412.06264</a></div>
</div>
<div id="ref-improving_rf" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Lee, Z. Lin, and G. Fanti, <span>“Improving the training of rectified flows.”</span> 2024. Available: <a href="https://arxiv.org/abs/2405.20320">https://arxiv.org/abs/2405.20320</a></div>
</div>
<div id="ref-equivariant_fm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">L. Klein, A. Krämer, and F. Noé, <span>“Equivariant flow matching.”</span> 2023. Available: <a href="https://arxiv.org/abs/2306.15030">https://arxiv.org/abs/2306.15030</a></div>
</div>
<div id="ref-jasco" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">O. Tal, A. Ziv, I. Gat, F. Kreuk, and Y. Adi, <span>“Joint audio and symbolic conditioning for temporally controlled text-to-music generation.”</span> 2024. Available: <a href="https://arxiv.org/abs/2406.10970">https://arxiv.org/abs/2406.10970</a></div>
</div>
<div id="ref-pnp_flow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">S. Martin, A. Gagneux, P. Hagemann, and G. Steidl, <span>“PnP-flow: Plug-and-play image restoration with flow matching.”</span> 2024. Available: <a href="https://arxiv.org/abs/2410.02423">https://arxiv.org/abs/2410.02423</a></div>
</div>
<div id="ref-stableflow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">O. Avrahami <em>et al.</em>, <span>“Stable flow: Vital layers for training-free image editing.”</span> 2024. Available: <a href="https://arxiv.org/abs/2411.14430">https://arxiv.org/abs/2411.14430</a></div>
</div>
<div id="ref-discrete_fm" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">I. Gat <em>et al.</em>, <span>“Discrete flow matching.”</span> 2024. Available: <a href="https://arxiv.org/abs/2407.15595">https://arxiv.org/abs/2407.15595</a></div>
</div>
</div>
<hr>
<ol start="3" type="a">
<li>2025 Scott H. Hawley</li>
</ol>
<p>Did you use this work as a reference? Please cite it:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">@misc</span>{<span class="ot">stuff</span>...</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/drscotthawley\.github\.io\/blog");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>