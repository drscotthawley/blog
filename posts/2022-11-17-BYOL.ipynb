{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bbc181dc",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2022/11/17/BYOL\n",
    "author: Scott H. Hawley\n",
    "badges: true\n",
    "date: '2022-11-17'\n",
    "categories:\n",
    "- ssl\n",
    "description: \"Now that we understand contrastive losses and what they're good for --\n",
    "  let's get rid of them!\"\n",
    "image: https://production-media.paperswithcode.com/methods/Screenshot_2021-03-15_at_19.58.32_ENGHInW.png\n",
    "output-file: 2022-11-17-byol.html\n",
    "title: \"BYOL - Contrastive Representation Learning without Contrastive Losses\"\n",
    "toc: true\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785dd90",
   "metadata": {},
   "source": [
    "*\"In this work, we thus tasked ourselves to find out whether...negative examples are indispensable to prevent collapsing while preserving high performance.\"* -- the BYOL paper\n",
    "\n",
    "## References:\n",
    "\n",
    "This post is indebted to\n",
    "* [The Bootstrap Your Own Latent (BYOL) paper](https://arxiv.org/pdf/2006.07733.pdf) by Grill et al -- you could just stop reading my blog and read that, it's not a scary paper! (And yes, actually use the arXiv version because it's got extra figures not in the NeurIPS version I found.)   \n",
    "* The [wonderful tutorial from The AI Summer](https://theaisummer.com/byol/) by Nikolas Adaloglou\n",
    "* The [\"BYOL-A: BYOL for Audio\" paper](https://arxiv.org/pdf/2103.06695.pdf) by Nizumi et al\n",
    "* The [`byol-pytorch`](https://github.com/lucidrains/byol-pytorch) code repository by Phil Wang aka lucidrains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5e7bc",
   "metadata": {},
   "source": [
    "## Review\n",
    "Previously, on \"Trying to Understand Embeddings, with Scott\", i.e. [Part 3 of my blog series](https://drscotthawley.github.io/blog/2021/08/01/Live-CL-Demo.html), we'd worked out way to think of embeddings, and contrastive losses, and even built a toy model. \n",
    "\n",
    "In the toy model there were pairwise losses (boo!) and triplet losses (yay!), and even an \"Attract Only\" option whereby we got rid of 'repulsion' entirely. After the \"Attract Only\" ran, we would rescale the answers and that rescaling would produce a kind of \"repulsion\".  In that sense, the \"Attract Only\" method was one way to \"remove the contrastive loss\" thing. \n",
    "\n",
    "...uh... but as far as I know, nobody does that.  The [SimCLR](https://arxiv.org/abs/2002.05709) (\"sim-clear\") method mentioned a bit in earlier posts and elsewhere is one way of dealing with the problem of finding \"challenging\" negative examples, by working on a kind of \"attraction\", but not as naive as the toy model I made. \n",
    "\n",
    "\n",
    "BYOL is another way to simplify 'contrastive' learning and avoid hard-negative mining and it seems a bit like \"attract only\" in that it no longer means *explicitly* including a respulsive term in the loss function, but BYOL different from SimCLR and not as naive as my own scheme.  Instead, BYOL, uses an another network to do some comparisons. \n",
    "\n",
    "Recal that the goal of these systems is to get \"good\", \"semantically meaningful\" representations, however we can.  If it takes multiple networks to do that, no worries.\n",
    "\n",
    "In [Part 2](https://drscotthawley.github.io/blog/scottergories/2021/06/17/Contrasting-Contrastive-Loss.html) of this blog series, we looked at Siamese Networks, where two copies of the same network are employed for pairwise contrastive learning.  With BYOL however, the two networks have the same architectures but *different weights*, and this difference helps to force \"semantically interesting\" embedding choices.\n",
    "\n",
    "> **Anthropomorphism**: The use of two very different networks to try to arrive at similar embedding points is akin to having two very different people talk about something (while each trying on lots of very different funny-colored eyeglasses!) and iteratively refine their understanding through discussion until they can come to (some sufficient level of) agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6201121",
   "metadata": {},
   "source": [
    "## Strategy: How we're going to do this\n",
    "I'm a firm believer in toy models, so my plan is to use the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and then BYOL-embed a 3-dimensional set of represenations that we can look at and play with.\n",
    "\n",
    "Oh, and since BYOL is a self-supervised method, we're going to *throw away the labels* from Fashion-MNIST ;-). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f46727",
   "metadata": {},
   "source": [
    "## How BYOL Works\n",
    "\n",
    "### In Diagrams\n",
    "\n",
    "First let's steal multiple diagrams that all attempt to show the same thing.\n",
    "\n",
    "From the original BYOL paper, we have this one: \n",
    "![byol_orig_process](images/byol_orig_proc_diagram.png)\n",
    "\n",
    "> *\"BYOL’s goal is to learn a representation $y_\\theta$ which can then be used for downstream tasks.\"* -- the BYOL paper.  \n",
    "\n",
    "> So beyond the \"representation\" parts we want to ultimately use, we'll tack on additional \"projection\" parts (and even a \"prediction\" part) to facilitate the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4624",
   "metadata": {},
   "source": [
    "Later in the BYOL paper (Figure 8), we have this version:\n",
    "![byol_aisummer_process](images/byol_aisummer_proc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45964dbb",
   "metadata": {},
   "source": [
    "And from the BYOL-A paper we have this version: \n",
    "![byol-a process diagram](images/byola_proc_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d82ed4",
   "metadata": {},
   "source": [
    "In each case, what we see are 3 main parts:\n",
    "\n",
    "1. A single input ($x$) gets modified in two different ways ($v$ and $v')$. The two different ways are termed *views*.  \n",
    "2. Each view is sent through a different network (\"online\" or \"target\") and gets mapped to the *same* embedding space.  \n",
    "3. Then the loss/training is about minimizing the distance between those points in the embeddings space. \n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "there is only minimizing, i.e. \"attraction\" going on. There is no \"repulsion\".\n",
    "\n",
    ":::\n",
    "\n",
    "Those were broad strokes.  What about the details?  What about the \"exponential moving average\" bit, and the $q_\\theta(z_\\theta)$ and $z'_\\xi$, and that...equation?  We'll get there. \n",
    "\n",
    "> Note also that we don't \"want to keep\" those points $q_\\theta(z_\\theta)$ and $z'_\\xi$, they're just used along the way to help us learn the representations $y_\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef408e",
   "metadata": {},
   "source": [
    "### Contrastive Learning Without Contrastive Loss, via Different Networks\n",
    "\n",
    "The two networks aren't *totally* different.  If you look at the second diagram above (with the dogs), you'll see that the the first couple layers (in yellow) are of the same types: ResNet then MLP.  They don't have the same weights, but the weights are \"related\". \n",
    "\n",
    "And one of the networks (the \"target\") learns \"slower\" than the other (\"online\") network... in a sense.  This is the \"exponential moving average\" (EMA) part. EMA gets used in many contexts in machine learning (ML) to try to help keep things stable so that the system doesn't jump around too much, i.e. to keep the system from behaving erratically.  Think of reinforcement learning, where you want your robot to smoothly improve its position information instead of undergoing wild overcorrections. \n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "In some ML contexts, the slower-learning network is called the \"teacher\" rather than the \"target\", and the \"online\" network is termed the \"student\". [Here's a link for an influential paper on \"Teacher-Student\" models](https://paperswithcode.com/paper/knowledge-distillation-and-student-teacher).  I find this terminology to be counter-intutitive because in the BYOL case the \"student\" would be *teaching* the \"teacher\".\n",
    "\n",
    ":::\n",
    "\n",
    "The target network gets its weights *only* from the EMA of the corresponding weights in the online network.  The target weights are not obtained via gradient descent; only the online weights are updated via gradient descent.)  In other words, if the online weights are $\\theta$ and the target weights are $\\xi$, then the EMA operation consists of\n",
    "\n",
    "$$\\xi \\leftarrow \\tau \\xi + (1 - \\tau) \\theta, $$\n",
    "for some choice of the \"EMA spread/strength\" (hyper)parameter $\\tau$. \n",
    "\n",
    "The terms \"target\" and \"online\" can also refer to the representation \"points\" in the embedding space. Using such terminology, the BYOL paper explains the method this way: \n",
    "\n",
    "> *\"the core\n",
    "motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially\n",
    "enhanced representation, referred to as online, by predicting the target representation. From there, we\n",
    "can expect to build a sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as new target networks for further training...\"*\n",
    "\n",
    "...i.e. we update the target (a bit, using the EMA) and do it all again. \n",
    "\n",
    "Ok, so then what's with the extra \"projection\" and \"prediction\" layers?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fb142",
   "metadata": {},
   "source": [
    "### The Mapping Functions\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The BYOL authors use the subscript \"${}_\\theta$\" to refer to weights in the online network, and the subscript \"${}_\\xi$\" to refer to weights in the target network.  Vectors in the target network are also denoted via primes, e.g., $v'$, $y'$, $z'$.\n",
    "\n",
    ":::\n",
    "\n",
    "* **Encoder $f$** ($f_\\theta$ and $f_\\xi$): Views (i.e., $v$ and $v'$, i.e., augmented versions of the input $x$) are mapped to embeddings $y$ ($y_\\theta$ in the online network) via the \"encoder\" function $f$ ($f_\\theta$ online). And remember, *\"BYOL’s goal is to learn a representation $y_\\theta$ which can then be used for downstream tasks.\"*  For images, $f$ is typically a ResNet. \n",
    "\n",
    "* **Projector $g$** ($g_\\theta$ and $g_\\xi$): Maps the embeddings $y$ to points $z$ in the space where loss will be evaluated. In particular, $z'_\\xi$ is important because it's a point output by the target network, which the online network is going to try to \"predict\".  $g$ can just be an MLP (though see below for comments about BatchNorm).\n",
    "\n",
    "* **Predictor $q_\\theta$**: is only on the online network.  The predictors output $q_\\theta(z_\\theta)$ is the online network's *prediction* of the target network's output $z'_\\xi$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88095509",
   "metadata": {},
   "source": [
    "### Hold up: Questions\n",
    "\n",
    "1. Why's the predictor there at all?  In other words, why can't we just compare $z_\\theta$ and $z'_\\xi$ without this additional $q_\\theta$ function?  \n",
    "\n",
    "2. And for that matter, why can't we just compare $y_\\theta$ and $y'_\\xi$ directly? \n",
    "\n",
    "Let's answer these in reverse order: \n",
    "\n",
    "<ol start=\"2\" reversed=\"reversed\">\n",
    "    <li>Comparing $y_\\theta$ and $y'_\\xi$ is what we were already doing before with ordinary contrastive losses.</li>\n",
    "    <li>And then SimCLR came along and introduced an additional mapping function akin to our \"projector\" $g$ in which we could compare $z_\\theta$ and $z'_\\xi$ -- so that's been tried already.  And it does work quite well for assisting with contrastive representation learning without having to worry to much about finding \"hard negatives\".</li>\n",
    "</ol>\n",
    "\n",
    "But now we're trying something different, with the goal of avoiding negative examples (i.e. contrastive losses) and the goal of...beating SimCLR. ;-) So bear with this discussion!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3739f",
   "metadata": {},
   "source": [
    "### The Loss\n",
    "We define a loss in the \"projected\" space between the points $q_\\theta(z_\\theta)$ and $z'_\\xi$, that's just the ordinary mean L2 norm (\"Euclidean distance\") between them. So"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399fa18",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_{\\theta\\xi}= ||\\bar{q_{\\theta}}(z_\\theta) - \\bar{z}'_\\xi||_2^2$$\n",
    "\n",
    "Or you can write it in terms of a dot product normalized by the magnitudes, which is what we see written in the BYOL paper: \n",
    "\n",
    "$$\\mathcal{L}_{\\theta\\xi} = 2 - 2\\cdot\\frac{\\langle q_\\theta(z_\\theta),  z'_\\xi \\rangle }{\\big\\|q_\\theta(z_\\theta)\\big\\|_2\\cdot \\big\\|z'_\\xi\\big\\|_2  }\n",
    "$$\n",
    "\n",
    "If that reminds you of a cosine similarity -- good, because that's exactly what it is. See, the graph of $2(1-\\cos x)$ has a nice minimum when its argument is zero, kind of like a parabola on a certain domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "x = np.linspace(-np.pi, np.pi,num=100)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('2 - 2 cos(x)')\n",
    "plt.plot(x, 2*(1-np.cos(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672382c",
   "metadata": {},
   "source": [
    "One other thing they do is to \"symmetrize\" the loss by also passing $v'$ through the online network and passing $v$ through the target network, to compute what they call $\\tilde{\\mathcal{L}}_{\\xi\\theta}$, and then the full loss is the sum of these two losses:\n",
    "\n",
    "$$\\mathcal{L}^{\\rm BYOL} = \\mathcal{L}_{\\theta\\xi} + \\tilde{\\mathcal{L}}_{\\xi\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c127f5",
   "metadata": {},
   "source": [
    "### Further Theory-Reading\n",
    "* Still you may ask, \"But whyyyyyyy does this work?\" Here's a good paper that tries to answer just that: [\"Understanding self-supervised Learning Dynamics without Contrastive Pairs\"](https://arxiv.org/abs/2102.06810) by Tian et al (2021). \n",
    "* \"What's up with the batch norm / group norm stuff that appears in some discussions of BYOL?\" (which I may or may not have mentioned above, LOL):  See [\"BYOL works even without batch statistics\"](https://arxiv.org/abs/2010.10241) by Richemond et al (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ee320",
   "metadata": {},
   "source": [
    "## Let's Go!  Quick Implemenation for Images\n",
    "\n",
    "In a later post we can talk about writing our own implmentation from scratch (e.g. for  something other than images, such as audio).  But to just get started with all this, \n",
    "what better place to start a coding implementation than [lucidrains' repository](https://github.com/lucidrains/byol-pytorch)? It's super easy to install: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647767af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -qq install byol-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232928e7",
   "metadata": {},
   "source": [
    "...and we can just \"tack it on\" to whatever network/task we might have.  He provides a sample use case in his README which we'll modify slightly.  First, he sets up a simple test using random images, which we'll run a version of now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from byol_pytorch import BYOL \n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae067093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a13f79dfac44ab4bd5eb9cde8825cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet = models.resnet50(weights=True) # this will download resnet50 weights. \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "if torch.device('cpu') == device: print(\"Warning: Running on the CPU.\")\n",
    "\n",
    "image_size = 28   # size for fashion mnist images\n",
    "\n",
    "learner = BYOL(     # lucidrains' class\n",
    "    resnet,\n",
    "    image_size = image_size,\n",
    "    hidden_layer = 'avgpool'\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "def sample_unlabelled_images():\n",
    "    return torch.randn(20, 3, image_size, image_size).to(device)  # make batch of 20 RGB images from random pixels.\n",
    "\n",
    "for i in tqdm_notebook(range(50)):\n",
    "    images = sample_unlabelled_images()\n",
    "    loss = learner(images)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    learner.update_moving_average() # update moving average of target encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb28a6",
   "metadata": {},
   "source": [
    "Great! It works!\n",
    "\n",
    "Now, rather than using random images, we'll use Fashion-MNIST. Let's get the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047fe7b",
   "metadata": {},
   "source": [
    "And... we should note that we don't *have* to use ResNet50 -- in fact, we don't have to use ResNet-Anything! We could specify some other model, which for our dataset, a very simple model could suffice.  \n",
    "\n",
    "And/or, rather than a classifcation model, we could choose something like a U-Net, and then try to get the \"interior\" represenation of the U-Net to offer a more interesting represenation than it otherwise might. \n",
    "\n",
    "For now, just to avoid having to deviate from lucidrains' demo much,  we will stick with pretrained ResNet and just \"drop down\" in complexity to `resnet18`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598b061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104c60786f344b0fa79c7ac973a282b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0/5:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6c1e53d7a43e097b27eefec24ca97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1fb4af6088404ba78802e1a271dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2798d977d8c4e398e03df92729388c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20344289a17c4a05991fa221d2954a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet = models.resnet18(weights=True) # reset resnet weights. \n",
    "\n",
    "learner = BYOL(\n",
    "    resnet, \n",
    "    image_size=28, \n",
    "    hidden_layer = 'avgpool', # activations from this layer will be used as y_theta!\n",
    "    use_momentum = True # set to false for 'SimSiam' variant. https://arxiv.org/abs/2011.10566\n",
    ").to(device)\n",
    "\n",
    "def train_it(learner, lr=3e-4, epochs=5, steps=200):\n",
    "    opt = torch.optim.Adam(learner.parameters(), lr=lr)\n",
    "    for e in range(epochs): \n",
    "        pbar = tqdm_notebook(range(steps), desc=f\"Epoch {e}/{epochs}: \")\n",
    "        for i in pbar:\n",
    "            images, labels = next(iter(train_dataloader))\n",
    "            images = images.to(device).tile([1,3,1,1]) # put on GPU & create RGB from greyscale\n",
    "            loss = learner(images)\n",
    "            pbar.set_postfix({'loss':f\"{loss.detach():.3g}\"})\n",
    "            pbar.refresh()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            learner.update_moving_average() # update moving average of target encoder\n",
    "            \n",
    "train_it(learner) # operates on learner & resnet in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13940a",
   "metadata": {},
   "source": [
    "### Inspecting Our Results\n",
    "How do we access and inspect the representations learned from this? lucidrains' README tells us that we already specified that:\n",
    "\n",
    "> the name (or index) of the hidden layer, whose output is used as the latent representation used for self-supervised training.\n",
    "\n",
    "...So we specified the layer named \"`avgpool`\" as the layer of our network `resnet` whose activations will serve as our learned representations.  We can print out the names of the layers to see where `avgpool` is (look way near the bottom):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450eaeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920da942",
   "metadata": {},
   "source": [
    "So pretty much all the way at the end, just before the last Linear layer.  Let's see how we can get these layer outputs / activations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ea6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some mo' images\n",
    "images, labels = next(iter(train_dataloader))\n",
    "images = images.to(device).tile([1,3,1,1]) # put on GPU & create RGB from greyscale\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc860d7",
   "metadata": {},
   "source": [
    "One way is to use some code we can find in lucidrains' source code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps = learner.online_encoder.get_representation(images)\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd14806",
   "metadata": {},
   "source": [
    "But the 'classic' way to do this in PyTorch is to [register a \"forward hook\"](https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6), as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155ea9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "resnet.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "output = resnet(images)\n",
    "reps = activation['avgpool'].squeeze()\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22606d3e",
   "metadata": {},
   "source": [
    "Note that our images are 28x28=784 monochrome pixels, so a \"representation\" via 512 points does not make much of a compression. ....But at this point, you can see the basics of how this works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb06baa",
   "metadata": {},
   "source": [
    "No promises, but I may do a later version of this blog where we write our BYOL code from scratch, and/or use a U-Net or some other architecture, and/or look more closely into BYOL-A, but for now, this seems like a reasonable stopping point. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60e709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
