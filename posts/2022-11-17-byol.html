<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Scott H. Hawley">
<meta name="dcterms.date" content="2022-11-17">
<meta name="description" content="Now that we understand contrastive losses and what they’re good for – let’s get rid of them!">

<title>blog - BYOL - Contrastive Representation Learning without Contrastive Losses</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="blog - BYOL - Contrastive Representation Learning without Contrastive Losses">
<meta name="twitter:description" content="Now that we understand contrastive losses and what they’re good for – let’s get rid of them!">
<meta name="twitter:image" content="https://production-media.paperswithcode.com/methods/Screenshot_2021-03-15_at_19.58.32_ENGHInW.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drscotthawley" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/drscotthawley" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">BYOL - Contrastive Representation Learning without Contrastive Losses</h1>
                  <div>
        <div class="description">
          Now that we understand contrastive losses and what they’re good for – let’s get rid of them!
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ssl</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Scott H. Hawley </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 17, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#references" id="toc-references" class="nav-link active" data-scroll-target="#references">References:</a></li>
  <li><a href="#review" id="toc-review" class="nav-link" data-scroll-target="#review">Review</a></li>
  <li><a href="#strategy-how-were-going-to-do-this" id="toc-strategy-how-were-going-to-do-this" class="nav-link" data-scroll-target="#strategy-how-were-going-to-do-this">Strategy: How we’re going to do this</a></li>
  <li><a href="#how-byol-works" id="toc-how-byol-works" class="nav-link" data-scroll-target="#how-byol-works">How BYOL Works</a>
  <ul class="collapse">
  <li><a href="#in-diagrams" id="toc-in-diagrams" class="nav-link" data-scroll-target="#in-diagrams">In Diagrams</a></li>
  <li><a href="#contrastive-learning-without-contrastive-loss-via-different-networks" id="toc-contrastive-learning-without-contrastive-loss-via-different-networks" class="nav-link" data-scroll-target="#contrastive-learning-without-contrastive-loss-via-different-networks">Contrastive Learning Without Contrastive Loss, via Different Networks</a></li>
  <li><a href="#the-mapping-functions" id="toc-the-mapping-functions" class="nav-link" data-scroll-target="#the-mapping-functions">The Mapping Functions</a></li>
  <li><a href="#hold-up-questions" id="toc-hold-up-questions" class="nav-link" data-scroll-target="#hold-up-questions">Hold up: Questions</a></li>
  <li><a href="#the-loss" id="toc-the-loss" class="nav-link" data-scroll-target="#the-loss">The Loss</a></li>
  <li><a href="#further-theory-reading" id="toc-further-theory-reading" class="nav-link" data-scroll-target="#further-theory-reading">Further Theory-Reading</a></li>
  </ul></li>
  <li><a href="#lets-go-quick-implemenation-for-images" id="toc-lets-go-quick-implemenation-for-images" class="nav-link" data-scroll-target="#lets-go-quick-implemenation-for-images">Let’s Go! Quick Implemenation for Images</a>
  <ul class="collapse">
  <li><a href="#inspecting-our-results" id="toc-inspecting-our-results" class="nav-link" data-scroll-target="#inspecting-our-results">Inspecting Our Results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><em>“In this work, we thus tasked ourselves to find out whether…negative examples are indispensable to prevent collapsing while preserving high performance.”</em> – the BYOL paper</p>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References:</h2>
<p>This post is indebted to * <a href="https://arxiv.org/pdf/2006.07733.pdf">The Bootstrap Your Own Latent (BYOL) paper</a> by Grill et al – you could just stop reading my blog and read that, it’s not a scary paper! (And yes, actually use the arXiv version because it’s got extra figures not in the NeurIPS version I found.)<br>
* The <a href="https://theaisummer.com/byol/">wonderful tutorial from The AI Summer</a> by Nikolas Adaloglou * The <a href="https://arxiv.org/pdf/2103.06695.pdf">“BYOL-A: BYOL for Audio” paper</a> by Nizumi et al * The <a href="https://github.com/lucidrains/byol-pytorch"><code>byol-pytorch</code></a> code repository by Phil Wang aka lucidrains</p>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Previously, on “Trying to Understand Embeddings, with Scott”, i.e.&nbsp;<a href="https://drscotthawley.github.io/blog/2021/08/01/Live-CL-Demo.html">Part 3 of my blog series</a>, we’d worked out way to think of embeddings, and contrastive losses, and even built a toy model.</p>
<p>In the toy model there were pairwise losses (boo!) and triplet losses (yay!), and even an “Attract Only” option whereby we got rid of ‘repulsion’ entirely. After the “Attract Only” ran, we would rescale the answers and that rescaling would produce a kind of “repulsion”. In that sense, the “Attract Only” method was one way to “remove the contrastive loss” thing.</p>
<p>…uh… but as far as I know, nobody does that. The <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> (“sim-clear”) method mentioned a bit in earlier posts and elsewhere is one way of dealing with the problem of finding “challenging” negative examples, by working on a kind of “attraction”, but not as naive as the toy model I made.</p>
<p>BYOL is another way to simplify ‘contrastive’ learning and avoid hard-negative mining and it seems a bit like “attract only” in that it no longer means <em>explicitly</em> including a respulsive term in the loss function, but BYOL different from SimCLR and not as naive as my own scheme. Instead, BYOL, uses an another network to do some comparisons.</p>
<p>Recal that the goal of these systems is to get “good”, “semantically meaningful” representations, however we can. If it takes multiple networks to do that, no worries.</p>
<p>In <a href="https://drscotthawley.github.io/blog/scottergories/2021/06/17/Contrasting-Contrastive-Loss.html">Part 2</a> of this blog series, we looked at Siamese Networks, where two copies of the same network are employed for pairwise contrastive learning. With BYOL however, the two networks have the same architectures but <em>different weights</em>, and this difference helps to force “semantically interesting” embedding choices.</p>
<blockquote class="blockquote">
<p><strong>Anthropomorphism</strong>: The use of two very different networks to try to arrive at similar embedding points is akin to having two very different people talk about something (while each trying on lots of very different funny-colored eyeglasses!) and iteratively refine their understanding through discussion until they can come to (some sufficient level of) agreement.</p>
</blockquote>
</section>
<section id="strategy-how-were-going-to-do-this" class="level2">
<h2 class="anchored" data-anchor-id="strategy-how-were-going-to-do-this">Strategy: How we’re going to do this</h2>
<p>I’m a firm believer in toy models, so my plan is to use the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> dataset and then BYOL-embed a 3-dimensional set of represenations that we can look at and play with.</p>
<p>Oh, and since BYOL is a self-supervised method, we’re going to <em>throw away the labels</em> from Fashion-MNIST ;-).</p>
</section>
<section id="how-byol-works" class="level2">
<h2 class="anchored" data-anchor-id="how-byol-works">How BYOL Works</h2>
<section id="in-diagrams" class="level3">
<h3 class="anchored" data-anchor-id="in-diagrams">In Diagrams</h3>
<p>First let’s steal multiple diagrams that all attempt to show the same thing.</p>
<p>From the original BYOL paper, we have this one: <img src="images/byol_orig_proc_diagram.png" class="img-fluid" alt="byol_orig_process"></p>
<blockquote class="blockquote">
<p><em>“BYOL’s goal is to learn a representation <span class="math inline">\(y_\theta\)</span> which can then be used for downstream tasks.”</em> – the BYOL paper.</p>
</blockquote>
<blockquote class="blockquote">
<p>So beyond the “representation” parts we want to ultimately use, we’ll tack on additional “projection” parts (and even a “prediction” part) to facilitate the training.</p>
</blockquote>
<p>Later in the BYOL paper (Figure 8), we have this version: <img src="images/byol_aisummer_proc.png" class="img-fluid" alt="byol_aisummer_process"></p>
<p>And from the BYOL-A paper we have this version: <img src="images/byola_proc_diagram.png" class="img-fluid" alt="byol-a process diagram"></p>
<p>In each case, what we see are 3 main parts:</p>
<ol type="1">
<li>A single input (<span class="math inline">\(x\)</span>) gets modified in two different ways (<span class="math inline">\(v\)</span> and <span class="math inline">\(v')\)</span>. The two different ways are termed <em>views</em>.<br>
</li>
<li>Each view is sent through a different network (“online” or “target”) and gets mapped to the <em>same</em> embedding space.<br>
</li>
<li>Then the loss/training is about minimizing the distance between those points in the embeddings space.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>there is only minimizing, i.e.&nbsp;“attraction” going on. There is no “repulsion”.</p>
</div>
</div>
<p>Those were broad strokes. What about the details? What about the “exponential moving average” bit, and the <span class="math inline">\(q_\theta(z_\theta)\)</span> and <span class="math inline">\(z'_\xi\)</span>, and that…equation? We’ll get there.</p>
<blockquote class="blockquote">
<p>Note also that we don’t “want to keep” those points <span class="math inline">\(q_\theta(z_\theta)\)</span> and <span class="math inline">\(z'_\xi\)</span>, they’re just used along the way to help us learn the representations <span class="math inline">\(y_\theta\)</span>.</p>
</blockquote>
</section>
<section id="contrastive-learning-without-contrastive-loss-via-different-networks" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning-without-contrastive-loss-via-different-networks">Contrastive Learning Without Contrastive Loss, via Different Networks</h3>
<p>The two networks aren’t <em>totally</em> different. If you look at the second diagram above (with the dogs), you’ll see that the the first couple layers (in yellow) are of the same types: ResNet then MLP. They don’t have the same weights, but the weights are “related”.</p>
<p>And one of the networks (the “target”) learns “slower” than the other (“online”) network… in a sense. This is the “exponential moving average” (EMA) part. EMA gets used in many contexts in machine learning (ML) to try to help keep things stable so that the system doesn’t jump around too much, i.e.&nbsp;to keep the system from behaving erratically. Think of reinforcement learning, where you want your robot to smoothly improve its position information instead of undergoing wild overcorrections.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In some ML contexts, the slower-learning network is called the “teacher” rather than the “target”, and the “online” network is termed the “student”. <a href="https://paperswithcode.com/paper/knowledge-distillation-and-student-teacher">Here’s a link for an influential paper on “Teacher-Student” models</a>. I find this terminology to be counter-intutitive because in the BYOL case the “student” would be <em>teaching</em> the “teacher”.</p>
</div>
</div>
<p>The target network gets its weights <em>only</em> from the EMA of the corresponding weights in the online network. The target weights are not obtained via gradient descent; only the online weights are updated via gradient descent.) In other words, if the online weights are <span class="math inline">\(\theta\)</span> and the target weights are <span class="math inline">\(\xi\)</span>, then the EMA operation consists of</p>
<p><span class="math display">\[\xi \leftarrow \tau \xi + (1 - \tau) \theta, \]</span> for some choice of the “EMA spread/strength” (hyper)parameter <span class="math inline">\(\tau\)</span>.</p>
<p>The terms “target” and “online” can also refer to the representation “points” in the embedding space. Using such terminology, the BYOL paper explains the method this way:</p>
<blockquote class="blockquote">
<p><em>“the core motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation. From there, we can expect to build a sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as new target networks for further training…”</em></p>
</blockquote>
<p>…i.e.&nbsp;we update the target (a bit, using the EMA) and do it all again.</p>
<p>Ok, so then what’s with the extra “projection” and “prediction” layers?</p>
</section>
<section id="the-mapping-functions" class="level3">
<h3 class="anchored" data-anchor-id="the-mapping-functions">The Mapping Functions</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The BYOL authors use the subscript “<span class="math inline">\({}_\theta\)</span>” to refer to weights in the online network, and the subscript “<span class="math inline">\({}_\xi\)</span>” to refer to weights in the target network. Vectors in the target network are also denoted via primes, e.g., <span class="math inline">\(v'\)</span>, <span class="math inline">\(y'\)</span>, <span class="math inline">\(z'\)</span>.</p>
</div>
</div>
<ul>
<li><p><strong>Encoder <span class="math inline">\(f\)</span></strong> (<span class="math inline">\(f_\theta\)</span> and <span class="math inline">\(f_\xi\)</span>): Views (i.e., <span class="math inline">\(v\)</span> and <span class="math inline">\(v'\)</span>, i.e., augmented versions of the input <span class="math inline">\(x\)</span>) are mapped to embeddings <span class="math inline">\(y\)</span> (<span class="math inline">\(y_\theta\)</span> in the online network) via the “encoder” function <span class="math inline">\(f\)</span> (<span class="math inline">\(f_\theta\)</span> online). And remember, <em>“BYOL’s goal is to learn a representation <span class="math inline">\(y_\theta\)</span> which can then be used for downstream tasks.”</em> For images, <span class="math inline">\(f\)</span> is typically a ResNet.</p></li>
<li><p><strong>Projector <span class="math inline">\(g\)</span></strong> (<span class="math inline">\(g_\theta\)</span> and <span class="math inline">\(g_\xi\)</span>): Maps the embeddings <span class="math inline">\(y\)</span> to points <span class="math inline">\(z\)</span> in the space where loss will be evaluated. In particular, <span class="math inline">\(z'_\xi\)</span> is important because it’s a point output by the target network, which the online network is going to try to “predict”. <span class="math inline">\(g\)</span> can just be an MLP (though see below for comments about BatchNorm).</p></li>
<li><p><strong>Predictor <span class="math inline">\(q_\theta\)</span></strong>: is only on the online network. The predictors output <span class="math inline">\(q_\theta(z_\theta)\)</span> is the online network’s <em>prediction</em> of the target network’s output <span class="math inline">\(z'_\xi\)</span>.</p></li>
</ul>
</section>
<section id="hold-up-questions" class="level3">
<h3 class="anchored" data-anchor-id="hold-up-questions">Hold up: Questions</h3>
<ol type="1">
<li><p>Why’s the predictor there at all? In other words, why can’t we just compare <span class="math inline">\(z_\theta\)</span> and <span class="math inline">\(z'_\xi\)</span> without this additional <span class="math inline">\(q_\theta\)</span> function?</p></li>
<li><p>And for that matter, why can’t we just compare <span class="math inline">\(y_\theta\)</span> and <span class="math inline">\(y'_\xi\)</span> directly?</p></li>
</ol>
<p>Let’s answer these in reverse order:</p>
<ol start="2" reversed="reversed">
<li>
Comparing <span class="math inline">\(y_\theta\)</span> and <span class="math inline">\(y'_\xi\)</span> is what we were already doing before with ordinary contrastive losses.
</li>
<li>
And then SimCLR came along and introduced an additional mapping function akin to our “projector” <span class="math inline">\(g\)</span> in which we could compare <span class="math inline">\(z_\theta\)</span> and <span class="math inline">\(z'_\xi\)</span> – so that’s been tried already. And it does work quite well for assisting with contrastive representation learning without having to worry to much about finding “hard negatives”.
</li>
</ol>
<p>But now we’re trying something different, with the goal of avoiding negative examples (i.e.&nbsp;contrastive losses) and the goal of…beating SimCLR. ;-) So bear with this discussion!</p>
</section>
<section id="the-loss" class="level3">
<h3 class="anchored" data-anchor-id="the-loss">The Loss</h3>
<p>We define a loss in the “projected” space between the points <span class="math inline">\(q_\theta(z_\theta)\)</span> and <span class="math inline">\(z'_\xi\)</span>, that’s just the ordinary mean L2 norm (“Euclidean distance”) between them. So</p>
<p><span class="math display">\[\mathcal{L}_{\theta\xi}= ||\bar{q_{\theta}}(z_\theta) - \bar{z}'_\xi||_2^2\]</span></p>
<p>Or you can write it in terms of a dot product normalized by the magnitudes, which is what we see written in the BYOL paper:</p>
<p><span class="math display">\[\mathcal{L}_{\theta\xi} = 2 - 2\cdot\frac{\langle q_\theta(z_\theta),  z'_\xi \rangle }{\big\|q_\theta(z_\theta)\big\|_2\cdot \big\|z'_\xi\big\|_2  }
\]</span></p>
<p>If that reminds you of a cosine similarity – good, because that’s exactly what it is. See, the graph of <span class="math inline">\(2(1-\cos x)\)</span> has a nice minimum when its argument is zero, kind of like a parabola on a certain domain:</p>
<p>::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}</p>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span>np.pi, np.pi,num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'2 - 2 cos(x)'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x, <span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>np.cos(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-11-17-BYOL_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
<p>:::</p>
<p>One other thing they do is to “symmetrize” the loss by also passing <span class="math inline">\(v'\)</span> through the online network and passing <span class="math inline">\(v\)</span> through the target network, to compute what they call <span class="math inline">\(\tilde{\mathcal{L}}_{\xi\theta}\)</span>, and then the full loss is the sum of these two losses:</p>
<p><span class="math display">\[\mathcal{L}^{\rm BYOL} = \mathcal{L}_{\theta\xi} + \tilde{\mathcal{L}}_{\xi\theta}\]</span></p>
</section>
<section id="further-theory-reading" class="level3">
<h3 class="anchored" data-anchor-id="further-theory-reading">Further Theory-Reading</h3>
<ul>
<li>Still you may ask, “But whyyyyyyy does this work?” Here’s a good paper that tries to answer just that: <a href="https://arxiv.org/abs/2102.06810">“Understanding self-supervised Learning Dynamics without Contrastive Pairs”</a> by Tian et al (2021).</li>
<li>“What’s up with the batch norm / group norm stuff that appears in some discussions of BYOL?” (which I may or may not have mentioned above, LOL): See <a href="https://arxiv.org/abs/2010.10241">“BYOL works even without batch statistics”</a> by Richemond et al (2020).</li>
</ul>
</section>
</section>
<section id="lets-go-quick-implemenation-for-images" class="level2">
<h2 class="anchored" data-anchor-id="lets-go-quick-implemenation-for-images">Let’s Go! Quick Implemenation for Images</h2>
<p>In a later post we can talk about writing our own implmentation from scratch (e.g.&nbsp;for something other than images, such as audio). But to just get started with all this, what better place to start a coding implementation than <a href="https://github.com/lucidrains/byol-pytorch">lucidrains’ repository</a>? It’s super easy to install:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip <span class="op">-</span>qq install byol<span class="op">-</span>pytorch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<p>…and we can just “tack it on” to whatever network/task we might have. He provides a sample use case in his README which we’ll modify slightly. First, he sets up a simple test using random images, which we’ll run a version of now:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> byol_pytorch <span class="im">import</span> BYOL </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models, transforms, datasets</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm_notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>resnet <span class="op">=</span> models.resnet50(weights<span class="op">=</span><span class="va">True</span>) <span class="co"># this will download resnet50 weights. </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'mps'</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.device(<span class="st">'cpu'</span>) <span class="op">==</span> device: <span class="bu">print</span>(<span class="st">"Warning: Running on the CPU."</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">28</span>   <span class="co"># size for fashion mnist images</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> BYOL(     <span class="co"># lucidrains' class</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    resnet,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    image_size <span class="op">=</span> image_size,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    hidden_layer <span class="op">=</span> <span class="st">'avgpool'</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(learner.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_unlabelled_images():</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.randn(<span class="dv">20</span>, <span class="dv">3</span>, image_size, image_size).to(device)  <span class="co"># make batch of 20 RGB images from random pixels.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> tqdm_notebook(<span class="bu">range</span>(<span class="dv">50</span>)):</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> sample_unlabelled_images()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> learner(images)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    opt.zero_grad()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    opt.step()</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    learner.update_moving_average() <span class="co"># update moving average of target encoder</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0a13f79dfac44ab4bd5eb9cde8825cfe","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Great! It works!</p>
<p>Now, rather than using random images, we’ll use Fashion-MNIST. Let’s get the data…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.ToTensor()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(training_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And… we should note that we don’t <em>have</em> to use ResNet50 – in fact, we don’t have to use ResNet-Anything! We could specify some other model, which for our dataset, a very simple model could suffice.</p>
<p>And/or, rather than a classifcation model, we could choose something like a U-Net, and then try to get the “interior” represenation of the U-Net to offer a more interesting represenation than it otherwise might.</p>
<p>For now, just to avoid having to deviate from lucidrains’ demo much, we will stick with pretrained ResNet and just “drop down” in complexity to <code>resnet18</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>resnet <span class="op">=</span> models.resnet18(weights<span class="op">=</span><span class="va">True</span>) <span class="co"># reset resnet weights. </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> BYOL(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    resnet, </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    image_size<span class="op">=</span><span class="dv">28</span>, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    hidden_layer <span class="op">=</span> <span class="st">'avgpool'</span>, <span class="co"># activations from this layer will be used as y_theta!</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    use_momentum <span class="op">=</span> <span class="va">True</span> <span class="co"># set to false for 'SimSiam' variant. https://arxiv.org/abs/2011.10566</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_it(learner, lr<span class="op">=</span><span class="fl">3e-4</span>, epochs<span class="op">=</span><span class="dv">5</span>, steps<span class="op">=</span><span class="dv">200</span>):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> torch.optim.Adam(learner.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epochs): </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        pbar <span class="op">=</span> tqdm_notebook(<span class="bu">range</span>(steps), desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">: "</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device).tile([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>]) <span class="co"># put on GPU &amp; create RGB from greyscale</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> learner(images)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix({<span class="st">'loss'</span>:<span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>detach()<span class="sc">:.3g}</span><span class="ss">"</span>})</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            pbar.refresh()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            opt.zero_grad()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            opt.step()</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            learner.update_moving_average() <span class="co"># update moving average of target encoder</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>train_it(learner) <span class="co"># operates on learner &amp; resnet in-place</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"104c60786f344b0fa79c7ac973a282b7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b0c6c1e53d7a43e097b27eefec24ca97","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bb1fb4af6088404ba78802e1a271dee9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"c2798d977d8c4e398e03df92729388c8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"20344289a17c4a05991fa221d2954a13","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<section id="inspecting-our-results" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-our-results">Inspecting Our Results</h3>
<p>How do we access and inspect the representations learned from this? lucidrains’ README tells us that we already specified that:</p>
<blockquote class="blockquote">
<p>the name (or index) of the hidden layer, whose output is used as the latent representation used for self-supervised training.</p>
</blockquote>
<p>…So we specified the layer named “<code>avgpool</code>” as the layer of our network <code>resnet</code> whose activations will serve as our learned representations. We can print out the names of the layers to see where <code>avgpool</code> is (look way near the bottom):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>resnet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)</code></pre>
</div>
</div>
<p>So pretty much all the way at the end, just before the last Linear layer. Let’s see how we can get these layer outputs / activations:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get some mo' images</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> images.to(device).tile([<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>]) <span class="co"># put on GPU &amp; create RGB from greyscale</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>images.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([128, 3, 28, 28])</code></pre>
</div>
</div>
<p>One way is to use some code we can find in lucidrains’ source code…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>reps <span class="op">=</span> learner.online_encoder.get_representation(images)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>reps.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([128, 512])</code></pre>
</div>
</div>
<p>But the ‘classic’ way to do this in PyTorch is to <a href="https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6">register a “forward hook”</a>, as in:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> {}</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_activation(name):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        activation[name] <span class="op">=</span> output.detach()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hook</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>resnet.avgpool.register_forward_hook(get_activation(<span class="st">'avgpool'</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> resnet(images)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>reps <span class="op">=</span> activation[<span class="st">'avgpool'</span>].squeeze()</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>reps.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([128, 512])</code></pre>
</div>
</div>
<p>Note that our images are 28x28=784 monochrome pixels, so a “representation” via 512 points does not make much of a compression. ….But at this point, you can see the basics of how this works.</p>
<p>No promises, but I may do a later version of this blog where we write our BYOL code from scratch, and/or use a U-Net or some other architecture, and/or look more closely into BYOL-A, but for now, this seems like a reasonable stopping point. :-)</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/drscotthawley\.github\.io\/blog");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>